{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*\\*\\* Note! This notebook is a repost, I made the original private after some ensembling and postprocessing steps put it into the top 10 on the leaderboard. Kaggle doesn't allow specific versions of a notebook to be made private and I didn't think a top 10 solution should be shared \\*\\*\\*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook constructs a prediction model for the Predict Future Sales competition that is the final project for the Coursera course \"[How to Win a Data Science Competition](http://www.coursera.org/learn/competitive-data-science/home/welcome)\". The task is to predict monthly sales for various items in different retail outlets of the Russian company 1C.  \n",
    "\n",
    "I spent several months on this as practice using pandas, so some parts are a bit more complicated than might be expected of a typical short project submission.\n",
    "\n",
    "There are some other very good notebooks for this competition which are well worth looking at and taught me a lot:\n",
    "https://www.kaggle.com/dlarionov/feature-engineering-xgboost  \n",
    "https://www.kaggle.com/gordotron85/future-sales-xgboost-top-3  \n",
    "https://www.kaggle.com/deepdivelm/feature-engineering-lightgbm-exploring-performance  \n",
    "\n",
    "This is the top-scoring public notebook at the time of writing (0.84325, place 51 on the public leaderboard), which is mainly because of two novel feature types which work well when combined together. First, there is an item name group feature that groups together items with very similar names that are likely to refer to different versions of the same item (e.g. different editions of the same game or music album). Second, the way the test set was generated was exploited to count how many items sold in the month being predicted were in the same group as the item being predicted (e.g. same category, same name group). This combines well with the item name group feature to detect new items which are part of large multi-format releases that are likely to sell well. Detecting high-selling new items is one of the hardest challenges for the model in this competition (and has to be performed manually to get a really high score, I think).\n",
    "\n",
    "I hope you find the notebook interesting, and I welcome feedback - suggestions for improvements, advice about parts that are unclear, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053912,
     "end_time": "2021-04-28T18:11:27.348187",
     "exception": false,
     "start_time": "2021-04-28T18:11:27.294275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data loading and preprocessing, utility function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049686,
     "end_time": "2021-04-28T18:11:27.448493",
     "exception": false,
     "start_time": "2021-04-28T18:11:27.398807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 1.032316,
     "end_time": "2021-04-28T18:11:28.530522",
     "exception": false,
     "start_time": "2021-04-28T18:11:27.498206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few utility functions used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n",
    "    \"\"\" \n",
    "    Iterates through all the columns of a dataframe and downcasts the data type\n",
    "     to reduce memory usage. Can also factorize categorical columns to integer dtype.\n",
    "    \"\"\"\n",
    "    def _downcast_numeric(series, allow_categorical=allow_categorical):\n",
    "        \"\"\"\n",
    "        Downcast a numeric series into either the smallest possible int dtype or a specified float dtype.\n",
    "        \"\"\"\n",
    "        if pd.api.types.is_sparse(series.dtype) is True:\n",
    "            return series\n",
    "        elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n",
    "            if pd.api.types.is_datetime64_any_dtype(series.dtype):\n",
    "                return series\n",
    "            else:\n",
    "                if allow_categorical:\n",
    "                    return series\n",
    "                else:\n",
    "                    codes, uniques = series.factorize()\n",
    "                    series = pd.Series(data=codes, index=series.index)\n",
    "                    series = _downcast_numeric(series)\n",
    "                    return series\n",
    "        else:\n",
    "            series = pd.to_numeric(series, downcast=\"integer\")\n",
    "        if pd.api.types.is_float_dtype(series.dtype):\n",
    "            series = series.astype(float_dtype)\n",
    "        return series\n",
    "\n",
    "    if silent is False:\n",
    "        start_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    if df.ndim == 1:\n",
    "        df = _downcast_numeric(df)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            df.loc[:, col] = _downcast_numeric(df.loc[:,col])\n",
    "    if silent is False:\n",
    "        end_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "        print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def shrink_mem_new_cols(matrix, oldcols=None, allow_categorical=False):\n",
    "    # Calls reduce_mem_usage on columns which have not yet been optimized\n",
    "    if oldcols is not None:\n",
    "        newcols = matrix.columns.difference(oldcols)\n",
    "    else:\n",
    "        newcols = matrix.columns\n",
    "    matrix.loc[:,newcols] = reduce_mem_usage(matrix.loc[:,newcols], allow_categorical=allow_categorical)\n",
    "    oldcols = matrix.columns  # This is used to track which columns have already been downcast\n",
    "    return matrix, oldcols\n",
    "\n",
    "\n",
    "def list_if_not(s, dtype=str):\n",
    "    # Puts a variable in a list if it is not already a list\n",
    "    if type(s) not in (dtype, list):\n",
    "        raise TypeError\n",
    "    if (s != \"\") & (type(s) is not list):\n",
    "        s = [s]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049382,
     "end_time": "2021-04-28T18:11:28.630332",
     "exception": false,
     "start_time": "2021-04-28T18:11:28.58095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 3.762579,
     "end_time": "2021-04-28T18:11:32.444498",
     "exception": false,
     "start_time": "2021-04-28T18:11:28.681919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv(\"items.csv.zip\")\n",
    "shops = pd.read_csv(\"shops.csv\")\n",
    "train = pd.read_csv(\"sales_train.csv.zip\")#, parse_dates=['date'])\n",
    "test = pd.read_csv(\"test.csv.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the date column to the datetime dtype to enable datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 3.762579,
     "end_time": "2021-04-28T18:11:32.444498",
     "exception": false,
     "start_time": "2021-04-28T18:11:28.681919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049628,
     "end_time": "2021-04-28T18:11:32.545881",
     "exception": false,
     "start_time": "2021-04-28T18:11:32.496253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048834,
     "end_time": "2021-04-28T18:11:32.643753",
     "exception": false,
     "start_time": "2021-04-28T18:11:32.594919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The training dataframe is cleaned with standard steps  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge some duplicate shops\n",
    "train[\"shop_id\"] = train[\"shop_id\"].replace({0: 57, 1: 58, 11: 10, 40: 39})\n",
    "# Keep only shops that are in the test set\n",
    "train = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]\n",
    "\n",
    "# Drop training items with extreme or negative sale counts\n",
    "train = train[(train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix wrong prices\n",
    "\n",
    "Some of prices may be wrong. Let's find them. At first we look for prices with big difference between min and max price for each item and find the most frequent normal prices for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each item get price with mode value > 5\n",
    "def amode(col):\n",
    "    i = 0\n",
    "    res = 0\n",
    "    count = col.value_counts().index\n",
    "    while res <= 5 and i < len(count):\n",
    "        res = count[i]\n",
    "        i += 1\n",
    "    return res\n",
    "\n",
    "def alast(col):\n",
    "    return res\n",
    "    \n",
    "# Group train dataset by prices, aggregate by price mode\n",
    "item_prices = train[['item_id', 'item_price']].groupby('item_id').agg({'item_price': [np.min, \n",
    "                                                                                      np.max, \n",
    "                                                                                      amode]})\n",
    "# Add feature for difference between min and max prices\n",
    "item_prices['price_diff'] = abs(item_prices.item_price.amax/item_prices.item_price.amin)\n",
    "\n",
    "# Get all prices with the difference between min and max prices more than 15 and min price less than 5\n",
    "# Save indexes of these prices\n",
    "wrong_prices = item_prices[(item_prices.item_price.amin <= 5) & \n",
    "                           (item_prices.price_diff >= 15)].sort_values('price_diff', ascending=False)\n",
    "\n",
    "wrong_prices.head()\n",
    "\n",
    "for i_id in list(wrong_prices.index):\n",
    "    train.loc[(train.item_id == i_id) & \n",
    "              (train.item_price <= 5), 'item_price'] = wrong_prices.loc[i_id, 'item_price'].amode\n",
    "    \n",
    "del wrong_prices\n",
    "gc.collect()\n",
    "\n",
    "# Also drop training items with extreme or negative prices\n",
    "train = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053208,
     "end_time": "2021-04-28T18:11:34.280403",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.227195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050656,
     "end_time": "2021-04-28T18:11:34.384271",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.333615",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The test data seems to be every possible combination (the cartesian product) of shops and items that registered a sale in the test month, with the target as the total month's sales made for each of these shop-item combinations. Here a training matrix is made that replicates this structure for every month in the training data period. The test items are concatenated to the end of the training data so that features can be generated for the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.065775,
     "end_time": "2021-04-28T18:11:34.500373",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.434598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_testlike_train(sales_train, test=None):\n",
    "    indexlist = []\n",
    "    for i in sales_train.date_block_num.unique():\n",
    "        x = itertools.product(\n",
    "            [i],\n",
    "            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n",
    "            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n",
    "        )\n",
    "        indexlist.append(np.array(list(x)))\n",
    "    df = pd.DataFrame(\n",
    "        data=np.concatenate(indexlist, axis=0),\n",
    "        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    # Add revenue column to sales_train\n",
    "    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n",
    "    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n",
    "    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n",
    "        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n",
    "        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n",
    "    )\n",
    "\n",
    "    # Merge the grouped data with the index\n",
    "    df = df.merge(\n",
    "        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    if test is not None:\n",
    "        test[\"date_block_num\"] = 34\n",
    "        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
    "        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
    "        test[\"item_id\"] = test.item_id.astype(np.int16)\n",
    "        test = test.drop(columns=\"ID\")\n",
    "\n",
    "        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n",
    "\n",
    "    # Fill empty item_cnt entries with 0\n",
    "    df.item_cnt_month = df.item_cnt_month.fillna(0)\n",
    "    df.item_revenue_month = df.item_revenue_month.fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 23.143354,
     "end_time": "2021-04-28T18:11:57.693556",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.550202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = create_testlike_train(train, test)\n",
    "del(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function reduce_mem_usage downcasts datatypes to reduce memory usage, which is necessary to prevent memory overflow errors in the Kaggle notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 23.143354,
     "end_time": "2021-04-28T18:11:57.693556",
     "exception": false,
     "start_time": "2021-04-28T18:11:34.550202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 405.44 MB\n",
      "Memory usage after optimization is: 152.04 MB\n",
      "Decreased by 62.5%\n"
     ]
    }
   ],
   "source": [
    "matrix = reduce_mem_usage(matrix, silent=False)\n",
    "oldcols = matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051209,
     "end_time": "2021-04-28T18:11:57.796356",
     "exception": false,
     "start_time": "2021-04-28T18:11:57.745147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature engineering  \n",
    "In this section predictor feature columns are generated and added to the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050933,
     "end_time": "2021-04-28T18:11:57.90026",
     "exception": false,
     "start_time": "2021-04-28T18:11:57.849327",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Item name groups with fuzzywuzzy\n",
    "\n",
    "Items in the items table are ordered alphabetically according to the item_name field, so that similar items are generally listed next to each other. For example, the first two items in the table below are the same game \"Fuse\" for two different consoles, followed by two different licensing options for the same internet security program. This ordering can be used to help group related items together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.079826,
     "end_time": "2021-04-28T18:11:58.030271",
     "exception": false,
     "start_time": "2021-04-28T18:11:57.950445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3565</th>\n",
       "      <td>Fuse [PS3, английская версия]</td>\n",
       "      <td>3565</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>Fuse [Xbox 360, английская версия]</td>\n",
       "      <td>3566</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3567</th>\n",
       "      <td>G Data Internet Security 2013 (1ПК / 1 год) (G...</td>\n",
       "      <td>3567</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>G Data Internet Security 2013 (3ПК / 1 год) (G...</td>\n",
       "      <td>3568</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3569</th>\n",
       "      <td>GABIN  The Best Of Gabin  2CD</td>\n",
       "      <td>3569</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              item_name  item_id  \\\n",
       "3565                      Fuse [PS3, английская версия]     3565   \n",
       "3566                 Fuse [Xbox 360, английская версия]     3566   \n",
       "3567  G Data Internet Security 2013 (1ПК / 1 год) (G...     3567   \n",
       "3568  G Data Internet Security 2013 (3ПК / 1 год) (G...     3568   \n",
       "3569                      GABIN  The Best Of Gabin  2CD     3569   \n",
       "\n",
       "      item_category_id  \n",
       "3565                19  \n",
       "3566                23  \n",
       "3567                76  \n",
       "3568                76  \n",
       "3569                55  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items.query(\"item_id>3564\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell groups similar items together by sequentially looping through items, measuring the similarity of the names of ajacent items using the string matching package fuzzywuzzy (https://github.com/seatgeek/fuzzywuzzy), and assigning items to the same group if their match value is above a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 10.183104,
     "end_time": "2021-04-28T18:12:08.265598",
     "exception": false,
     "start_time": "2021-04-28T18:11:58.082494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def add_item_name_groups(matrix, train, items, sim_thresh, feature_name=\"item_name_group\"):\n",
    "    def partialmatchgroups(items, sim_thresh=sim_thresh):\n",
    "        def strip_brackets(string):\n",
    "            string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "            string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "            return string\n",
    "\n",
    "        items = items.copy()\n",
    "        items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "        items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "\n",
    "        def partialcompare(s):\n",
    "            return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "\n",
    "        items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "        # Assign groups\n",
    "        grp = 0\n",
    "        for i in range(items.shape[0]):\n",
    "            items.loc[i, \"partialmatchgroup\"] = grp\n",
    "            if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "                grp += 1\n",
    "        items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "        return items\n",
    "\n",
    "    items = partialmatchgroups(items)\n",
    "    items = items.rename(columns={\"partialmatchgroup\": feature_name})\n",
    "    items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n",
    "\n",
    "    items[feature_name] = items[feature_name].apply(str)\n",
    "    items[feature_name] = items[feature_name].factorize()[0]\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "    train = train.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "    return matrix, train\n",
    "\n",
    "\n",
    "matrix, train = add_item_name_groups(matrix, train, items, 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050709,
     "end_time": "2021-04-28T18:12:08.36996",
     "exception": false,
     "start_time": "2021-04-28T18:12:08.319251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Music artist / first word of item name  \n",
    "\n",
    "This function assigns music items into groups according to the artist name, which is extracted from the item name with regular expressions according to 3 common patterns used to indicate the artist name (all uppercase, separated from the release title by a doublespace, or separated by dot-space (. ).  \n",
    "For non-music categories, the items are grouped according to the first word in the item name instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 2.989192,
     "end_time": "2021-04-28T18:12:11.411023",
     "exception": false,
     "start_time": "2021-04-28T18:12:08.421831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def add_first_word_features(matrix, items=items, feature_name=\"artist_name_or_first_word\"):\n",
    "    # This extracts artist names for music categories and adds them as a feature.\n",
    "    def extract_artist(st):\n",
    "        st = st.strip()\n",
    "        if st.startswith(\"V/A\"):\n",
    "            artist = \"V/A\"\n",
    "        elif st.startswith(\"СБ\"):\n",
    "            artist = \"СБ\"\n",
    "        else:\n",
    "            # Retrieves artist names using the double space or all uppercase pattern\n",
    "            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n",
    "            match_dubspace = mus_artist_dubspace.match(st)\n",
    "            mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n",
    "            match_capsonly = mus_artist_capsonly.match(st)\n",
    "            candidates = [match_dubspace, match_capsonly]\n",
    "            candidates = [m[0] for m in candidates if m is not None]\n",
    "            # Sometimes one of the patterns catches some extra words so choose the shortest one\n",
    "            if len(candidates):\n",
    "                artist = min(candidates, key=len)\n",
    "            else:\n",
    "                # If neither of the previous patterns found something, use the dot-space pattern\n",
    "                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n",
    "                match = mus_artist_dotspace.match(st)\n",
    "                if match:\n",
    "                    artist = match[0]\n",
    "                else:\n",
    "                    artist = \"\"\n",
    "        artist = artist.upper()\n",
    "        artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n",
    "        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n",
    "        artist = artist.strip()\n",
    "        return artist\n",
    "\n",
    "    items = items.copy()\n",
    "    all_stopwords = stopwords.words(\"russian\")\n",
    "    all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "    def first_word(string):\n",
    "        # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n",
    "        string = re.sub(r\"[^\\w\\s]\", \"\", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        tokens = string.lower().split()\n",
    "        tokens = [t for t in tokens if t not in all_stopwords]\n",
    "        token = tokens[0] if len(tokens) > 0 else \"\"\n",
    "        return token\n",
    "\n",
    "    music_categories = [55, 56, 57, 58, 59, 60]\n",
    "    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[\n",
    "        items.item_category_id.isin(music_categories), \"item_name\"\n",
    "    ].apply(extract_artist)\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other music\"\n",
    "    items.loc[~items.item_category_id.isin(music_categories), feature_name] = items.loc[\n",
    "        ~items.item_category_id.isin(music_categories), \"item_name\"\n",
    "    ].apply(first_word)\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other non-music\"\n",
    "    items[feature_name] = items[feature_name].factorize()[0]\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\",)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = add_first_word_features(\n",
    "    matrix, items=items, feature_name=\"artist_name_or_first_word\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051751,
     "end_time": "2021-04-28T18:12:11.513824",
     "exception": false,
     "start_time": "2021-04-28T18:12:11.462073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Item name length as a feature\n",
    "The name of the item_name field is surprisingly predictive, presumably because similar items often have similar length names. This is recorded both from the raw item name and the name cleaned of special characters and bracketed terms, which often contain information about release formats that obscure similarities between items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 1.574616,
     "end_time": "2021-04-28T18:12:13.140382",
     "exception": false,
     "start_time": "2021-04-28T18:12:11.565766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_item_name(string):\n",
    "    # Removes bracketed terms, special characters and extra whitespace\n",
    "    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "items[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\n",
    "items[\"item_name_length\"] = items[\"item_name\"].apply(len)\n",
    "matrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')\n",
    "items = items.drop(columns=['item_name_length', 'item_name_cleaned_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 1.963919,
     "end_time": "2021-04-28T18:12:15.155549",
     "exception": false,
     "start_time": "2021-04-28T18:12:13.19163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created name features\n"
     ]
    }
   ],
   "source": [
    "print(\"Created name features\")\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.051242,
     "end_time": "2021-04-28T18:12:15.258377",
     "exception": false,
     "start_time": "2021-04-28T18:12:15.207135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Time features\n",
    "Day and month-resolution time features are created from the training dataframe, e.g. number of days since the first and last sale of each item.\n",
    "\n",
    "The time since the first sale of each items is also used to create a mean sales-per-day feature (\"item_cnt_day_avg\"), which is potentially useful to correct sales counts for items which are less than a month old and therefore were not available to buy for the entire preceding month.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": 0.086102,
     "end_time": "2021-04-28T18:12:15.39602",
     "exception": false,
     "start_time": "2021-04-28T18:12:15.309918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import holidays\n",
    "\n",
    "def add_time_features(m, train, correct_item_cnt_day=False):\n",
    "    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n",
    "\n",
    "    def item_shop_age_months(m):\n",
    "        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(\n",
    "            lambda x: x - x.min()\n",
    "        )\n",
    "        # Sales tend to plateau after 12 months\n",
    "        m[\"new_item\"] = m[\"item_age\"] == 0\n",
    "        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n",
    "        m[\"shop_age\"] = (\n",
    "            m.groupby(\"shop_id\")[\"date_block_num\"]\n",
    "            .transform(lambda x: x - x.min())\n",
    "            .astype(\"int8\")\n",
    "        )\n",
    "        return m\n",
    "\n",
    "    # Add dummy values for the test month so that features are created correctly\n",
    "    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n",
    "    dummies = dummies.assign(\n",
    "        date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,\n",
    "    )\n",
    "    train = pd.concat([train, dummies])\n",
    "    del dummies\n",
    "\n",
    "    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n",
    "    month_last_day[~month_last_day.dt.is_month_end] = (\n",
    "        month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n",
    "    )\n",
    "    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n",
    "    month_first_day[~month_first_day.dt.is_month_start] = (\n",
    "        month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n",
    "    )\n",
    "    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n",
    "    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n",
    "    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n",
    "    first_shop_item_date = (\n",
    "        train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n",
    "    )\n",
    "    first_item_name_group_date = (\n",
    "        train.groupby(\"item_name_group\").date.min().rename(\"first_name_group_date\")\n",
    "    )\n",
    "\n",
    "    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(\n",
    "        first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\"\n",
    "    )\n",
    "    m = m.merge(\n",
    "        first_item_name_group_date, left_on=\"item_name_group\", right_index=True, how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n",
    "    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n",
    "    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n",
    "    m[\"item_in_shop_days\"] = (\n",
    "        m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n",
    "    )\n",
    "    m = m.drop(columns=\"item_first_sale_days\")\n",
    "    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] / m[\"item_in_shop_days\"]\n",
    "    m[\"month_length\"] = m[\"month_length\"].dt.days\n",
    "\n",
    "    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n",
    "    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n",
    "    m[\"first_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n",
    "    m[\"first_shop_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n",
    "    m[\"first_name_group_sale_days\"] = m[\"month_first_day\"] - m[\"first_name_group_date\"]\n",
    "    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n",
    "    m[\"first_item_sale_days\"] = (\n",
    "        m[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n",
    "    )\n",
    "    m[\"first_shop_item_sale_days\"] = (\n",
    "        m[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n",
    "    )\n",
    "    m[\"first_name_group_sale_days\"] = (\n",
    "        m[\"first_name_group_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n",
    "    )\n",
    "\n",
    "    # Add days since last sale\n",
    "    def last_sale_days(matrix):\n",
    "        last_shop_item_dates = []\n",
    "        for dbn in range(1, 35):\n",
    "            lsid_temp = (\n",
    "                train.query(f\"date_block_num<{dbn}\")\n",
    "                .groupby([\"shop_id\", \"item_id\"])\n",
    "                .date.max()\n",
    "                .rename(\"last_shop_item_sale_date\")\n",
    "                .reset_index()\n",
    "            )\n",
    "            lsid_temp[\"date_block_num\"] = dbn\n",
    "            last_shop_item_dates.append(lsid_temp)\n",
    "\n",
    "        last_shop_item_dates = pd.concat(last_shop_item_dates)\n",
    "        matrix = matrix.merge(\n",
    "            last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n",
    "            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n",
    "            m.loc[m[feat_name] > 2000, feat_name] = missingval\n",
    "            m.loc[m[feat_name].isna(), feat_name] = missingval\n",
    "            return m\n",
    "\n",
    "        matrix = days_since_last_feat(\n",
    "            matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999\n",
    "        )\n",
    "\n",
    "        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\"])\n",
    "        return matrix\n",
    "\n",
    "    def add_seasons_and_weekends(matrix, train):\n",
    "        date_blocks = pd.DataFrame(train.groupby(\"date_block_num\").agg({\"date\": \"min\"}))\n",
    "\n",
    "        def add_season(col):\n",
    "            if 0 < col <= 2 or col == 12:\n",
    "                return 'winter'\n",
    "            if 3 <= col < 6:\n",
    "                return 'spring'\n",
    "            if 6 <= col < 9:\n",
    "                return 'summer'\n",
    "            if 9 <= col < 12:\n",
    "                return 'autumn'\n",
    "            return np.nan\n",
    "\n",
    "        # get number of holidays + weekend in each month    \n",
    "        ru_holidays = []\n",
    "\n",
    "        for date, name in sorted(holidays.RU(years=[2013, 2014, 2015]).items()):\n",
    "            ru_holidays.append(date)\n",
    "\n",
    "        def add_holidays_and_weekends(row):\n",
    "            business_dates = pd.bdate_range(f\"{row.year}-{row.month}-01\", f\"{row.year}-{row.month}-{row.days_in_month}\")\n",
    "            business_dates = [b for b in business_dates if b not in ru_holidays]\n",
    "            return row.days_in_month - len(business_dates)\n",
    "\n",
    "        date_blocks[\"year\"] = date_blocks.date.dt.year\n",
    "        date_blocks[\"month\"] = date_blocks.date.dt.month\n",
    "        date_blocks[\"days_in_month\"] = date_blocks.date.dt.daysinmonth.astype(np.int8) \n",
    "        date_blocks[\"season\"] = date_blocks.month.apply(add_season)\n",
    "        date_blocks[\"holidays_and_weekends_in_month\"] = date_blocks.apply(add_holidays_and_weekends, axis=1).astype(np.int8)\n",
    "        date_blocks.drop([\"date\", \"year\", \"month\", \"days_in_month\"], axis=1, inplace=True)\n",
    "\n",
    "        matrix = matrix.merge(date_blocks, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "\n",
    "        return matrix\n",
    "    \n",
    "    m = last_sale_days(m)\n",
    "    # Month id feature\n",
    "    m[\"month\"] = m[\"month_first_day\"].dt.month\n",
    "\n",
    "    m = m.drop(\n",
    "        columns=[\n",
    "            \"first_day\",\n",
    "            \"month_first_day\",\n",
    "            \"month_last_day\",\n",
    "            \"first_shop_date\",\n",
    "            \"first_item_date\",\n",
    "            \"first_name_group_date\",\n",
    "            \"item_in_shop_days\",\n",
    "            \"first_shop_item_date\",\n",
    "            \"month_length\",\n",
    "        ],\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "\n",
    "    m = item_shop_age_months(m)\n",
    "\n",
    "    if correct_item_cnt_day == True:\n",
    "        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n",
    "        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n",
    "\n",
    "    m = add_seasons_and_weekends(m, train)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "papermill": {
     "duration": 44.568578,
     "end_time": "2021-04-28T18:13:00.017133",
     "exception": false,
     "start_time": "2021-04-28T18:12:15.448555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time features created\n"
     ]
    }
   ],
   "source": [
    "matrix = add_time_features(matrix, train, False)\n",
    "print(\"Time features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052584,
     "end_time": "2021-04-28T18:13:00.379709",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.327125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Price features  \n",
    "\n",
    "The price of the item in the last month in which it was sold, and its price relative to other items in the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": 0.067926,
     "end_time": "2021-04-28T18:13:00.501112",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.433186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_price_features(matrix, train):\n",
    "    # Get mean prices per month from train dataframe\n",
    "    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n",
    "    price_features = pd.DataFrame(price_features)\n",
    "    price_features = price_features.reset_index()\n",
    "    # Calculate normalized differenced from mean category price per month\n",
    "    price_features = price_features.merge(\n",
    "        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n",
    "    )\n",
    "    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n",
    "        [\"date_block_num\", \"item_category_id\"]\n",
    "    )[\"item_price\"].transform(lambda x: (x - x.mean()) / x.mean())\n",
    "    # Retain only the necessary features\n",
    "    price_features = price_features[\n",
    "        [\n",
    "            \"date_block_num\",\n",
    "            \"item_id\",\n",
    "            \"item_price\",\n",
    "            \"norm_diff_cat_price\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        \"item_price\",\n",
    "        \"norm_diff_cat_price\",\n",
    "    ]\n",
    "    newnames = [\"last_\" + f for f in features]\n",
    "    aggs = {f: \"last\" for f in features}\n",
    "    renames = {f: \"last_\" + f for f in features}\n",
    "    features = []\n",
    "    for dbn in range(1, 35):\n",
    "        f_temp = (\n",
    "            price_features.query(f\"date_block_num<{dbn}\")\n",
    "            .groupby(\"item_id\")\n",
    "            .agg(aggs)\n",
    "            .rename(columns=renames)\n",
    "        )\n",
    "        f_temp[\"date_block_num\"] = dbn\n",
    "        features.append(f_temp)\n",
    "    features = pd.concat(features).reset_index()\n",
    "    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "papermill": {
     "duration": 4.30382,
     "end_time": "2021-04-28T18:13:04.85857",
     "exception": false,
     "start_time": "2021-04-28T18:13:00.55475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = add_price_features(matrix, train)\n",
    "del(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052081,
     "end_time": "2021-04-28T18:13:04.964343",
     "exception": false,
     "start_time": "2021-04-28T18:13:04.912262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Item category features  \n",
    "In addition to the item categories provided with the data, I also manually defined two additional category groupings - supercategory (e.g. \"games\", \"music\") and platform (e.g. \"PS4\", \"mp3\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = matrix.merge(items[['item_id', 'item_category_id']], on='item_id', how='left')\n",
    "\n",
    "platform_map = {\n",
    "    0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 8, 10: 1, 11: 2,\n",
    "    12: 3, 13: 4, 14: 5, 15: 6, 16: 7, 17: 8, 18: 1, 19: 2, 20: 3, 21: 4, 22: 5,\n",
    "    23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 0, 29: 0, 30: 0, 31: 0, 32: 8, 33: 11,\n",
    "    34: 11, 35: 3, 36: 0, 37: 12, 38: 12, 39: 12, 40: 13, 41: 13, 42: 14, 43: 15,\n",
    "    44: 15, 45: 15, 46: 14, 47: 14, 48: 14, 49: 14, 50: 14, 51: 14, 52: 14, 53: 14,\n",
    "    54: 8, 55: 16, 56: 16, 57: 17, 58: 18, 59: 13, 60: 16, 61: 8, 62: 8, 63: 8, 64: 8,\n",
    "    65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 8, 73: 0, 74: 10, 75: 0,\n",
    "    76: 0, 77: 0, 78: 0, 79: 8, 80: 8, 81: 8, 82: 8, 83: 8,\n",
    "}\n",
    "matrix['platform_id'] = matrix['item_category_id'].map(platform_map)\n",
    "\n",
    "supercat_map = {\n",
    "    0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 1, 12: 1,\n",
    "    13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3,\n",
    "    24: 3, 25: 0, 26: 2, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 2, 33: 2, 34: 2,\n",
    "    35: 2, 36: 2, 37: 4, 38: 4, 39: 4, 40: 4, 41: 4, 42: 5, 43: 5, 44: 5, 45: 5,\n",
    "    46: 5, 47: 5, 48: 5, 49: 5, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 6, 56: 6,\n",
    "    57: 6, 58: 6, 59: 6, 60: 6, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0,\n",
    "    68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 7, 74: 7, 75: 7, 76: 7, 77: 7, 78: 7,\n",
    "    79: 2, 80: 2, 81: 0, 82: 0, 83: 0\n",
    "}\n",
    "matrix['supercategory_id'] = matrix['item_category_id'].map(supercat_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.054288,
     "end_time": "2021-04-28T18:13:08.800954",
     "exception": false,
     "start_time": "2021-04-28T18:13:08.746666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shop city and shop type\n",
    "(from https://www.kaggle.com/dlarionov/feature-engineering-xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "papermill": {
     "duration": 1.896395,
     "end_time": "2021-04-28T18:13:10.753735",
     "exception": false,
     "start_time": "2021-04-28T18:13:08.85734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_shop_type(col):\n",
    "    if 'ТЦ' in col:\n",
    "        return 1\n",
    "    if 'ТРЦ' in col or 'МТРЦ' in col:\n",
    "        return 2\n",
    "    if 'ТК' in col:\n",
    "        return 3\n",
    "    if 'ТРК' in col:\n",
    "        return 4\n",
    "    for c in ['Якутск Орджоникидзе', 'Жуковский', 'Воронеж (Плехановская, 13)', 'Магазин С21']:\n",
    "        if c in col:\n",
    "            return 5\n",
    "    return 0\n",
    "\n",
    "def add_shop_feats(matrix, shops):\n",
    "    shops.loc[\n",
    "        shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n",
    "    ] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n",
    "    shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
    "    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n",
    "    \n",
    "    shops[\"shop_type\"] = shops.shop_name.apply(add_shop_type)\n",
    "    \n",
    "    shop_labels = shops[[\"shop_id\", \"city_code\", \"shop_type\"]]\n",
    "    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n",
    "    return matrix\n",
    "\n",
    "matrix = add_shop_feats(matrix, shops)\n",
    "del(shops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053194,
     "end_time": "2021-04-28T18:13:10.860623",
     "exception": false,
     "start_time": "2021-04-28T18:13:10.807429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Shop and item category clustering\n",
    "\n",
    "Shops and item categories are grouped into clusters according to their sales profiles. \n",
    "The following function performs and plots the results of a principle component analysis decomposition and clustering of the shops and item categories.\n",
    "\n",
    "The proportion of explained variance between items explained by each of the PCA dimensions is plotted, and the individual items are plotted according to their scores on the PCA dimensions and coloured according to their cluster assignment.\n",
    "\n",
    "The silhouette score (a metric of clustering quality) for different values of the cluster number parameter is also plotted. These plots were used to decide the number of clusters.\n",
    "\n",
    "For both shops and item categories, over 80% of differences occur on a single dimension, indicating that differences are mainly in magnitude rather than proportion. The item component score plots show that the clustering is mainly into a large cluster containing the large majority of items, and a few clusters containing outlier items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "papermill": {
     "duration": 0.29023,
     "end_time": "2021-04-28T18:13:11.205754",
     "exception": false,
     "start_time": "2021-04-28T18:13:10.915524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def cluster_feature(matrix, target_feature, clust_feature, level_feature, n_components=4, n_clusters=5, aggfunc=\"mean\", exclude=None):\n",
    "    start_month = 20\n",
    "    end_month = 32\n",
    "    pt = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\n",
    "    if exclude is not None:\n",
    "        pt = matrix[~matrix[clust_feature].isin(exclude)]\n",
    "    pt = pt.pivot_table(values=target_feature, columns=clust_feature, index=level_feature, fill_value=0, aggfunc=aggfunc)\n",
    "    pt = pt.transpose()\n",
    "    pca = PCA(n_components=10)\n",
    "    components = pca.fit_transform(pt)\n",
    "    components = pd.DataFrame(components)\n",
    "    # Plot PCA explained variance\n",
    "    sns.set_theme()\n",
    "    features = list(range(pca.n_components_))\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "#     ax.bar(features, pca.explained_variance_ratio_, color=\"black\")\n",
    "    sns.barplot(x=features, y=pca.explained_variance_ratio_, ax=ax)\n",
    "    plt.title(\"Variance by PCA components\")\n",
    "    plt.xlabel(\"component\")\n",
    "    plt.ylabel(\"explained variance\")\n",
    "    plt.xticks(features)\n",
    "\n",
    "    scorelist = []\n",
    "    nrange = range(2, 10)\n",
    "    for n in nrange:\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n)\n",
    "        labels = clusterer.fit_predict(components)\n",
    "        silscore = silhouette_score(pt, labels)\n",
    "        scorelist.append(silscore)\n",
    "    ax = fig.add_subplot(122)\n",
    "    sns.lineplot(x=nrange, y=scorelist, ax=ax)\n",
    "    plt.title(\"Clustering quality by number of clusters\")\n",
    "    plt.xlabel(\"n clusters\")\n",
    "    plt.ylabel(\"silhouette score\")\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(pt)\n",
    "    components = pd.DataFrame(components)\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n",
    "    labels = clusterer.fit_predict(components)\n",
    "    x = components[0]\n",
    "    y = components[1]\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.scatterplot(x=x, y=y, hue=labels, palette=sns.color_palette(\"hls\", n_clusters), ax=ax)\n",
    "    plt.title(\"Items by cluster\")\n",
    "    plt.xlabel(\"component 1 score\")\n",
    "    plt.ylabel(\"component 2 score\")\n",
    "    for i, txt in enumerate(pt.index.to_list()):\n",
    "        ax.annotate(str(txt), (x[i], y[i]))\n",
    "    groups = {}\n",
    "    for i, s in enumerate(pt.index):\n",
    "        groups[s] = labels[i]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.053194,
     "end_time": "2021-04-28T18:13:10.860623",
     "exception": false,
     "start_time": "2021-04-28T18:13:10.807429",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Item categories are clustered according to their mean sales in each month of the year. The principle component plot shows that 3 categories are outliers in this respect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "papermill": {
     "duration": 3.980666,
     "end_time": "2021-04-28T18:13:15.23941",
     "exception": false,
     "start_time": "2021-04-28T18:13:11.258744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "category_group_dict = cluster_feature(matrix, 'item_cnt_month', 'item_category_id', 'date_block_num', n_components=2, n_clusters=4, aggfunc=\"mean\", exclude =[])\n",
    "matrix['category_cluster'] = matrix['item_category_id'].map(category_group_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shops are clustered by their summed sales of each item category. The principle component plots show that shops mainly differ in the magnitude of their sales, with shop 31 being an outlier due to the volume of its sales. Shops 12 and 55 are outliers on an orthogonal dimension because they sell different (online only) items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "papermill": {
     "duration": 3.477294,
     "end_time": "2021-04-28T18:13:18.77379",
     "exception": false,
     "start_time": "2021-04-28T18:13:15.296496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shop_group_dict = cluster_feature(matrix, 'item_cnt_month', 'shop_id', 'item_category_id', n_components=4, n_clusters=4, aggfunc=\"mean\", exclude=[36])\n",
    "shop_group_dict[36] = shop_group_dict[37]  # Shop36 added separately because it only has one month of data\n",
    "matrix['shop_cluster'] = matrix['shop_id'].map(shop_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "papermill": {
     "duration": 9.033337,
     "end_time": "2021-04-28T18:13:27.865241",
     "exception": false,
     "start_time": "2021-04-28T18:13:18.831904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058948,
     "end_time": "2021-04-28T18:13:27.984695",
     "exception": false,
     "start_time": "2021-04-28T18:13:27.925747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Number of unique item features\n",
    "\n",
    "These features count the number of unique items sharing the same value of a grouping feature or set of features as the current item in the current month, e.g. number of new items in the same category.  \n",
    "\n",
    "This could considered to be a kind of data leakage feature, as the set of items in each month (and therefore the test set) is determined by whether each item recorded a sale or not in the month being predicted, which isn't known in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "papermill": {
     "duration": 1.661615,
     "end_time": "2021-04-28T18:14:25.413563",
     "exception": false,
     "start_time": "2021-04-28T18:14:23.751948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uniques(matrix, groupers, name, limitation=None):\n",
    "    if limitation is not None:\n",
    "        s = (\n",
    "            matrix.query(limitation)\n",
    "            .groupby(groupers)\n",
    "            .item_id.nunique()\n",
    "            .rename(name)\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        s = matrix.groupby(groupers).item_id.nunique().rename(name).reset_index()\n",
    "    matrix = matrix.merge(s, on=groupers, how=\"left\")\n",
    "    matrix[name] = matrix[name].fillna(0)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\"], \"unique_items_month\")\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_name_group\"], \"name_group_unique_month\")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n",
    "    \"name_group_cat_unique_month\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_name_group\"],\n",
    "    \"name_group_new_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n",
    "    \"name_group_new_cat_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "\n",
    "matrix = uniques(\n",
    "    matrix, [\"date_block_num\", \"artist_name_or_first_word\"], \"first_word_unique_month\"\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n",
    "    \"first_word_cat_unique_month\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"artist_name_or_first_word\"],\n",
    "    \"first_word_new_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n",
    "    \"first_word_new_cat_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\"], \"unique_items_cat\")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\"],\n",
    "    \"new_items_cat\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "matrix = uniques(matrix, [\"date_block_num\"], \"new_items_month\", limitation=\"new_item==True\")\n",
    "\n",
    "matrix[\"cat_items_proportion\"] = matrix[\"unique_items_cat\"] / matrix[\"unique_items_month\"]\n",
    "matrix[\"name_group_new_proportion_month\"] = (\n",
    "    matrix[\"name_group_new_unique_month\"] / matrix[\"name_group_unique_month\"]\n",
    ")\n",
    "\n",
    "matrix = matrix.drop(columns=[\"unique_items_month\", \"name_group_unique_month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "papermill": {
     "duration": 5.877312,
     "end_time": "2021-04-28T18:14:31.349756",
     "exception": false,
     "start_time": "2021-04-28T18:14:25.472444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058252,
     "end_time": "2021-04-28T18:14:31.467011",
     "exception": false,
     "start_time": "2021-04-28T18:14:31.408759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Percentage change in an aggregate feature  \n",
    "This uses the pandas pct_change method to calculate the proportional change in mean sales count for a specific grouping for a specific time interval, e.g. increase / decrease in mean sales of an item between the last 2 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "papermill": {
     "duration": 0.074182,
     "end_time": "2021-04-28T18:14:31.600743",
     "exception": false,
     "start_time": "2021-04-28T18:14:31.526561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_pct_change(\n",
    "    matrix,\n",
    "    group_feats,\n",
    "    target=\"item_cnt_month\",\n",
    "    aggfunc=\"mean\",\n",
    "    periods=1,\n",
    "    lag=1,\n",
    "    clip_value=None,\n",
    "):\n",
    "    periods = list_if_not(periods, int)\n",
    "    group_feats = list_if_not(group_feats)\n",
    "    group_feats_full = [\"date_block_num\"] + group_feats\n",
    "    dat = matrix.pivot_table(\n",
    "        index=group_feats + [\"date_block_num\"],\n",
    "        values=target,\n",
    "        aggfunc=aggfunc,\n",
    "        fill_value=0,\n",
    "        dropna=False,\n",
    "    ).astype(\"float32\")\n",
    "    for g in group_feats:\n",
    "        firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "        dat = dat.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "        dat.loc[dat.index.get_level_values(\"date_block_num\") < dat[\"firsts\"], target] = float(\n",
    "            \"nan\"\n",
    "        )\n",
    "        del dat[\"firsts\"]\n",
    "    for period in periods:\n",
    "        feat_name = \"_\".join(\n",
    "            group_feats + [target] + [aggfunc] + [\"delta\"] + [str(period)] + [f\"lag_{lag}\"]\n",
    "        )\n",
    "        print(f\"Adding feature {feat_name}\")\n",
    "        dat = (\n",
    "            dat.groupby(group_feats)[target]\n",
    "            .transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\"))\n",
    "            .rename(feat_name)\n",
    "        )\n",
    "        if clip_value is not None:\n",
    "            dat = dat.clip(lower=-clip_value, upper=clip_value)\n",
    "    dat = dat.reset_index()\n",
    "    dat[\"date_block_num\"] += lag\n",
    "    matrix = matrix.merge(dat, on=[\"date_block_num\"] + group_feats, how=\"left\")\n",
    "    matrix[feat_name] = reduce_mem_usage(matrix[feat_name])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "papermill": {
     "duration": 55.239396,
     "end_time": "2021-04-28T18:15:26.899005",
     "exception": false,
     "start_time": "2021-04-28T18:14:31.659609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feature item_id_item_cnt_month_mean_delta_1_lag_1\n",
      "Adding feature item_category_id_item_cnt_month_mean_delta_1_lag_1\n",
      "Adding feature item_name_group_item_cnt_month_mean_delta_1_lag_1\n",
      "Adding feature item_category_id_item_cnt_month_mean_delta_1_lag_12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_name_group\"], \"item_cnt_month\", clip_value=3)\n",
    "# Delta 1 feature lagged by 12 months, intended to capture seasonal trends\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "papermill": {
     "duration": 1.450712,
     "end_time": "2021-04-28T18:15:28.412578",
     "exception": false,
     "start_time": "2021-04-28T18:15:26.961866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063575,
     "end_time": "2021-04-28T18:15:28.537486",
     "exception": false,
     "start_time": "2021-04-28T18:15:28.473911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Windowed aggregates\n",
    "\n",
    "Features aggregated over a specific window to reduce noise. Available windows are expanding (i.e. all preceding timepoints), rolling (i.e. fixed number of equally weighted timepoints) and exponentially weighted mean.  \n",
    "\n",
    "\n",
    "A note about feature names: these are set automatically according to the pattern < grouping features > - < aggregated features > - < monthly aggregation function > - < window type > , where < window type > is either \"rolling - < window aggregation function > - win - < window length in months >\" for square rolling windows, \"expanding - < window aggregation function >\" for expanding windows, and \"ewm_hl - < decay rate in terms of half-life > for exponential weighted means, all connected by underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "papermill": {
     "duration": 0.452093,
     "end_time": "2021-04-28T18:15:29.05061",
     "exception": false,
     "start_time": "2021-04-28T18:15:28.598517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shop_id = 16\n",
    "item_id = 482\n",
    "im = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\n",
    "im['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\n",
    "im['expanding mean'] = im['item_cnt_month'].expanding().mean()\n",
    "im['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\n",
    "im = im.set_index('date_block_num')\n",
    "ax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "papermill": {
     "duration": 0.088969,
     "end_time": "2021-04-28T18:15:29.204666",
     "exception": false,
     "start_time": "2021-04-28T18:15:29.115697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_rolling_stats(\n",
    "    matrix,\n",
    "    features,\n",
    "    window=12,\n",
    "    kind=\"rolling\",\n",
    "    argfeat=\"item_cnt_month\",\n",
    "    aggfunc=\"mean\",\n",
    "    rolling_aggfunc=\"mean\",\n",
    "    dtype=\"float16\",\n",
    "    reshape_source=True,\n",
    "    lag_offset=0,\n",
    "):\n",
    "    def rolling_stat(\n",
    "        matrix,\n",
    "        source,\n",
    "        feats,\n",
    "        feat_name,\n",
    "        window=12,\n",
    "        argfeat=\"item_cnt_month\",\n",
    "        aggfunc=\"mean\",\n",
    "        dtype=dtype,\n",
    "        lag_offset=0,\n",
    "    ):\n",
    "        # Calculate a statistic on a windowed section of a source table,  grouping on specific features\n",
    "        store = []\n",
    "        for i in range(2 + lag_offset, 35 + lag_offset):\n",
    "            if len(feats) > 0:\n",
    "                mes = (\n",
    "                    source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n",
    "                    .groupby(feats)[argfeat]\n",
    "                    .agg(aggfunc)\n",
    "                    .astype(dtype)\n",
    "                    .rename(feat_name)\n",
    "                    .reset_index()\n",
    "                )\n",
    "            else:\n",
    "                mes = {}\n",
    "                mes[feat_name] = (\n",
    "                    source.loc[\n",
    "                        source.date_block_num.isin(range(max([i - window, 0]), i)), argfeat\n",
    "                    ]\n",
    "                    .agg(aggfunc)\n",
    "                    .astype(dtype)\n",
    "                )\n",
    "                mes = pd.DataFrame(data=mes, index=[i])\n",
    "            mes[\"date_block_num\"] = i - lag_offset\n",
    "            store.append(mes)\n",
    "        store = pd.concat(store)\n",
    "        matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n",
    "        return matrix\n",
    "\n",
    "    \"\"\" An issue when using windowed functions is that missing values from months when items recorded no sales are skipped rather than being correctly\n",
    "    treated as zeroes. Creating a pivot_table fills in the zeros.\"\"\"\n",
    "    if (reshape_source == True) or (kind == \"ewm\"):\n",
    "        source = matrix.pivot_table(\n",
    "            index=features + [\"date_block_num\"],\n",
    "            values=argfeat,\n",
    "            aggfunc=aggfunc,\n",
    "            fill_value=0,\n",
    "            dropna=False,\n",
    "        ).astype(dtype)\n",
    "        for g in features:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            # Set values before the items first appearance to nan so they are ignored rather than being treated as zero sales.\n",
    "            source.loc[\n",
    "                source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat\n",
    "            ] = float(\"nan\")\n",
    "            del source[\"firsts\"]\n",
    "        source = source.reset_index()\n",
    "    else:\n",
    "        source = matrix\n",
    "\n",
    "    if kind == \"rolling\":\n",
    "        feat_name = (\n",
    "            f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n",
    "        )\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(\n",
    "            matrix,\n",
    "            source,\n",
    "            features,\n",
    "            feat_name,\n",
    "            window=window,\n",
    "            argfeat=argfeat,\n",
    "            aggfunc=rolling_aggfunc,\n",
    "            dtype=dtype,\n",
    "            lag_offset=lag_offset,\n",
    "        )\n",
    "    elif kind == \"expanding\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(\n",
    "            matrix,\n",
    "            source,\n",
    "            features,\n",
    "            feat_name,\n",
    "            window=100,\n",
    "            argfeat=argfeat,\n",
    "            aggfunc=aggfunc,\n",
    "            dtype=dtype,\n",
    "            lag_offset=lag_offset,\n",
    "        )\n",
    "    elif kind == \"ewm\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        source[feat_name] = (\n",
    "            source.groupby(features)[argfeat]\n",
    "            .ewm(halflife=window, min_periods=1)\n",
    "            .agg(rolling_aggfunc)\n",
    "            .to_numpy(dtype=dtype)\n",
    "        )\n",
    "        del source[argfeat]\n",
    "        #         source = source.reset_index()\n",
    "        source[\"date_block_num\"] += 1 - lag_offset\n",
    "        return matrix.merge(source, on=[\"date_block_num\"] + features, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063375,
     "end_time": "2021-04-28T18:15:29.331871",
     "exception": false,
     "start_time": "2021-04-28T18:15:29.268496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Create rolling mean features. The combinations of grouping features and window types here were selected by generating a large number of features and then pruning them with the scikit-learn RFECV function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "papermill": {
     "duration": 1905.447107,
     "end_time": "2021-04-28T18:47:14.842926",
     "exception": false,
     "start_time": "2021-04-28T18:15:29.395819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature \"shop_id_artist_name_or_first_word_item_category_id_item_age_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_id_artist_name_or_first_word_item_category_id_new_item_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"shop_id_artist_name_or_first_word_new_item_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"shop_id_category_cluster_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_id_item_category_id_item_age_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"shop_id_item_category_id_item_age_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_id_item_category_id_item_cnt_month_mean_ewm_hl_1\"\n",
      "Creating feature \"shop_id_item_category_id_new_item_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"shop_id_item_category_id_new_item_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_id_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_id_item_id_item_cnt_month_mean_ewm_hl_1\"\n",
      "Creating feature \"shop_id_item_id_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_id_item_name_group_item_category_id_new_item_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_id_item_name_group_new_item_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"shop_id_supercategory_id_new_item_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"shop_cluster_item_id_item_cnt_month_mean_ewm_hl_1\"\n",
      "Creating feature \"shop_cluster_item_category_id_item_age_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"shop_cluster_item_name_group_new_item_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"category_cluster_item_age_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"category_cluster_new_item_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"item_id_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"artist_name_or_first_word_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"artist_name_or_first_word_item_cnt_month_mean_ewm_hl_1\"\n",
      "Creating feature \"artist_name_or_first_word_item_age_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"artist_name_or_first_word_item_category_id_item_age_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"artist_name_or_first_word_new_item_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"item_category_id_item_age_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"item_category_id_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"item_category_id_item_cnt_month_mean_ewm_hl_1\"\n",
      "Creating feature \"item_category_id_new_item_item_cnt_month_mean_expanding_mean\"\n",
      "Creating feature \"item_name_group_item_age_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"item_name_group_item_cnt_month_mean_ewm_hl_1\"\n",
      "Creating feature \"item_name_group_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"platform_id_item_cnt_month_mean_rolling_mean_win_12\"\n",
      "Creating feature \"platform_id_item_cnt_month_mean_ewm_hl_1\"\n"
     ]
    }
   ],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"category_cluster\"], window=12)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], window=12)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\", \"item_category_id\", \"new_item\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"supercategory_id\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"shop_cluster\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_cluster\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_cluster\", \"item_name_group\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"category_cluster\", \"item_age\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"category_cluster\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"item_id\"], window=12)\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"artist_name_or_first_word\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_category_id\", \"item_age\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_category_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"item_category_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_category_id\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_name_group\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_name_group\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(matrix, [\"item_name_group\"], window=12)\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], kind=\"ewm\", window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "papermill": {
     "duration": 17.789665,
     "end_time": "2021-04-28T18:47:32.707445",
     "exception": false,
     "start_time": "2021-04-28T18:47:14.91778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrixcheckpoint\n",
      "Widnow aggregate features created\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\n",
    "\n",
    "matrix.to_pickle(\"matrixcheckpoint1.pkl\")\n",
    "print(\"Saved matrixcheckpoint\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"Widnow aggregate features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.448793,
     "end_time": "2021-04-28T18:47:36.713076",
     "exception": false,
     "start_time": "2021-04-28T18:47:36.264283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following code block calculates windowed mean sales features with day resolution accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "papermill": {
     "duration": 92.446203,
     "end_time": "2021-04-28T18:49:09.252422",
     "exception": false,
     "start_time": "2021-04-28T18:47:36.806219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature \"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"\n",
      "Creating feature \"item_id_item_cnt_month_sum_expanding_sum\"\n"
     ]
    }
   ],
   "source": [
    "# Summed sales & accurate windowed mean sales per day features\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_id\"],\n",
    "    aggfunc=\"sum\",\n",
    "    rolling_aggfunc=\"sum\",\n",
    "    kind=\"rolling\",\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_id\"],\n",
    "    aggfunc=\"sum\",\n",
    "    rolling_aggfunc=\"sum\",\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix[\"1year\"] = 365\n",
    "matrix[\"item_id_day_mean_expanding\"] = matrix[\n",
    "    \"item_id_item_cnt_month_sum_expanding_sum\"\n",
    "] / matrix[[\"first_item_sale_days\"]].min(axis=1)\n",
    "matrix[\"shop_id_item_id_day_mean_win_12\"] = matrix[\n",
    "    \"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"\n",
    "] / matrix[[\"first_item_sale_days\", \"shop_open_days\", \"1year\"]].min(axis=1)\n",
    "matrix.loc[matrix.new_item == True, \"item_id_day_mean_expanding\",] = float(\"nan\")\n",
    "matrix = matrix.drop(columns=[\"1year\", \"item_id_item_cnt_month_sum_expanding_sum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.217115,
     "end_time": "2021-04-28T18:49:09.640666",
     "exception": false,
     "start_time": "2021-04-28T18:49:09.423551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Revenue features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "papermill": {
     "duration": 91.965703,
     "end_time": "2021-04-28T18:50:41.781069",
     "exception": false,
     "start_time": "2021-04-28T18:49:09.815366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature \"shop_id_item_name_group_item_revenue_month_mean_rolling_mean_win_12\"\n"
     ]
    }
   ],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\"],\n",
    "    window=12,\n",
    "    argfeat=\"item_revenue_month\",\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.090066,
     "end_time": "2021-04-28T18:50:41.952101",
     "exception": false,
     "start_time": "2021-04-28T18:50:41.862035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Windowed mean unique item features and ratio of new items in category with mean over the previous year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "papermill": {
     "duration": 28.595028,
     "end_time": "2021-04-28T18:51:10.635694",
     "exception": false,
     "start_time": "2021-04-28T18:50:42.040666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating feature \"item_category_id_new_items_cat_mean_rolling_mean_win_12\"\n",
      "Creating feature \"item_name_group_name_group_new_unique_month_mean_rolling_mean_win_12\"\n"
     ]
    }
   ],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_category_id\"],\n",
    "    argfeat=\"new_items_cat\",\n",
    "    window=12,\n",
    "    reshape_source=True,\n",
    "    lag_offset=1,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_name_group\"],\n",
    "    argfeat=\"name_group_new_unique_month\",\n",
    "    window=12,\n",
    "    reshape_source=True,\n",
    "    lag_offset=1,\n",
    ")\n",
    "\n",
    "matrix[\"new_items_cat_1_12_ratio\"] = (\n",
    "    matrix[\"new_items_cat\"]\n",
    "    / matrix[\"item_category_id_new_items_cat_mean_rolling_mean_win_12\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "papermill": {
     "duration": 7.661105,
     "end_time": "2021-04-28T18:51:18.734463",
     "exception": false,
     "start_time": "2021-04-28T18:51:11.073358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.108926,
     "end_time": "2021-04-28T18:51:19.025566",
     "exception": false,
     "start_time": "2021-04-28T18:51:18.91664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Lagged features and mean encodings  \n",
    "Values for the same shop-item combination from previous months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "papermill": {
     "duration": 0.091696,
     "end_time": "2021-04-28T18:51:19.215776",
     "exception": false,
     "start_time": "2021-04-28T18:51:19.12408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def simple_lag_feature(matrix, lag_feature, lags):\n",
    "    for lag in lags:\n",
    "        newname = lag_feature + f\"_lag_{lag}\"\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n",
    "        targetseries[\"date_block_num\"] += lag\n",
    "        targetseries = targetseries.rename(columns={lag_feature: newname})\n",
    "        matrix = matrix.merge(\n",
    "            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n",
    "        )\n",
    "        matrix.loc[\n",
    "            (matrix.item_age >= lag) & (matrix.shop_age >= lag) & (matrix[newname].isna()),\n",
    "            newname,\n",
    "        ] = 0\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "papermill": {
     "duration": 120.906823,
     "end_time": "2021-04-28T18:53:20.206272",
     "exception": false,
     "start_time": "2021-04-28T18:51:19.299449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feature item_cnt_month_lag_1\n",
      "Adding feature item_cnt_month_lag_2\n",
      "Adding feature item_cnt_month_lag_3\n",
      "Adding feature item_cnt_day_avg_lag_1\n",
      "Adding feature item_cnt_day_avg_lag_2\n",
      "Adding feature item_cnt_day_avg_lag_3\n",
      "Adding feature item_revenue_month_lag_1\n",
      "Lag features created\n"
     ]
    }
   ],
   "source": [
    "matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1, 2, 3])\n",
    "matrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1, 2, 3])\n",
    "matrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\n",
    "gc.collect()\n",
    "print(\"Lag features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084107,
     "end_time": "2021-04-28T18:53:20.373538",
     "exception": false,
     "start_time": "2021-04-28T18:53:20.289431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mean encodings\n",
    "The mean or sum value of a target feature for each level of a categorical feature or combination of categorical features, lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "papermill": {
     "duration": 0.093518,
     "end_time": "2021-04-28T18:53:20.548179",
     "exception": false,
     "start_time": "2021-04-28T18:53:20.454661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_apply_ME(\n",
    "    matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\", aggfunc=\"mean\"\n",
    "):\n",
    "    grouping_fields = list_if_not(grouping_fields)\n",
    "    for lag in lags:\n",
    "        newname = \"_\".join(grouping_fields + [target] + [aggfunc] + [f\"lag_{lag}\"])\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        me_series = (\n",
    "            matrix.groupby([\"date_block_num\"] + grouping_fields)[target]\n",
    "            .agg(aggfunc)\n",
    "            .rename(newname)\n",
    "            .reset_index()\n",
    "        )\n",
    "        me_series[\"date_block_num\"] += lag\n",
    "        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n",
    "        del me_series\n",
    "        matrix[newname] = matrix[newname].fillna(0)\n",
    "        for g in grouping_fields:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            matrix = matrix.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            matrix.loc[\n",
    "                matrix[\"date_block_num\"] < (matrix[\"firsts\"] + (lag)), newname\n",
    "            ] = float(\"nan\")\n",
    "            del matrix[\"firsts\"]\n",
    "        matrix[newname] = reduce_mem_usage(matrix[newname])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "papermill": {
     "duration": 116.204762,
     "end_time": "2021-04-28T18:55:16.832993",
     "exception": false,
     "start_time": "2021-04-28T18:53:20.628231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feature item_name_group_item_cnt_month_mean_lag_1\n",
      "Adding feature item_name_group_item_cnt_month_sum_lag_1\n",
      "Adding feature item_id_item_cnt_month_mean_lag_1\n",
      "Adding feature item_id_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature platform_id_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature item_name_group_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature platform_id_item_cnt_month_mean_lag_1\n",
      "Adding feature supercategory_id_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature item_category_id_new_item_item_cnt_month_mean_lag_1\n",
      "Adding feature shop_id_item_category_id_item_cnt_month_mean_lag_1\n",
      "Adding feature shop_cluster_item_id_item_cnt_month_mean_lag_1\n",
      "Adding feature shop_cluster_item_id_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature city_code_item_id_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature city_code_item_name_group_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature shop_type_item_id_item_cnt_day_avg_mean_lag_1\n",
      "Adding feature shop_type_item_id_item_cnt_month_mean_lag_1\n"
     ]
    }
   ],
   "source": [
    "matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\", aggfunc=\"sum\")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"supercategory_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_id\", \"item_category_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_type\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"shop_type\", \"item_id\"], target=\"item_cnt_month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.085228,
     "end_time": "2021-04-28T18:55:17.004209",
     "exception": false,
     "start_time": "2021-04-28T18:55:16.918981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ratios between lag 1 sales and rolling 12 month means, to capture decreases from previous means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "papermill": {
     "duration": 0.12325,
     "end_time": "2021-04-28T18:55:17.213537",
     "exception": false,
     "start_time": "2021-04-28T18:55:17.090287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_id_item_cnt_month_mean_lag_1\"]\n",
    "    / matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n",
    ")\n",
    "matrix[\"shop_id_item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_cnt_day_avg_lag_1\"] / matrix[\"shop_id_item_id_day_mean_win_12\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "papermill": {
     "duration": 11.498338,
     "end_time": "2021-04-28T18:55:28.796406",
     "exception": false,
     "start_time": "2021-04-28T18:55:17.298068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved matrixcheckpoint\n",
      "Mean encoding features created\n"
     ]
    }
   ],
   "source": [
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\n",
    "matrix.to_pickle(\"matrixcheckpoint2.pkl\")\n",
    "print(\"Saved matrixcheckpoint\")\n",
    "gc.collect()\n",
    "print(\"Mean encoding features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.295401,
     "end_time": "2021-04-28T18:55:46.830211",
     "exception": false,
     "start_time": "2021-04-28T18:55:44.53481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some columns that were used to generate other features can now be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "papermill": {
     "duration": 5.783446,
     "end_time": "2021-04-28T18:55:54.944188",
     "exception": false,
     "start_time": "2021-04-28T18:55:49.160742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "surplus_columns = [\n",
    "    \"item_revenue_month\",\n",
    "    \"item_cnt_day_avg\",\n",
    "    \"item_name_group\",\n",
    "    \"artist_name_or_first_word\",\n",
    "    \"item_age\",\n",
    "    \"shop_open_days\",\n",
    "    \"shop_age\",\n",
    "    \"platform_id\",\n",
    "    \"supercategory_id\",\n",
    "    \"city_code\",\n",
    "    \"category_cluster\",\n",
    "    \"shop_cluster\",\n",
    "    \"shop_type\",\n",
    "    \"new_items_cat\",\n",
    "    \"shop_id_item_id_day_mean_win_12\",\n",
    "    \"item_id_item_cnt_month_mean_rolling_mean_win_12\",\n",
    "]\n",
    "matrix = matrix.drop(columns=surplus_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084959,
     "end_time": "2021-04-28T18:55:55.115949",
     "exception": false,
     "start_time": "2021-04-28T18:55:55.03099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predictive words in item_name\n",
    "\n",
    "One-hot features are made for words in the item_name field that are predictive of item sales.  \n",
    "\n",
    "To select *k* word features from the 1000's of words found in item names, words are discarded if they are not in the names of a threshold number of items, or are not in the names of new items in the test or validation months. Remaining words are then selected by the scikit-learn SelectKBest function according to their correlation with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "papermill": {
     "duration": 0.117744,
     "end_time": "2021-04-28T18:55:55.579227",
     "exception": false,
     "start_time": "2021-04-28T18:55:55.461483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"sklearn\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "def name_token_feats(matrix, items, k=50, item_n_threshold=5, target_month_start=33):\n",
    "    def name_correction(st):\n",
    "        st = re.sub(r\"[^\\w\\s]\", \"\", st)\n",
    "        st = re.sub(r\"\\s{2,}\", \" \", st)\n",
    "        st = st.lower().strip()\n",
    "        return st\n",
    "\n",
    "    items[\"item_name_clean\"] = items[\"item_name\"].apply(name_correction)\n",
    "\n",
    "    def create_item_id_bow_matrix(items):\n",
    "        all_stopwords = stopwords.words(\"russian\")\n",
    "        all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "        vectorizer = CountVectorizer(stop_words=all_stopwords)\n",
    "        X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n",
    "        X = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "        print(f\"{len(vectorizer.vocabulary_)} words found in all items\")\n",
    "        featuremap = {\n",
    "            col: \"word_\" + token\n",
    "            for col, token in zip(\n",
    "                range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names()\n",
    "            )\n",
    "        }\n",
    "        X = X.rename(columns=featuremap)\n",
    "        return X\n",
    "\n",
    "    items_bow = create_item_id_bow_matrix(items)\n",
    "    items_bow = items_bow.clip(0, 1)  # Made the word counts binary\n",
    "    common_word_mask = items_bow.sum(axis=0) > item_n_threshold\n",
    "    target_items = matrix.query(\n",
    "        f\"date_block_num>={target_month_start} & new_item==True\"\n",
    "    ).item_id.unique()\n",
    "    target_item_mask = items_bow.loc[target_items, :].sum(axis=0) > 1\n",
    "    items_bow = items_bow.loc[:, common_word_mask & target_item_mask]\n",
    "    print(f\"{items_bow.shape[1]} words of interest\")\n",
    "    mxbow = matrix[[\"date_block_num\", \"item_id\", \"item_cnt_month\"]].query(\"date_block_num<34\")\n",
    "    mxbow = mxbow.merge(items_bow, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "    X = mxbow.drop(columns=[\"date_block_num\", \"item_id\", \"item_cnt_month\"])\n",
    "    y = mxbow[\"item_cnt_month\"].clip(0, 20)\n",
    "    selektor = SelectKBest(f_regression, k=k)\n",
    "    selektor.fit(X, y)\n",
    "    tokencols = X.columns[selektor.get_support()]\n",
    "    print(f\"{k} word features selected\")\n",
    "    return items_bow[tokencols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "papermill": {
     "duration": 1213.623185,
     "end_time": "2021-04-28T19:16:09.288159",
     "exception": false,
     "start_time": "2021-04-28T18:55:55.664974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19134 words found in all items\n",
      "449 words of interest\n",
      "50 word features selected\n"
     ]
    }
   ],
   "source": [
    "items = pd.read_csv(\"items.csv.zip\")\n",
    "word_frame = name_token_feats(matrix, items, k=50, item_n_threshold=5)\n",
    "matrix = matrix.merge(word_frame, left_on='item_id', right_index=True, how='left')\n",
    "# LightGBM didn't seem to work with sparse features in this case, so we'll convert them to int\n",
    "sparsecols = [c for c in matrix.columns if pd.api.types.is_sparse(matrix[c].dtype)]\n",
    "matrix[sparsecols] = matrix[sparsecols].sparse.to_dense().astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature frame is saved and the notebook kernel is reset to free up memory for LightGBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "papermill": {
     "duration": 8.028085,
     "end_time": "2021-04-28T19:16:22.397834",
     "exception": false,
     "start_time": "2021-04-28T19:16:14.369749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features generated, dataframe saved\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "matrix.to_pickle(\"checkpoint_final.pkl\")\n",
    "print(\"All features generated, dataframe saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "papermill": {
     "duration": 0.44176,
     "end_time": "2021-04-28T19:16:23.862813",
     "exception": false,
     "start_time": "2021-04-28T19:16:23.421053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08846,
     "end_time": "2021-04-28T19:16:24.040872",
     "exception": false,
     "start_time": "2021-04-28T19:16:23.952412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.098249,
     "end_time": "2021-04-28T19:16:24.227277",
     "exception": false,
     "start_time": "2021-04-28T19:16:24.129028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature frame is loaded and the target is clipped to match the test items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\n",
    "# drop first 2 months\n",
    "matrix = matrix[matrix.date_block_num >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:44:57,059]\u001b[0m A new study created in memory with name: no-name-92cb9254-4c8b-4de4-b81a-fd6bf8e72af8\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|                   | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.317404 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.802652\tvalid_1's rmse: 0.735579\n",
      "[100]\tvalid_0's rmse: 0.767314\tvalid_1's rmse: 0.726061\n",
      "[150]\tvalid_0's rmse: 0.747089\tvalid_1's rmse: 0.717437\n",
      "[200]\tvalid_0's rmse: 0.73491\tvalid_1's rmse: 0.715937\n",
      "[250]\tvalid_0's rmse: 0.725592\tvalid_1's rmse: 0.714783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.714146:  14%|8     | 1/7 [01:12<07:17, 72.90s/it]\u001b[32m[I 2021-08-27 13:46:09,971]\u001b[0m Trial 0 finished with value: 0.7141463337709645 and parameters: {'feature_fraction': 0.4}. Best is trial 0 with value: 0.7141463337709645.\u001b[0m\n",
      "feature_fraction, val_score: 0.714146:  14%|8     | 1/7 [01:12<07:17, 72.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300]\tvalid_0's rmse: 0.716956\tvalid_1's rmse: 0.716025\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid_0's rmse: 0.722018\tvalid_1's rmse: 0.714146\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.298833 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.79583\tvalid_1's rmse: 0.725896\n",
      "[100]\tvalid_0's rmse: 0.759987\tvalid_1's rmse: 0.71469\n",
      "[150]\tvalid_0's rmse: 0.741704\tvalid_1's rmse: 0.712693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.711117:  29%|#7    | 2/7 [02:00<04:48, 57.76s/it]\u001b[32m[I 2021-08-27 13:46:57,132]\u001b[0m Trial 1 finished with value: 0.7111165335465969 and parameters: {'feature_fraction': 0.8}. Best is trial 1 with value: 0.7111165335465969.\u001b[0m\n",
      "feature_fraction, val_score: 0.711117:  29%|#7    | 2/7 [02:00<04:48, 57.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's rmse: 0.738138\tvalid_1's rmse: 0.711117\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.317572 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.796561\tvalid_1's rmse: 0.732604\n",
      "[100]\tvalid_0's rmse: 0.758703\tvalid_1's rmse: 0.72238\n",
      "[150]\tvalid_0's rmse: 0.740022\tvalid_1's rmse: 0.718583\n",
      "[200]\tvalid_0's rmse: 0.727775\tvalid_1's rmse: 0.715382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.711117:  43%|##5   | 3/7 [02:52<03:41, 55.41s/it]\u001b[32m[I 2021-08-27 13:47:49,751]\u001b[0m Trial 2 finished with value: 0.714955990810734 and parameters: {'feature_fraction': 1.0}. Best is trial 1 with value: 0.7111165335465969.\u001b[0m\n",
      "feature_fraction, val_score: 0.711117:  43%|##5   | 3/7 [02:52<03:41, 55.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[207]\tvalid_0's rmse: 0.726139\tvalid_1's rmse: 0.714956\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.313539 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.796147\tvalid_1's rmse: 0.731761\n",
      "[100]\tvalid_0's rmse: 0.759518\tvalid_1's rmse: 0.721542\n",
      "[150]\tvalid_0's rmse: 0.740954\tvalid_1's rmse: 0.716679\n",
      "[200]\tvalid_0's rmse: 0.728309\tvalid_1's rmse: 0.715346\n",
      "[250]\tvalid_0's rmse: 0.718207\tvalid_1's rmse: 0.716351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.711117:  57%|###4  | 4/7 [03:46<02:44, 54.83s/it]\u001b[32m[I 2021-08-27 13:48:43,679]\u001b[0m Trial 3 finished with value: 0.7150527501005625 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 1 with value: 0.7111165335465969.\u001b[0m\n",
      "feature_fraction, val_score: 0.711117:  57%|###4  | 4/7 [03:46<02:44, 54.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[222]\tvalid_0's rmse: 0.723681\tvalid_1's rmse: 0.715053\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.292269 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.800601\tvalid_1's rmse: 0.733891\n",
      "[100]\tvalid_0's rmse: 0.763134\tvalid_1's rmse: 0.725954\n",
      "[150]\tvalid_0's rmse: 0.743978\tvalid_1's rmse: 0.720911\n",
      "[200]\tvalid_0's rmse: 0.729848\tvalid_1's rmse: 0.719116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.711117:  71%|####2 | 5/7 [04:41<01:49, 54.99s/it]\u001b[32m[I 2021-08-27 13:49:38,967]\u001b[0m Trial 4 finished with value: 0.7177829034691917 and parameters: {'feature_fraction': 0.6}. Best is trial 1 with value: 0.7111165335465969.\u001b[0m\n",
      "feature_fraction, val_score: 0.711117:  71%|####2 | 5/7 [04:41<01:49, 54.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[193]\tvalid_0's rmse: 0.731607\tvalid_1's rmse: 0.717783\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.295058 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.799158\tvalid_1's rmse: 0.727887\n",
      "[100]\tvalid_0's rmse: 0.764186\tvalid_1's rmse: 0.717028\n",
      "[150]\tvalid_0's rmse: 0.745654\tvalid_1's rmse: 0.714541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.711117:  86%|#####1| 6/7 [05:31<00:53, 53.16s/it]\u001b[32m[I 2021-08-27 13:50:28,572]\u001b[0m Trial 5 finished with value: 0.7142179660809053 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.7111165335465969.\u001b[0m\n",
      "feature_fraction, val_score: 0.711117:  86%|#####1| 6/7 [05:31<00:53, 53.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[133]\tvalid_0's rmse: 0.751198\tvalid_1's rmse: 0.714218\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.298319 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.799152\tvalid_1's rmse: 0.73385\n",
      "[100]\tvalid_0's rmse: 0.761956\tvalid_1's rmse: 0.724978\n",
      "[150]\tvalid_0's rmse: 0.743768\tvalid_1's rmse: 0.720511\n",
      "[200]\tvalid_0's rmse: 0.731849\tvalid_1's rmse: 0.718093\n",
      "[250]\tvalid_0's rmse: 0.722053\tvalid_1's rmse: 0.715545\n",
      "[300]\tvalid_0's rmse: 0.713026\tvalid_1's rmse: 0.7143\n",
      "[350]\tvalid_0's rmse: 0.704643\tvalid_1's rmse: 0.714474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.711117: 100%|######| 7/7 [06:37<00:00, 57.45s/it]\u001b[32m[I 2021-08-27 13:51:34,864]\u001b[0m Trial 6 finished with value: 0.7140524964439173 and parameters: {'feature_fraction': 0.7}. Best is trial 1 with value: 0.7111165335465969.\u001b[0m\n",
      "feature_fraction, val_score: 0.711117: 100%|######| 7/7 [06:37<00:00, 56.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[331]\tvalid_0's rmse: 0.707494\tvalid_1's rmse: 0.714052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:   0%|                   | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.298125 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.801578\tvalid_1's rmse: 0.736309\n",
      "[100]\tvalid_0's rmse: 0.764107\tvalid_1's rmse: 0.725817\n",
      "[150]\tvalid_0's rmse: 0.745352\tvalid_1's rmse: 0.721307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:   5%|5          | 1/20 [00:43<13:54, 43.91s/it]\u001b[32m[I 2021-08-27 13:52:18,785]\u001b[0m Trial 7 finished with value: 0.7205064649556713 and parameters: {'num_leaves': 29}. Best is trial 7 with value: 0.7205064649556713.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:   5%|5          | 1/20 [00:43<13:54, 43.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[140]\tvalid_0's rmse: 0.7487\tvalid_1's rmse: 0.720506\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.288982 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.712765\tvalid_1's rmse: 0.722927\n",
      "[100]\tvalid_0's rmse: 0.672603\tvalid_1's rmse: 0.720382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  10%|#1         | 2/20 [01:33<14:08, 47.15s/it]\u001b[32m[I 2021-08-27 13:53:08,196]\u001b[0m Trial 8 finished with value: 0.7196176423140166 and parameters: {'num_leaves': 188}. Best is trial 8 with value: 0.7196176423140166.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  10%|#1         | 2/20 [01:33<14:08, 47.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[83]\tvalid_0's rmse: 0.682315\tvalid_1's rmse: 0.719618\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.303195 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.808912\tvalid_1's rmse: 0.739706\n",
      "[100]\tvalid_0's rmse: 0.773376\tvalid_1's rmse: 0.728735\n",
      "[150]\tvalid_0's rmse: 0.755668\tvalid_1's rmse: 0.723423\n",
      "[200]\tvalid_0's rmse: 0.742424\tvalid_1's rmse: 0.722057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  15%|#6         | 3/20 [02:22<13:39, 48.23s/it]\u001b[32m[I 2021-08-27 13:53:57,722]\u001b[0m Trial 9 finished with value: 0.7217926848223519 and parameters: {'num_leaves': 24}. Best is trial 8 with value: 0.7196176423140166.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  15%|#6         | 3/20 [02:22<13:39, 48.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[203]\tvalid_0's rmse: 0.741668\tvalid_1's rmse: 0.721793\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291636 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.723275\tvalid_1's rmse: 0.722445\n",
      "[100]\tvalid_0's rmse: 0.685051\tvalid_1's rmse: 0.718961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  20%|##2        | 4/20 [03:18<13:37, 51.07s/it]\u001b[32m[I 2021-08-27 13:54:53,130]\u001b[0m Trial 10 finished with value: 0.7170265209989374 and parameters: {'num_leaves': 152}. Best is trial 10 with value: 0.7170265209989374.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  20%|##2        | 4/20 [03:18<13:37, 51.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\tvalid_0's rmse: 0.662356\tvalid_1's rmse: 0.717422\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's rmse: 0.673951\tvalid_1's rmse: 0.717027\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.314710 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.717634\tvalid_1's rmse: 0.727559\n",
      "[100]\tvalid_0's rmse: 0.678458\tvalid_1's rmse: 0.725272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  25%|##7        | 5/20 [04:11<12:56, 51.79s/it]\u001b[32m[I 2021-08-27 13:55:46,192]\u001b[0m Trial 11 finished with value: 0.7247582635915362 and parameters: {'num_leaves': 167}. Best is trial 10 with value: 0.7170265209989374.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  25%|##7        | 5/20 [04:11<12:56, 51.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's rmse: 0.681106\tvalid_1's rmse: 0.724758\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.302676 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.708676\tvalid_1's rmse: 0.721104\n",
      "[100]\tvalid_0's rmse: 0.668099\tvalid_1's rmse: 0.717209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  30%|###3       | 6/20 [05:07<12:26, 53.30s/it]\u001b[32m[I 2021-08-27 13:56:42,418]\u001b[0m Trial 12 finished with value: 0.7169225819723208 and parameters: {'num_leaves': 204}. Best is trial 12 with value: 0.7169225819723208.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  30%|###3       | 6/20 [05:07<12:26, 53.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's rmse: 0.670166\tvalid_1's rmse: 0.716923\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.311547 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.75059\tvalid_1's rmse: 0.720254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  35%|###8       | 7/20 [05:47<10:36, 48.95s/it]\u001b[32m[I 2021-08-27 13:57:22,421]\u001b[0m Trial 13 finished with value: 0.7168052326270762 and parameters: {'num_leaves': 83}. Best is trial 13 with value: 0.7168052326270762.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  35%|###8       | 7/20 [05:47<10:36, 48.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[60]\tvalid_0's rmse: 0.739928\tvalid_1's rmse: 0.716805\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.294688 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.727935\tvalid_1's rmse: 0.723298\n",
      "[100]\tvalid_0's rmse: 0.688623\tvalid_1's rmse: 0.718879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  40%|####4      | 8/20 [06:39<09:58, 49.83s/it]\u001b[32m[I 2021-08-27 13:58:14,147]\u001b[0m Trial 14 finished with value: 0.7188003203221527 and parameters: {'num_leaves': 136}. Best is trial 13 with value: 0.7168052326270762.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  40%|####4      | 8/20 [06:39<09:58, 49.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[104]\tvalid_0's rmse: 0.686517\tvalid_1's rmse: 0.7188\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.290935 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.70399\tvalid_1's rmse: 0.718056\n",
      "[100]\tvalid_0's rmse: 0.664512\tvalid_1's rmse: 0.715178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  45%|####9      | 9/20 [07:35<09:29, 51.79s/it]\u001b[32m[I 2021-08-27 13:59:10,222]\u001b[0m Trial 15 finished with value: 0.714883074375145 and parameters: {'num_leaves': 224}. Best is trial 15 with value: 0.714883074375145.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  45%|####9      | 9/20 [07:35<09:29, 51.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's rmse: 0.661662\tvalid_1's rmse: 0.714883\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.292246 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.700679\tvalid_1's rmse: 0.723555\n",
      "[100]\tvalid_0's rmse: 0.658769\tvalid_1's rmse: 0.719025\n",
      "[150]\tvalid_0's rmse: 0.63566\tvalid_1's rmse: 0.71735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  50%|#####     | 10/20 [08:38<09:11, 55.18s/it]\u001b[32m[I 2021-08-27 14:00:13,019]\u001b[0m Trial 16 finished with value: 0.7169122633665905 and parameters: {'num_leaves': 245}. Best is trial 15 with value: 0.714883074375145.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  50%|#####     | 10/20 [08:38<09:11, 55.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[136]\tvalid_0's rmse: 0.640591\tvalid_1's rmse: 0.716912\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.287458 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.703543\tvalid_1's rmse: 0.716509\n",
      "[100]\tvalid_0's rmse: 0.663188\tvalid_1's rmse: 0.712534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  55%|#####5    | 11/20 [09:30<08:07, 54.19s/it]\u001b[32m[I 2021-08-27 14:01:04,970]\u001b[0m Trial 17 finished with value: 0.7119302419683886 and parameters: {'num_leaves': 227}. Best is trial 17 with value: 0.7119302419683886.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  55%|#####5    | 11/20 [09:30<08:07, 54.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[86]\tvalid_0's rmse: 0.672042\tvalid_1's rmse: 0.71193\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.287514 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.698113\tvalid_1's rmse: 0.725738\n",
      "[100]\tvalid_0's rmse: 0.656554\tvalid_1's rmse: 0.725357\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's rmse: 0.676874\tvalid_1's rmse: 0.724947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  60%|######    | 12/20 [10:19<07:02, 52.81s/it]\u001b[32m[I 2021-08-27 14:01:54,625]\u001b[0m Trial 18 finished with value: 0.7249465353436575 and parameters: {'num_leaves': 251}. Best is trial 17 with value: 0.7119302419683886.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  60%|######    | 12/20 [10:19<07:02, 52.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.289249 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.705596\tvalid_1's rmse: 0.725355\n",
      "[100]\tvalid_0's rmse: 0.665742\tvalid_1's rmse: 0.720987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  65%|######5   | 13/20 [11:08<06:00, 51.57s/it]\u001b[32m[I 2021-08-27 14:02:43,331]\u001b[0m Trial 19 finished with value: 0.7202212379270576 and parameters: {'num_leaves': 217}. Best is trial 17 with value: 0.7119302419683886.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  65%|######5   | 13/20 [11:08<06:00, 51.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's rmse: 0.681858\tvalid_1's rmse: 0.720221\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.292450 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.750885\tvalid_1's rmse: 0.718497\n",
      "[100]\tvalid_0's rmse: 0.71562\tvalid_1's rmse: 0.713373\n",
      "[150]\tvalid_0's rmse: 0.695276\tvalid_1's rmse: 0.713188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  70%|#######   | 14/20 [11:56<05:03, 50.57s/it]\u001b[32m[I 2021-08-27 14:03:31,601]\u001b[0m Trial 20 finished with value: 0.7124869588248104 and parameters: {'num_leaves': 82}. Best is trial 17 with value: 0.7119302419683886.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  70%|#######   | 14/20 [11:56<05:03, 50.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's rmse: 0.704293\tvalid_1's rmse: 0.712487\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.302741 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.747715\tvalid_1's rmse: 0.719906\n",
      "[100]\tvalid_0's rmse: 0.710462\tvalid_1's rmse: 0.715408\n",
      "[150]\tvalid_0's rmse: 0.691395\tvalid_1's rmse: 0.713517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  75%|#######5  | 15/20 [12:45<04:10, 50.05s/it]\u001b[32m[I 2021-08-27 14:04:20,434]\u001b[0m Trial 21 finished with value: 0.7131934850811696 and parameters: {'num_leaves': 89}. Best is trial 17 with value: 0.7119302419683886.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  75%|#######5  | 15/20 [12:45<04:10, 50.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's rmse: 0.696007\tvalid_1's rmse: 0.713193\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.303535 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.742004\tvalid_1's rmse: 0.727409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  80%|########  | 16/20 [13:23<03:05, 46.47s/it]\u001b[32m[I 2021-08-27 14:04:58,592]\u001b[0m Trial 22 finished with value: 0.7260903365781487 and parameters: {'num_leaves': 98}. Best is trial 17 with value: 0.7119302419683886.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  80%|########  | 16/20 [13:23<03:05, 46.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[54]\tvalid_0's rmse: 0.737054\tvalid_1's rmse: 0.72609\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.287660 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.765521\tvalid_1's rmse: 0.721198\n",
      "[100]\tvalid_0's rmse: 0.72948\tvalid_1's rmse: 0.712096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  85%|########5 | 17/20 [14:05<02:15, 45.04s/it]\u001b[32m[I 2021-08-27 14:05:40,288]\u001b[0m Trial 23 finished with value: 0.7119051203085789 and parameters: {'num_leaves': 59}. Best is trial 23 with value: 0.7119051203085789.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  85%|########5 | 17/20 [14:05<02:15, 45.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[99]\tvalid_0's rmse: 0.729976\tvalid_1's rmse: 0.711905\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.290050 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.875867\tvalid_1's rmse: 0.786395\n",
      "[100]\tvalid_0's rmse: 0.848207\tvalid_1's rmse: 0.765745\n",
      "[150]\tvalid_0's rmse: 0.830905\tvalid_1's rmse: 0.758222\n",
      "[200]\tvalid_0's rmse: 0.817434\tvalid_1's rmse: 0.749932\n",
      "[250]\tvalid_0's rmse: 0.809045\tvalid_1's rmse: 0.743853\n",
      "[300]\tvalid_0's rmse: 0.801328\tvalid_1's rmse: 0.739076\n",
      "[350]\tvalid_0's rmse: 0.795157\tvalid_1's rmse: 0.734006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  90%|######### | 18/20 [14:57<01:34, 47.26s/it]\u001b[32m[I 2021-08-27 14:06:32,735]\u001b[0m Trial 24 finished with value: 0.7338445208695838 and parameters: {'num_leaves': 6}. Best is trial 23 with value: 0.7119051203085789.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  90%|######### | 18/20 [14:57<01:34, 47.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[349]\tvalid_0's rmse: 0.795231\tvalid_1's rmse: 0.733845\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.293490 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.768618\tvalid_1's rmse: 0.726678\n",
      "[100]\tvalid_0's rmse: 0.732501\tvalid_1's rmse: 0.718225\n",
      "[150]\tvalid_0's rmse: 0.713616\tvalid_1's rmse: 0.716359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.711117:  95%|#########5| 19/20 [15:46<00:47, 47.61s/it]\u001b[32m[I 2021-08-27 14:07:21,176]\u001b[0m Trial 25 finished with value: 0.7156812473550327 and parameters: {'num_leaves': 56}. Best is trial 23 with value: 0.7119051203085789.\u001b[0m\n",
      "num_leaves, val_score: 0.711117:  95%|#########5| 19/20 [15:46<00:47, 47.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's rmse: 0.71182\tvalid_1's rmse: 0.715681\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.292291 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.73601\tvalid_1's rmse: 0.717842\n",
      "[100]\tvalid_0's rmse: 0.697647\tvalid_1's rmse: 0.711213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.710346: 100%|##########| 20/20 [16:31<00:00, 46.94s/it]\u001b[32m[I 2021-08-27 14:08:06,555]\u001b[0m Trial 26 finished with value: 0.7103456309364898 and parameters: {'num_leaves': 115}. Best is trial 26 with value: 0.7103456309364898.\u001b[0m\n",
      "num_leaves, val_score: 0.710346: 100%|##########| 20/20 [16:31<00:00, 49.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[95]\tvalid_0's rmse: 0.70053\tvalid_1's rmse: 0.710346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.710346:   0%|                      | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.286745 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.28 MB) transferred to GPU in 0.256377 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.244125 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.01 MB) transferred to GPU in 0.248463 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.08 MB) transferred to GPU in 0.248571 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.96 MB) transferred to GPU in 0.237072 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.15 MB) transferred to GPU in 0.254763 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.05 MB) transferred to GPU in 0.241960 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.247828 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.99 MB) transferred to GPU in 0.237066 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.09 MB) transferred to GPU in 0.251375 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.05 MB) transferred to GPU in 0.247136 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.00 MB) transferred to GPU in 0.238142 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.11 MB) transferred to GPU in 0.258237 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.08 MB) transferred to GPU in 0.259452 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.06 MB) transferred to GPU in 0.238430 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.09 MB) transferred to GPU in 0.239244 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.13 MB) transferred to GPU in 0.239354 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.12 MB) transferred to GPU in 0.238980 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.05 MB) transferred to GPU in 0.235665 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.14 MB) transferred to GPU in 0.240280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.238034 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.06 MB) transferred to GPU in 0.240336 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.06 MB) transferred to GPU in 0.237042 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.04 MB) transferred to GPU in 0.238522 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.10 MB) transferred to GPU in 0.238771 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.735859\tvalid_1's rmse: 0.718906\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.03 MB) transferred to GPU in 0.238988 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.238255 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.21 MB) transferred to GPU in 0.239150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.23 MB) transferred to GPU in 0.239427 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.16 MB) transferred to GPU in 0.239226 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.16 MB) transferred to GPU in 0.238991 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.08 MB) transferred to GPU in 0.238859 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.17 MB) transferred to GPU in 0.238115 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.04 MB) transferred to GPU in 0.239108 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.88 MB) transferred to GPU in 0.247557 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.08 MB) transferred to GPU in 0.249347 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.04 MB) transferred to GPU in 0.238752 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.00 MB) transferred to GPU in 0.236687 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.237947 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.96 MB) transferred to GPU in 0.248250 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.08 MB) transferred to GPU in 0.240072 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.240123 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.239579 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.97 MB) transferred to GPU in 0.239811 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.10 MB) transferred to GPU in 0.240411 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.08 MB) transferred to GPU in 0.239277 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.05 MB) transferred to GPU in 0.239631 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.10 MB) transferred to GPU in 0.238559 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.235423 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.04 MB) transferred to GPU in 0.237829 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.698844\tvalid_1's rmse: 0.714939\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.79 MB) transferred to GPU in 0.236797 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.02 MB) transferred to GPU in 0.237227 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.05 MB) transferred to GPU in 0.232954 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.05 MB) transferred to GPU in 0.237281 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.02 MB) transferred to GPU in 0.237456 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.04 MB) transferred to GPU in 0.238539 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.11 MB) transferred to GPU in 0.236976 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.15 MB) transferred to GPU in 0.237841 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.06 MB) transferred to GPU in 0.238015 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.04 MB) transferred to GPU in 0.235410 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.11 MB) transferred to GPU in 0.237500 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.01 MB) transferred to GPU in 0.238314 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.07 MB) transferred to GPU in 0.237734 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.00 MB) transferred to GPU in 0.235827 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.98 MB) transferred to GPU in 0.237701 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.10 MB) transferred to GPU in 0.237183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.04 MB) transferred to GPU in 0.238533 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (485.69 MB) transferred to GPU in 0.237173 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.03 MB) transferred to GPU in 0.238010 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.02 MB) transferred to GPU in 0.236980 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.14 MB) transferred to GPU in 0.238448 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.03 MB) transferred to GPU in 0.239120 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.12 MB) transferred to GPU in 0.238593 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.16 MB) transferred to GPU in 0.237737 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (486.05 MB) transferred to GPU in 0.238716 secs. 1 sparse feature groups\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's rmse: 0.689813\tvalid_1's rmse: 0.713954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.710346:  10%|#4            | 1/10 [01:37<14:37, 97.51s/it]\u001b[32m[I 2021-08-27 14:09:44,070]\u001b[0m Trial 27 finished with value: 0.7139535293806785 and parameters: {'bagging_fraction': 0.8088018463088382, 'bagging_freq': 2}. Best is trial 27 with value: 0.7139535293806785.\u001b[0m\n",
      "bagging, val_score: 0.710346:  10%|#4            | 1/10 [01:37<14:37, 97.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.302144 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.25 MB) transferred to GPU in 0.129645 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.01 MB) transferred to GPU in 0.127208 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.94 MB) transferred to GPU in 0.127991 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.02 MB) transferred to GPU in 0.129334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.00 MB) transferred to GPU in 0.128192 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.05 MB) transferred to GPU in 0.127150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.00 MB) transferred to GPU in 0.127798 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.04 MB) transferred to GPU in 0.129149 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.10 MB) transferred to GPU in 0.129437 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.04 MB) transferred to GPU in 0.128365 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.03 MB) transferred to GPU in 0.127159 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.97 MB) transferred to GPU in 0.127465 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.96 MB) transferred to GPU in 0.129709 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738456\tvalid_1's rmse: 0.722406\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.00 MB) transferred to GPU in 0.130197 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.03 MB) transferred to GPU in 0.127772 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.06 MB) transferred to GPU in 0.128186 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.11 MB) transferred to GPU in 0.130151 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.12 MB) transferred to GPU in 0.127429 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.93 MB) transferred to GPU in 0.127891 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.09 MB) transferred to GPU in 0.127335 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.03 MB) transferred to GPU in 0.129334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.98 MB) transferred to GPU in 0.130905 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.00 MB) transferred to GPU in 0.129958 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.01 MB) transferred to GPU in 0.127539 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.06 MB) transferred to GPU in 0.129987 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.704838\tvalid_1's rmse: 0.720786\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.94 MB) transferred to GPU in 0.128457 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.99 MB) transferred to GPU in 0.131128 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.86 MB) transferred to GPU in 0.128793 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.78 MB) transferred to GPU in 0.127734 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.20 MB) transferred to GPU in 0.128270 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.72 MB) transferred to GPU in 0.128865 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.07 MB) transferred to GPU in 0.128348 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.69 MB) transferred to GPU in 0.127533 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.97 MB) transferred to GPU in 0.129020 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.91 MB) transferred to GPU in 0.128903 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.01 MB) transferred to GPU in 0.128144 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.94 MB) transferred to GPU in 0.128068 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.98 MB) transferred to GPU in 0.128056 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.68701\tvalid_1's rmse: 0.72125\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.00 MB) transferred to GPU in 0.129210 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.77 MB) transferred to GPU in 0.127504 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.07 MB) transferred to GPU in 0.129112 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (266.98 MB) transferred to GPU in 0.137711 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (267.01 MB) transferred to GPU in 0.139907 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.710346:  20%|##8           | 2/10 [02:43<10:32, 79.08s/it]\u001b[32m[I 2021-08-27 14:10:50,243]\u001b[0m Trial 28 finished with value: 0.7195396789101567 and parameters: {'bagging_fraction': 0.4442777597546684, 'bagging_freq': 4}. Best is trial 27 with value: 0.7139535293806785.\u001b[0m\n",
      "bagging, val_score: 0.710346:  20%|##8           | 2/10 [02:43<10:32, 79.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[139]\tvalid_0's rmse: 0.690201\tvalid_1's rmse: 0.71954\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291184 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.83 MB) transferred to GPU in 0.180987 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.51 MB) transferred to GPU in 0.177054 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.53 MB) transferred to GPU in 0.178449 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.53 MB) transferred to GPU in 0.182489 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.45 MB) transferred to GPU in 0.178274 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.65 MB) transferred to GPU in 0.178827 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.52 MB) transferred to GPU in 0.179849 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.51 MB) transferred to GPU in 0.179404 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.44 MB) transferred to GPU in 0.178342 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.55 MB) transferred to GPU in 0.179254 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736981\tvalid_1's rmse: 0.727726\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.53 MB) transferred to GPU in 0.177714 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.52 MB) transferred to GPU in 0.179682 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.51 MB) transferred to GPU in 0.179442 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.49 MB) transferred to GPU in 0.179638 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.49 MB) transferred to GPU in 0.179690 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.58 MB) transferred to GPU in 0.179789 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.59 MB) transferred to GPU in 0.179219 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.60 MB) transferred to GPU in 0.179498 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.49 MB) transferred to GPU in 0.178411 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.60 MB) transferred to GPU in 0.178175 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.702107\tvalid_1's rmse: 0.725313\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.55 MB) transferred to GPU in 0.179502 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.46 MB) transferred to GPU in 0.177412 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.48 MB) transferred to GPU in 0.179007 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (373.50 MB) transferred to GPU in 0.180752 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.710346:  30%|####2         | 3/10 [03:40<08:03, 69.13s/it]\u001b[32m[I 2021-08-27 14:11:47,543]\u001b[0m Trial 29 finished with value: 0.7247651365806167 and parameters: {'bagging_fraction': 0.6215118146699007, 'bagging_freq': 5}. Best is trial 27 with value: 0.7139535293806785.\u001b[0m\n",
      "bagging, val_score: 0.710346:  30%|####2         | 3/10 [03:40<08:03, 69.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's rmse: 0.707181\tvalid_1's rmse: 0.724765\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.303422 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.45 MB) transferred to GPU in 0.143208 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.15 MB) transferred to GPU in 0.144616 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.11 MB) transferred to GPU in 0.136261 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.18 MB) transferred to GPU in 0.135212 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.10 MB) transferred to GPU in 0.136518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.31 MB) transferred to GPU in 0.136163 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.13 MB) transferred to GPU in 0.136631 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.17 MB) transferred to GPU in 0.135702 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.21 MB) transferred to GPU in 0.135682 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.740284\tvalid_1's rmse: 0.723448\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.18 MB) transferred to GPU in 0.136618 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.19 MB) transferred to GPU in 0.135489 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.10 MB) transferred to GPU in 0.135150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.10 MB) transferred to GPU in 0.138065 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.10 MB) transferred to GPU in 0.137287 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.12 MB) transferred to GPU in 0.135498 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (283.22 MB) transferred to GPU in 0.136163 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.710346:  40%|#####6        | 4/10 [04:25<05:57, 59.57s/it]\u001b[32m[I 2021-08-27 14:12:32,446]\u001b[0m Trial 30 finished with value: 0.7211424282572546 and parameters: {'bagging_fraction': 0.4711372991557249, 'bagging_freq': 6}. Best is trial 27 with value: 0.7139535293806785.\u001b[0m\n",
      "bagging, val_score: 0.710346:  40%|#####6        | 4/10 [04:25<05:57, 59.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's rmse: 0.729327\tvalid_1's rmse: 0.721142\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.297202 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.196716 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196143 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.208940 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.211300 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.212071 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.209474 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195485 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208318 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206799 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.196082 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194941 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.193397 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195346 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195163 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207859 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.206661 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.207306 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.73732\tvalid_1's rmse: 0.718385\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.206456 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205889 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207673 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.206565 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208044 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.208837 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207662 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206242 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195420 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.193391 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.207933 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.211011 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.196056 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.194450 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196227 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.204297 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.699665\tvalid_1's rmse: 0.709849\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.206979 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.193346 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.193836 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.194415 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194838 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.193981 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196168 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195732 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.193826 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.204654 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.193282 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195834 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.194424 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.205269 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196745 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195757 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.680568\tvalid_1's rmse: 0.707608\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.196077 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207595 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206050 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195219 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.209477 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206457 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208690 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.41 MB) transferred to GPU in 0.193621 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194621 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.206826 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.194362 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.15 MB) transferred to GPU in 0.207267 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.208129 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.206583 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.17 MB) transferred to GPU in 0.195433 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.196246 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.205998 secs. 1 sparse feature groups\n",
      "[200]\tvalid_0's rmse: 0.667062\tvalid_1's rmse: 0.707051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.706255:  50%|#######       | 5/10 [05:58<05:57, 71.50s/it]\u001b[32m[I 2021-08-27 14:14:05,089]\u001b[0m Trial 31 finished with value: 0.7062551245023463 and parameters: {'bagging_fraction': 0.679275144218356, 'bagging_freq': 3}. Best is trial 31 with value: 0.7062551245023463.\u001b[0m\n",
      "bagging, val_score: 0.706255:  50%|#######       | 5/10 [05:58<05:57, 71.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's rmse: 0.67499\tvalid_1's rmse: 0.706255\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291713 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.55 MB) transferred to GPU in 0.135162 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.136749 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.22 MB) transferred to GPU in 0.134762 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.28 MB) transferred to GPU in 0.136414 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.136785 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.42 MB) transferred to GPU in 0.134771 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.136045 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.134672 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.31 MB) transferred to GPU in 0.134817 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.28 MB) transferred to GPU in 0.135142 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.31 MB) transferred to GPU in 0.137258 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.135694 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.20 MB) transferred to GPU in 0.134762 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.22 MB) transferred to GPU in 0.134994 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.136032 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.32 MB) transferred to GPU in 0.135643 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.36 MB) transferred to GPU in 0.135261 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.37 MB) transferred to GPU in 0.135717 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.17 MB) transferred to GPU in 0.134650 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.38 MB) transferred to GPU in 0.134834 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.135195 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.22 MB) transferred to GPU in 0.135616 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.135243 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.136716 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.134620 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.16 MB) transferred to GPU in 0.134822 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.135584 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.06 MB) transferred to GPU in 0.134589 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.98 MB) transferred to GPU in 0.134058 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.44 MB) transferred to GPU in 0.136648 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.96 MB) transferred to GPU in 0.135974 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.28 MB) transferred to GPU in 0.136602 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.94 MB) transferred to GPU in 0.135944 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.135362 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.13 MB) transferred to GPU in 0.136524 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.22 MB) transferred to GPU in 0.134692 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.16 MB) transferred to GPU in 0.136505 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.21 MB) transferred to GPU in 0.137524 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.24 MB) transferred to GPU in 0.138156 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.25 MB) transferred to GPU in 0.135913 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.32 MB) transferred to GPU in 0.136036 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.134937 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.137284 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.18 MB) transferred to GPU in 0.136695 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.135952 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.135487 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.21 MB) transferred to GPU in 0.135260 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.19 MB) transferred to GPU in 0.136622 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.135678 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.137506 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738628\tvalid_1's rmse: 0.73288\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.99 MB) transferred to GPU in 0.136586 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.137030 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.25 MB) transferred to GPU in 0.136242 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.134306 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.135264 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.24 MB) transferred to GPU in 0.138166 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.135557 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.60 MB) transferred to GPU in 0.138325 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.134456 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.20 MB) transferred to GPU in 0.134408 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.28 MB) transferred to GPU in 0.137697 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.21 MB) transferred to GPU in 0.136921 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.135667 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.20 MB) transferred to GPU in 0.136798 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.33 MB) transferred to GPU in 0.134555 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.99 MB) transferred to GPU in 0.135841 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.148760 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.01 MB) transferred to GPU in 0.146993 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.143136 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.24 MB) transferred to GPU in 0.143985 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.97 MB) transferred to GPU in 0.145143 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.15 MB) transferred to GPU in 0.144462 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.25 MB) transferred to GPU in 0.143844 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.54 MB) transferred to GPU in 0.143897 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.142542 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.143908 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.25 MB) transferred to GPU in 0.143745 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.144236 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.28 MB) transferred to GPU in 0.143196 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.32 MB) transferred to GPU in 0.143807 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.144132 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.143571 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.142434 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.147421 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.143494 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.95 MB) transferred to GPU in 0.144672 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.144391 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.44 MB) transferred to GPU in 0.143868 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.00 MB) transferred to GPU in 0.144910 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.21 MB) transferred to GPU in 0.144763 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.142796 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.144668 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.57 MB) transferred to GPU in 0.143725 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.143628 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.19 MB) transferred to GPU in 0.142519 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.143631 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.13 MB) transferred to GPU in 0.143819 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.05 MB) transferred to GPU in 0.142552 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.145069 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.12 MB) transferred to GPU in 0.142658 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.704117\tvalid_1's rmse: 0.728888\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.19 MB) transferred to GPU in 0.144998 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.24 MB) transferred to GPU in 0.143556 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.31 MB) transferred to GPU in 0.142721 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.22 MB) transferred to GPU in 0.144570 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.143183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.50 MB) transferred to GPU in 0.144406 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.143499 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.26 MB) transferred to GPU in 0.142487 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.24 MB) transferred to GPU in 0.143518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.29 MB) transferred to GPU in 0.144846 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.17 MB) transferred to GPU in 0.145279 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.22 MB) transferred to GPU in 0.144272 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.31 MB) transferred to GPU in 0.142975 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.16 MB) transferred to GPU in 0.142698 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.21 MB) transferred to GPU in 0.144518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.45 MB) transferred to GPU in 0.143134 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.16 MB) transferred to GPU in 0.143181 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (280.83 MB) transferred to GPU in 0.141531 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.141805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.21 MB) transferred to GPU in 0.142663 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.24 MB) transferred to GPU in 0.142181 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.135183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.33 MB) transferred to GPU in 0.134924 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.08 MB) transferred to GPU in 0.138076 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.56 MB) transferred to GPU in 0.137135 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.138035 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.27 MB) transferred to GPU in 0.134353 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.20 MB) transferred to GPU in 0.135014 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.25 MB) transferred to GPU in 0.135219 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.30 MB) transferred to GPU in 0.136465 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.31 MB) transferred to GPU in 0.135882 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.23 MB) transferred to GPU in 0.135290 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.22 MB) transferred to GPU in 0.136373 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.05 MB) transferred to GPU in 0.134393 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.19 MB) transferred to GPU in 0.136236 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (281.25 MB) transferred to GPU in 0.136111 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.706255:  60%|########4     | 6/10 [07:35<05:20, 80.22s/it]\u001b[32m[I 2021-08-27 14:15:42,234]\u001b[0m Trial 32 finished with value: 0.7278075672911697 and parameters: {'bagging_fraction': 0.4679889728700056, 'bagging_freq': 1}. Best is trial 31 with value: 0.7062551245023463.\u001b[0m\n",
      "bagging, val_score: 0.706255:  60%|########4     | 6/10 [07:35<05:20, 80.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's rmse: 0.701279\tvalid_1's rmse: 0.727808\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.289838 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.48 MB) transferred to GPU in 0.146666 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.21 MB) transferred to GPU in 0.144880 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.19 MB) transferred to GPU in 0.143856 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.19 MB) transferred to GPU in 0.144535 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.22 MB) transferred to GPU in 0.145415 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.32 MB) transferred to GPU in 0.147702 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.16 MB) transferred to GPU in 0.142830 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.21 MB) transferred to GPU in 0.143074 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.23 MB) transferred to GPU in 0.142943 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.22 MB) transferred to GPU in 0.143165 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.24 MB) transferred to GPU in 0.142774 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.18 MB) transferred to GPU in 0.143819 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.20 MB) transferred to GPU in 0.143389 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736958\tvalid_1's rmse: 0.721045\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.19 MB) transferred to GPU in 0.142579 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.18 MB) transferred to GPU in 0.144078 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.27 MB) transferred to GPU in 0.145666 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.29 MB) transferred to GPU in 0.144165 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.27 MB) transferred to GPU in 0.145479 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.10 MB) transferred to GPU in 0.146344 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.35 MB) transferred to GPU in 0.146940 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.22 MB) transferred to GPU in 0.144641 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.14 MB) transferred to GPU in 0.143961 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.21 MB) transferred to GPU in 0.142895 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.20 MB) transferred to GPU in 0.145576 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.24 MB) transferred to GPU in 0.144353 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.702805\tvalid_1's rmse: 0.719386\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.12 MB) transferred to GPU in 0.143354 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.18 MB) transferred to GPU in 0.142335 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (298.96 MB) transferred to GPU in 0.144404 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (298.96 MB) transferred to GPU in 0.143438 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.33 MB) transferred to GPU in 0.144785 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (298.87 MB) transferred to GPU in 0.144314 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.18 MB) transferred to GPU in 0.143824 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (298.88 MB) transferred to GPU in 0.145330 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.16 MB) transferred to GPU in 0.145057 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.05 MB) transferred to GPU in 0.143992 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.21 MB) transferred to GPU in 0.144879 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.18 MB) transferred to GPU in 0.145480 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.15 MB) transferred to GPU in 0.143575 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.684484\tvalid_1's rmse: 0.718579\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.19 MB) transferred to GPU in 0.144461 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.15 MB) transferred to GPU in 0.142878 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.19 MB) transferred to GPU in 0.143144 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (299.19 MB) transferred to GPU in 0.142905 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.706255:  70%|#########7    | 7/10 [08:43<03:48, 76.28s/it]\u001b[32m[I 2021-08-27 14:16:50,417]\u001b[0m Trial 33 finished with value: 0.7175606888286148 and parameters: {'bagging_fraction': 0.4978480032991467, 'bagging_freq': 4}. Best is trial 31 with value: 0.7062551245023463.\u001b[0m\n",
      "bagging, val_score: 0.706255:  70%|#########7    | 7/10 [08:43<03:48, 76.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[135]\tvalid_0's rmse: 0.688463\tvalid_1's rmse: 0.717561\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.303263 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.62 MB) transferred to GPU in 0.258348 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.45 MB) transferred to GPU in 0.244325 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.245155 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.245086 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.31 MB) transferred to GPU in 0.243672 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.53 MB) transferred to GPU in 0.239829 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.45 MB) transferred to GPU in 0.240807 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.49 MB) transferred to GPU in 0.242657 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.40 MB) transferred to GPU in 0.243918 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.50 MB) transferred to GPU in 0.241557 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.45 MB) transferred to GPU in 0.240622 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.37 MB) transferred to GPU in 0.246966 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.52 MB) transferred to GPU in 0.240626 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.47 MB) transferred to GPU in 0.244594 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.52 MB) transferred to GPU in 0.240285 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.47 MB) transferred to GPU in 0.241421 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.55 MB) transferred to GPU in 0.247316 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736399\tvalid_1's rmse: 0.722923\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.247436 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.51 MB) transferred to GPU in 0.241797 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.58 MB) transferred to GPU in 0.245781 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.42 MB) transferred to GPU in 0.251094 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.48 MB) transferred to GPU in 0.254187 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.254710 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.48 MB) transferred to GPU in 0.260280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.49 MB) transferred to GPU in 0.257182 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.40 MB) transferred to GPU in 0.253271 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.258251 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.69 MB) transferred to GPU in 0.258479 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.61 MB) transferred to GPU in 0.263500 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.57 MB) transferred to GPU in 0.246333 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.54 MB) transferred to GPU in 0.245825 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.51 MB) transferred to GPU in 0.241020 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.54 MB) transferred to GPU in 0.241938 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.45 MB) transferred to GPU in 0.261512 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.698379\tvalid_1's rmse: 0.716717\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.30 MB) transferred to GPU in 0.241990 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.49 MB) transferred to GPU in 0.242151 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.44 MB) transferred to GPU in 0.244377 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.43 MB) transferred to GPU in 0.246976 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.47 MB) transferred to GPU in 0.245905 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.35 MB) transferred to GPU in 0.244200 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.47 MB) transferred to GPU in 0.245358 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.50 MB) transferred to GPU in 0.241753 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.47 MB) transferred to GPU in 0.241600 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.42 MB) transferred to GPU in 0.244293 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.49 MB) transferred to GPU in 0.241409 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.244566 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.43 MB) transferred to GPU in 0.243188 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.51 MB) transferred to GPU in 0.242733 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.244936 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.46 MB) transferred to GPU in 0.240457 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.677619\tvalid_1's rmse: 0.71828\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.33 MB) transferred to GPU in 0.242654 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (508.43 MB) transferred to GPU in 0.245593 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.706255:  80%|###########2  | 8/10 [10:10<02:39, 79.59s/it]\u001b[32m[I 2021-08-27 14:18:17,074]\u001b[0m Trial 34 finished with value: 0.7161039983305763 and parameters: {'bagging_fraction': 0.8460965149038476, 'bagging_freq': 3}. Best is trial 31 with value: 0.7062551245023463.\u001b[0m\n",
      "bagging, val_score: 0.706255:  80%|###########2  | 8/10 [10:10<02:39, 79.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[125]\tvalid_0's rmse: 0.686968\tvalid_1's rmse: 0.716104\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291051 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.296664 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.288282 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.280061 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.277441 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.58 MB) transferred to GPU in 0.279348 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.69 MB) transferred to GPU in 0.278423 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.292721 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.63 MB) transferred to GPU in 0.277977 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.66 MB) transferred to GPU in 0.279420 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.66 MB) transferred to GPU in 0.280417 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.280260 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.282896 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.63 MB) transferred to GPU in 0.279661 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.281384 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.279951 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.280459 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.70 MB) transferred to GPU in 0.280841 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.279268 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.281133 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.279195 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.60 MB) transferred to GPU in 0.280788 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.280334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.281994 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.279473 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.281117 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.735001\tvalid_1's rmse: 0.72225\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.279849 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.278698 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.69 MB) transferred to GPU in 0.279273 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.70 MB) transferred to GPU in 0.281640 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.279721 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.284059 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.68 MB) transferred to GPU in 0.280359 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.59 MB) transferred to GPU in 0.278760 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.59 MB) transferred to GPU in 0.279182 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.53 MB) transferred to GPU in 0.280559 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.279899 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.279216 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.278096 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.279475 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.69 MB) transferred to GPU in 0.280963 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.281464 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.63 MB) transferred to GPU in 0.278116 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.280954 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.283770 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.66 MB) transferred to GPU in 0.278954 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.283568 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.59 MB) transferred to GPU in 0.278720 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.294763 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.60 MB) transferred to GPU in 0.292179 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.279663 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.697514\tvalid_1's rmse: 0.717778\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.280035 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.277570 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.279983 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.280449 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.64 MB) transferred to GPU in 0.279376 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.58 MB) transferred to GPU in 0.282703 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.281588 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.66 MB) transferred to GPU in 0.280150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.59 MB) transferred to GPU in 0.280051 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.61 MB) transferred to GPU in 0.279657 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.68 MB) transferred to GPU in 0.279253 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.63 MB) transferred to GPU in 0.279848 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.60 MB) transferred to GPU in 0.280061 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.65 MB) transferred to GPU in 0.279189 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.60 MB) transferred to GPU in 0.279395 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.63 MB) transferred to GPU in 0.278486 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.57 MB) transferred to GPU in 0.283023 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.67 MB) transferred to GPU in 0.279922 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (585.62 MB) transferred to GPU in 0.278706 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.706255:  90%|############6 | 9/10 [11:51<01:26, 86.42s/it]\u001b[32m[I 2021-08-27 14:19:58,519]\u001b[0m Trial 35 finished with value: 0.7167275235407309 and parameters: {'bagging_fraction': 0.9744820514958467, 'bagging_freq': 2}. Best is trial 31 with value: 0.7062551245023463.\u001b[0m\n",
      "bagging, val_score: 0.706255:  90%|############6 | 9/10 [11:51<01:26, 86.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's rmse: 0.694174\tvalid_1's rmse: 0.716728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.292498 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[50]\tvalid_0's rmse: 0.737261\tvalid_1's rmse: 0.720954\n",
      "[100]\tvalid_0's rmse: 0.700488\tvalid_1's rmse: 0.716161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.706255: 100%|#############| 10/10 [12:45<00:00, 76.13s/it]\u001b[32m[I 2021-08-27 14:20:51,605]\u001b[0m Trial 36 finished with value: 0.7150190911203045 and parameters: {'bagging_fraction': 0.669986131049334, 'bagging_freq': 1}. Best is trial 31 with value: 0.7062551245023463.\u001b[0m\n",
      "bagging, val_score: 0.706255: 100%|#############| 10/10 [12:45<00:00, 76.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's rmse: 0.69345\tvalid_1's rmse: 0.715019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.706255:   0%|       | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.307322 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.208566 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208656 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.206803 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.205900 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.193647 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.208032 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.207461 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194374 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.203768 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.195917 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.207133 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.205763 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.193818 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.205695 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207839 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198044 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.195874 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737423\tvalid_1's rmse: 0.720255\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.206402 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205984 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.214144 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194109 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195030 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194678 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209292 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206253 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195658 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196612 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.205391 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.195739 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.706255:  17%|1| 1/6 [00:56<04:40, 56.17s/i\u001b[32m[I 2021-08-27 14:21:47,780]\u001b[0m Trial 37 finished with value: 0.7181297730506547 and parameters: {'feature_fraction': 0.7520000000000001}. Best is trial 37 with value: 0.7181297730506547.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.706255:  17%|1| 1/6 [00:56<04:40, 56.17s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's rmse: 0.728947\tvalid_1's rmse: 0.71813\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.300387 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.196657 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195332 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194327 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194400 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208302 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.207776 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194228 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.193479 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198788 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.197581 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.206215 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195098 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.193577 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210077 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207839 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.207193 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.193491 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737034\tvalid_1's rmse: 0.722716\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.194488 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195053 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.204783 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.205497 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194867 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195276 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.212538 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195062 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.208123 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.206896 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.206062 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.195955 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.193714 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.194605 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.205914 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.197952 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.206875 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.699972\tvalid_1's rmse: 0.720704\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195124 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.706255:  33%|3| 2/6 [01:58<03:58, 59.54s/i\u001b[32m[I 2021-08-27 14:22:49,683]\u001b[0m Trial 38 finished with value: 0.719133524201937 and parameters: {'feature_fraction': 0.8160000000000001}. Best is trial 37 with value: 0.7181297730506547.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.706255:  33%|3| 2/6 [01:58<03:58, 59.54s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's rmse: 0.715495\tvalid_1's rmse: 0.719134\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.293153 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.198063 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208603 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.209188 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208447 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.205674 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.199015 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196600 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198927 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196297 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.196405 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195279 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197102 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210854 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.205698 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196659 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195017 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.194893 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736827\tvalid_1's rmse: 0.717331\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.198109 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211454 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207133 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.206745 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194924 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195781 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.193987 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.193500 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194425 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.193914 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.195484 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.194703 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197235 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.194917 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.194187 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.197484 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.210698 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700086\tvalid_1's rmse: 0.711122\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194384 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197106 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.213196 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.204555 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206990 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.205329 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195278 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.194585 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.193001 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.193312 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197744 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196659 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.195665 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194902 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195158 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.206243 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.680653\tvalid_1's rmse: 0.711473\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.194105 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206873 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.193072 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194808 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195681 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195807 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.193435 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.41 MB) transferred to GPU in 0.197009 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.193744 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.706255:  50%|5| 3/6 [03:23<03:33, 71.21s/i\u001b[32m[I 2021-08-27 14:24:14,766]\u001b[0m Trial 39 finished with value: 0.7103460571222306 and parameters: {'feature_fraction': 0.8480000000000001}. Best is trial 39 with value: 0.7103460571222306.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.706255:  50%|5| 3/6 [03:23<03:33, 71.21s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's rmse: 0.682165\tvalid_1's rmse: 0.710346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.288446 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.207049 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.212024 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194667 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194226 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194674 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.206291 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195954 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.209591 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195396 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.205169 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194667 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206197 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206541 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.204720 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196865 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.197282 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.194533 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736678\tvalid_1's rmse: 0.727172\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.207398 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.206488 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207700 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196528 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.209357 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.193821 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.198978 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209748 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207266 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.211868 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.198108 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.207052 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.210723 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.195732 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195506 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.699586\tvalid_1's rmse: 0.724712\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207317 secs. 1 sparse feature groups\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid_0's rmse: 0.715592\tvalid_1's rmse: 0.723716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.706255:  67%|6| 4/6 [04:26<02:16, 68.11s/i\u001b[32m[I 2021-08-27 14:25:18,132]\u001b[0m Trial 40 finished with value: 0.7237156064912157 and parameters: {'feature_fraction': 0.88}. Best is trial 39 with value: 0.7103460571222306.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.706255:  67%|6| 4/6 [04:26<02:16, 68.11s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.290734 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.205000 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205747 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.205655 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208570 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207641 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.205945 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195769 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.206063 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194778 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.195462 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.204782 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197031 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206217 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210643 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196913 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.194983 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.208959 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.73717\tvalid_1's rmse: 0.723011\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.207924 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208308 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208270 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.193810 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208802 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.205594 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208931 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207336 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.216689 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208632 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.195895 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.206953 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197491 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.195872 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.195519 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.198875 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.201775 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.701222\tvalid_1's rmse: 0.721108\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208769 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.212632 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.210775 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.706255:  83%|8| 5/6 [05:33<01:07, 67.67s/i\u001b[32m[I 2021-08-27 14:26:25,035]\u001b[0m Trial 41 finished with value: 0.719990226175609 and parameters: {'feature_fraction': 0.7200000000000001}. Best is trial 39 with value: 0.7103460571222306.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.706255:  83%|8| 5/6 [05:33<01:07, 67.67s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[80]\tvalid_0's rmse: 0.712036\tvalid_1's rmse: 0.71999\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.311164 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.197998 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196980 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195776 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194849 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194635 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198095 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.211141 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194733 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195058 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.198900 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197556 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.212008 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206379 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.202493 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195498 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.216149 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.195823 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736763\tvalid_1's rmse: 0.726996\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.212192 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.219233 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.229340 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.206814 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.201805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.212901 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209861 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.226842 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.212825 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.236660 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.219186 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.203817 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208289 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.200488 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.221868 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.197122 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.706255: 100%|#| 6/6 [06:39<00:00, 67.12s/i\u001b[32m[I 2021-08-27 14:27:31,075]\u001b[0m Trial 42 finished with value: 0.7241088644822068 and parameters: {'feature_fraction': 0.784}. Best is trial 39 with value: 0.7103460571222306.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.706255: 100%|#| 6/6 [06:39<00:00, 66.58s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's rmse: 0.719035\tvalid_1's rmse: 0.724109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:   0%|       | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.288771 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.197092 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197507 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197096 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197181 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194961 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.206090 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209035 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.206587 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195537 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.194700 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197885 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209297 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.205875 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195477 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.200166 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.200887 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.200385 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736026\tvalid_1's rmse: 0.719372\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.209427 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209706 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194902 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194862 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196487 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194801 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208674 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207879 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209668 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208194 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.210065 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.209368 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.204436 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.213523 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.205526 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.212744 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207835 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700979\tvalid_1's rmse: 0.717629\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197387 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197066 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.209007 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.196005 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.200721 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.208068 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196237 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206356 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.206240 secs. 1 sparse feature groups\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid_0's rmse: 0.702416\tvalid_1's rmse: 0.717111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:   5%| | 1/20 [01:12<22:55, 72.39s/i\u001b[32m[I 2021-08-27 14:28:43,472]\u001b[0m Trial 43 finished with value: 0.7171112987951822 and parameters: {'lambda_l1': 0.001502904470555896, 'lambda_l2': 0.18454750784392884}. Best is trial 43 with value: 0.7171112987951822.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:   5%| | 1/20 [01:12<22:55, 72.39s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.290877 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.194837 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196200 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.198364 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194421 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.203673 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.194799 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.198946 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196671 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195809 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.196732 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195322 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195619 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.201191 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195525 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197386 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.197416 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.209249 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.73695\tvalid_1's rmse: 0.722334\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.209636 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209265 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207072 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207513 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.207927 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.205953 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.206667 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209358 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.207358 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207008 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.201078 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.198558 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196252 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.195551 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.209422 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196672 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197130 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700542\tvalid_1's rmse: 0.718275\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197284 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.206214 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.210028 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.208498 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208851 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.195690 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.199331 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.214230 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.207864 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.205449 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208158 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195173 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.199150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.205514 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197113 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.198202 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  10%|1| 2/20 [02:32<23:04, 76.90s/i\u001b[32m[I 2021-08-27 14:30:03,533]\u001b[0m Trial 44 finished with value: 0.7172590133300029 and parameters: {'lambda_l1': 0.01386244125501208, 'lambda_l2': 2.1324480508548e-08}. Best is trial 43 with value: 0.7171112987951822.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  10%|1| 2/20 [02:32<23:04, 76.90s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[119]\tvalid_0's rmse: 0.691529\tvalid_1's rmse: 0.717259\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.285142 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.209816 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197742 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194717 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195904 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209663 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.196395 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.198031 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.207716 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196995 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.207623 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209469 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.193932 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198360 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196634 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.208555 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.208460 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.213139 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738298\tvalid_1's rmse: 0.712623\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.197545 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194403 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209678 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.209830 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.211922 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197943 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194606 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.278762 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196016 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197567 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.208589 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.198284 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.198339 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.195468 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.196031 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196128 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.212466 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.701596\tvalid_1's rmse: 0.709276\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196390 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.209633 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.209957 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  15%|1| 3/20 [03:39<20:31, 72.42s/i\u001b[32m[I 2021-08-27 14:31:10,627]\u001b[0m Trial 45 finished with value: 0.7079525761865375 and parameters: {'lambda_l1': 0.16061702768292913, 'lambda_l2': 3.951176110375194}. Best is trial 45 with value: 0.7079525761865375.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  15%|1| 3/20 [03:39<20:31, 72.42s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's rmse: 0.711937\tvalid_1's rmse: 0.707953\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291311 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.197769 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195536 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197242 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207843 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.205862 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195221 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195125 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208786 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207952 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.206944 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196771 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195687 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198246 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208106 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197587 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.205208 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.196492 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738602\tvalid_1's rmse: 0.713495\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.194797 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.198518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.206163 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.208990 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195048 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.210014 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197722 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195783 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195399 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195548 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.196381 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.194965 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195573 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.209426 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.209636 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.197348 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194295 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.703188\tvalid_1's rmse: 0.710004\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194123 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.196546 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196732 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.209756 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197144 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.198783 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196601 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.194738 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198560 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.210124 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.199151 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206726 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.209444 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.198652 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.210549 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.210562 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.68326\tvalid_1's rmse: 0.707743\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.207090 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.212959 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.214321 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210884 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.206125 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195925 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.194633 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.41 MB) transferred to GPU in 0.195453 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.213875 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.206888 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.195286 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.15 MB) transferred to GPU in 0.194727 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  20%|2| 4/20 [05:11<21:22, 80.16s/i\u001b[32m[I 2021-08-27 14:32:42,635]\u001b[0m Trial 46 finished with value: 0.7075692342440125 and parameters: {'lambda_l1': 2.583523091091767e-08, 'lambda_l2': 5.42957725236534}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  20%|2| 4/20 [05:11<21:22, 80.16s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[155]\tvalid_0's rmse: 0.682079\tvalid_1's rmse: 0.707569\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.303917 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.195974 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197417 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194579 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.199421 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.211401 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.200459 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.193565 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.193823 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.193600 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.196005 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194383 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195699 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196422 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197169 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197502 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.194280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.194530 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736167\tvalid_1's rmse: 0.719396\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.198384 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196561 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197418 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196764 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.197451 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194899 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195373 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209822 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.198228 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.214559 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.193944 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.202877 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195220 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.194612 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.193553 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.193394 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196328 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.699886\tvalid_1's rmse: 0.713151\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.203833 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.198500 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.198124 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.204945 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.204422 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196746 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196752 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.193852 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.207921 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208391 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206729 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.208840 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.204494 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.206207 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.207583 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.680852\tvalid_1's rmse: 0.710684\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.206805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198940 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194870 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194616 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194662 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197211 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197255 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.41 MB) transferred to GPU in 0.208758 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.213953 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.205944 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.209999 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.15 MB) transferred to GPU in 0.209533 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.206475 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.209349 secs. 1 sparse feature groups\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid_0's rmse: 0.678088\tvalid_1's rmse: 0.710295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  25%|2| 5/20 [06:43<21:06, 84.45s/i\u001b[32m[I 2021-08-27 14:34:14,693]\u001b[0m Trial 47 finished with value: 0.7102952572572311 and parameters: {'lambda_l1': 3.2331311743265406e-08, 'lambda_l2': 2.9796793255041878e-05}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  25%|2| 5/20 [06:43<21:06, 84.45s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.301325 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.196530 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195699 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.208998 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208287 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194821 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.197491 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196882 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.206048 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.193224 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.193846 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.207177 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.202702 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196591 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195569 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.198473 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.209902 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.196099 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737123\tvalid_1's rmse: 0.723818\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.201398 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196741 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209247 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.210392 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.207212 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197588 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.202394 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.214372 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.210816 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.205802 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.197261 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.240154 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.222778 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.197260 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.201124 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196250 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.198703 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700656\tvalid_1's rmse: 0.721393\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196094 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.208235 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  30%|3| 6/20 [07:49<18:12, 78.04s/i\u001b[32m[I 2021-08-27 14:35:20,292]\u001b[0m Trial 48 finished with value: 0.7198841160703967 and parameters: {'lambda_l1': 0.0001081331804088778, 'lambda_l2': 0.016715880583807283}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  30%|3| 6/20 [07:49<18:12, 78.04s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's rmse: 0.713088\tvalid_1's rmse: 0.719884\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.300462 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.208452 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209244 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.207165 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.206170 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197104 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.194950 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195532 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194676 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196565 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.206693 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.213552 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208661 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.213273 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206410 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196561 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.208664 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.202175 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737136\tvalid_1's rmse: 0.720007\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.198397 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205461 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211098 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.198800 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.197313 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.198795 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196911 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197717 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209498 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.206614 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.197207 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.195307 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.210396 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.207847 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.211025 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.197473 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197259 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.701024\tvalid_1's rmse: 0.714808\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196369 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195184 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.211494 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.195623 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.200157 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.196375 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194576 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196909 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195855 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.209286 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211889 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197938 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.223046 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.203365 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.224385 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  35%|3| 7/20 [09:08<17:01, 78.55s/i\u001b[32m[I 2021-08-27 14:36:39,904]\u001b[0m Trial 49 finished with value: 0.7133058307617829 and parameters: {'lambda_l1': 0.006690950293694873, 'lambda_l2': 4.1195940870783956e-07}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  35%|3| 7/20 [09:08<17:01, 78.55s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[116]\tvalid_0's rmse: 0.693964\tvalid_1's rmse: 0.713306\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291717 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.196144 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194705 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195981 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.200974 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.203737 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.209115 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.215299 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.197293 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195660 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.196686 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194509 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196539 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206091 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198297 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.208685 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.196905 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.198242 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737282\tvalid_1's rmse: 0.723316\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.208949 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208080 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211678 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197836 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.199490 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195563 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195771 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.194752 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209229 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194343 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.206518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.199289 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.198491 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.198687 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.231149 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.194650 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196379 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700756\tvalid_1's rmse: 0.718853\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.211798 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.207066 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.199502 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.210306 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194880 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.197534 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.229292 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.211593 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208128 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.213204 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194296 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196011 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.194316 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196860 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.198639 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.199687 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.680427\tvalid_1's rmse: 0.719106\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.197416 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.199316 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197385 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196491 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194606 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196573 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  40%|4| 8/20 [10:35<16:12, 81.00s/i\u001b[32m[I 2021-08-27 14:38:06,142]\u001b[0m Trial 50 finished with value: 0.7172033598785498 and parameters: {'lambda_l1': 0.03379224094314176, 'lambda_l2': 3.952079844228307e-07}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  40%|4| 8/20 [10:35<16:12, 81.00s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's rmse: 0.685404\tvalid_1's rmse: 0.717203\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.290774 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.195898 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205223 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195524 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207456 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209195 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.196806 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196069 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.209191 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206609 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.206360 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.199165 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196289 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197059 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196040 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195784 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.196788 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.197726 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737705\tvalid_1's rmse: 0.721788\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.196485 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.210807 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209527 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194820 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196170 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.231945 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211268 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209682 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209416 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209734 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.200334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.197458 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.198581 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.206158 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.207714 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.207494 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207584 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700813\tvalid_1's rmse: 0.720326\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197025 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197377 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.199563 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  45%|4| 9/20 [11:41<14:00, 76.40s/i\u001b[32m[I 2021-08-27 14:39:12,414]\u001b[0m Trial 51 finished with value: 0.7190730042877022 and parameters: {'lambda_l1': 0.0004342068571932004, 'lambda_l2': 3.5492328595747097e-07}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  45%|4| 9/20 [11:41<14:00, 76.40s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's rmse: 0.712189\tvalid_1's rmse: 0.719073\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291927 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.207250 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.206683 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.211725 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.204818 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.205706 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.194977 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.207915 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195542 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195675 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.205665 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.205248 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.199189 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198692 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.211664 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.209714 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.207882 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.204640 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737261\tvalid_1's rmse: 0.717579\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.206280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.206534 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.206131 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207690 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.212313 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197636 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208309 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209821 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197530 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196177 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.194764 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  50%|5| 10/20 [12:38<11:43, 70.33s/\u001b[32m[I 2021-08-27 14:40:09,174]\u001b[0m Trial 52 finished with value: 0.7165414660540914 and parameters: {'lambda_l1': 0.04833542195241035, 'lambda_l2': 3.2065704640078795e-05}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  50%|5| 10/20 [12:38<11:43, 70.33s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's rmse: 0.733691\tvalid_1's rmse: 0.716541\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.303817 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.199059 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197087 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.212011 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207360 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208800 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.209504 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209210 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.206791 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210452 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.205668 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.206063 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.204218 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206424 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207984 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195422 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198057 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.199114 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738987\tvalid_1's rmse: 0.724909\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.206441 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208474 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.221155 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.240674 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.204098 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195717 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194307 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195736 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194774 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196673 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.204712 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.206219 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.204476 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.194716 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.195493 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196477 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196456 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.704629\tvalid_1's rmse: 0.723096\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209061 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.200004 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.198290 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.217393 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.214268 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.199311 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.206618 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208253 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.211304 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.208416 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.203583 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195314 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.193823 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195977 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208968 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.210198 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.684617\tvalid_1's rmse: 0.720956\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.210361 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208848 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209073 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207390 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.208186 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206056 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.203599 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.41 MB) transferred to GPU in 0.198980 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197360 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.199691 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.207698 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.15 MB) transferred to GPU in 0.195907 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.199386 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195332 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.17 MB) transferred to GPU in 0.194263 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198753 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.211611 secs. 1 sparse feature groups\n",
      "[200]\tvalid_0's rmse: 0.671152\tvalid_1's rmse: 0.719854\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.90 MB) transferred to GPU in 0.205571 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  55%|5| 11/20 [14:17<11:54, 79.37s/\u001b[32m[I 2021-08-27 14:41:49,044]\u001b[0m Trial 53 finished with value: 0.7192995799129345 and parameters: {'lambda_l1': 1.6010285925252295e-08, 'lambda_l2': 7.391451683456145}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  55%|5| 11/20 [14:17<11:54, 79.37s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[172]\tvalid_0's rmse: 0.678535\tvalid_1's rmse: 0.7193\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.291027 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.208637 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208171 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.200730 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195490 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209053 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.213746 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196241 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208583 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209126 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.198477 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.211386 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209196 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210469 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209452 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.205632 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.208778 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.209657 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.740517\tvalid_1's rmse: 0.722134\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.196490 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207879 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211012 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.204617 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.199419 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.200045 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197167 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197434 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196334 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.198182 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.210321 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.208512 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208481 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.209062 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.195104 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.197862 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207785 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.705746\tvalid_1's rmse: 0.715597\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194493 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.207274 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197459 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.195682 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197300 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.194915 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196828 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195389 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208304 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.208433 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.210754 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.210932 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.208333 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207104 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.214256 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197446 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.686248\tvalid_1's rmse: 0.71462\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.207010 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208577 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210108 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207899 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  60%|6| 12/20 [15:44<10:51, 81.49s/\u001b[32m[I 2021-08-27 14:43:15,358]\u001b[0m Trial 54 finished with value: 0.7138968841206643 and parameters: {'lambda_l1': 7.8410105979663065, 'lambda_l2': 9.324547186077053}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  60%|6| 12/20 [15:44<10:51, 81.49s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's rmse: 0.692582\tvalid_1's rmse: 0.713897\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.306250 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.196948 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205523 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.207622 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.205649 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210630 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.196755 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197448 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208069 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209282 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.205733 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.206919 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207704 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195060 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206978 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196078 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.200187 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.196683 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.73687\tvalid_1's rmse: 0.71546\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.212339 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.214038 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197031 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196288 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208032 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.208832 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211192 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207006 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.199818 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195861 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.196677 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.209398 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.210240 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.197234 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.209223 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.199144 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195595 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.7007\tvalid_1's rmse: 0.712306\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196336 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195072 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.199122 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.197176 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207051 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.195640 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198091 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194450 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  65%|6| 13/20 [16:57<09:11, 78.83s/\u001b[32m[I 2021-08-27 14:44:28,081]\u001b[0m Trial 55 finished with value: 0.7122139316896805 and parameters: {'lambda_l1': 5.405181766350313e-06, 'lambda_l2': 0.09847677843575763}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  65%|6| 13/20 [16:57<09:11, 78.83s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[99]\tvalid_0's rmse: 0.701233\tvalid_1's rmse: 0.712214\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.290201 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.211682 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.198718 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197866 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195093 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198782 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197451 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196176 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194271 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.194204 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196247 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196320 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197530 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197401 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196189 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195734 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.195137 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736918\tvalid_1's rmse: 0.722476\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.195213 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197669 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197145 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207096 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.193924 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195453 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196906 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207355 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.214413 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208824 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.209803 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.209300 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195300 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.210870 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.206450 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196501 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194070 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700305\tvalid_1's rmse: 0.717758\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.198398 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.200125 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194976 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.210937 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.211175 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.196740 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.194724 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197149 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.207767 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.706255:  70%|7| 14/20 [18:09<07:41, 76.93s/\u001b[32m[I 2021-08-27 14:45:40,608]\u001b[0m Trial 56 finished with value: 0.7175633795994565 and parameters: {'lambda_l1': 2.597783796193965, 'lambda_l2': 0.0011720939459184036}. Best is trial 46 with value: 0.7075692342440125.\u001b[0m\n",
      "regularization_factors, val_score: 0.706255:  70%|7| 14/20 [18:09<07:41, 76.93s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[98]\tvalid_0's rmse: 0.701314\tvalid_1's rmse: 0.717563\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.298316 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.197024 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.201155 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.196805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209008 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195418 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198043 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196773 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198753 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194996 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.197986 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209680 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197024 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196705 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196068 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195397 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.197462 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.205445 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738244\tvalid_1's rmse: 0.710424\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.208780 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209088 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209364 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.209372 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.197316 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194993 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196537 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.199469 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196555 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207080 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.209529 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.207525 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.215759 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.206620 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.210171 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.197107 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194736 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.701143\tvalid_1's rmse: 0.704501\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194786 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.196938 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196459 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.195785 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197142 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.208859 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196967 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.200208 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.199436 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.210000 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194887 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.200423 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.200695 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.211295 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207639 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194947 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.681098\tvalid_1's rmse: 0.705175\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.196408 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208402 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195886 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.703937:  75%|7| 15/20 [19:32<06:33, 78.61s/\u001b[32m[I 2021-08-27 14:47:03,109]\u001b[0m Trial 57 finished with value: 0.703936910956981 and parameters: {'lambda_l1': 1.0922476640551503e-06, 'lambda_l2': 0.7681177267405033}. Best is trial 57 with value: 0.703936910956981.\u001b[0m\n",
      "regularization_factors, val_score: 0.703937:  75%|7| 15/20 [19:32<06:33, 78.61s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's rmse: 0.688192\tvalid_1's rmse: 0.703937\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.307083 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.196954 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.199647 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195529 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.205397 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.211096 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.207966 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.208423 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196201 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.209187 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.193907 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209245 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196104 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198711 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197333 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.206396 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195088 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.205722 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737639\tvalid_1's rmse: 0.721691\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.198280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196706 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196876 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.206719 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.207301 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.212786 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207985 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209133 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.214382 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.207905 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.208074 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.209485 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.201433 secs. 1 sparse feature groups\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's rmse: 0.72912\tvalid_1's rmse: 0.719226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.703937:  80%|8| 16/20 [20:30<04:50, 72.61s/\u001b[32m[I 2021-08-27 14:48:01,793]\u001b[0m Trial 58 finished with value: 0.7192256739758698 and parameters: {'lambda_l1': 7.835038808376896e-07, 'lambda_l2': 0.503953244466992}. Best is trial 57 with value: 0.703936910956981.\u001b[0m\n",
      "regularization_factors, val_score: 0.703937:  80%|8| 16/20 [20:30<04:50, 72.61s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.310546 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.195084 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.198393 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194910 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195713 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196112 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.197497 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195562 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.199429 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.197166 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.205565 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.198207 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.210729 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.213952 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207114 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.209622 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.209850 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736926\tvalid_1's rmse: 0.723466\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.208107 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197279 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211128 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207165 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.209648 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.213920 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195168 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.203903 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196511 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.199651 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.202332 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.207191 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206977 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.195650 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.194271 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.205447 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.213354 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.701325\tvalid_1's rmse: 0.717994\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209786 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.194724 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.196784 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.207900 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197211 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.208432 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208735 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.194949 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196598 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.197159 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.703937:  85%|8| 17/20 [21:44<03:38, 72.88s/\u001b[32m[I 2021-08-27 14:49:15,313]\u001b[0m Trial 59 finished with value: 0.7173057038041072 and parameters: {'lambda_l1': 6.821476707917864e-07, 'lambda_l2': 0.003565744914904353}. Best is trial 57 with value: 0.703936910956981.\u001b[0m\n",
      "regularization_factors, val_score: 0.703937:  85%|8| 17/20 [21:44<03:38, 72.88s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[101]\tvalid_0's rmse: 0.700608\tvalid_1's rmse: 0.717306\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.292176 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.194856 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211735 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.205900 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.211070 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198233 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195196 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198614 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.199465 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.208199 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195143 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195663 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195416 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198378 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.200999 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.196420 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.194335 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738381\tvalid_1's rmse: 0.720492\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.208667 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209016 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207165 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195086 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.199024 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196234 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.214525 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.209294 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194742 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.212101 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.195072 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.195877 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.703937:  90%|9| 18/20 [22:42<02:16, 68.47s/\u001b[32m[I 2021-08-27 14:50:13,521]\u001b[0m Trial 60 finished with value: 0.7196594141744861 and parameters: {'lambda_l1': 1.5280399368032256e-05, 'lambda_l2': 0.5730669458204268}. Best is trial 57 with value: 0.703936910956981.\u001b[0m\n",
      "regularization_factors, val_score: 0.703937:  90%|9| 18/20 [22:42<02:16, 68.47s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[55]\tvalid_0's rmse: 0.733161\tvalid_1's rmse: 0.719659\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.298791 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.209463 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207361 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.196789 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.198337 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195574 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195854 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209296 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195971 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197484 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.198570 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197104 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196217 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206730 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207809 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.209521 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198734 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.203044 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.736029\tvalid_1's rmse: 0.723574\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.206328 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196910 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197630 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.210149 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208801 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.208249 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.210511 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206738 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196859 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.211368 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.197021 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.196903 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196113 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.196801 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.200143 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196948 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.200145 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.70073\tvalid_1's rmse: 0.718135\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209712 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.211422 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.212587 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.197603 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.200937 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.198024 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195611 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197861 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.197199 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.212669 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.211544 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208532 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.199615 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197675 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.703937:  95%|9| 19/20 [24:00<01:11, 71.27s/\u001b[32m[I 2021-08-27 14:51:31,296]\u001b[0m Trial 61 finished with value: 0.7174438350031268 and parameters: {'lambda_l1': 1.5932361640078496e-07, 'lambda_l2': 0.05298359378096316}. Best is trial 57 with value: 0.703936910956981.\u001b[0m\n",
      "regularization_factors, val_score: 0.703937:  95%|9| 19/20 [24:00<01:11, 71.27s/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[113]\tvalid_0's rmse: 0.694959\tvalid_1's rmse: 0.717444\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.317205 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.197082 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194266 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197531 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.211729 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196536 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.208237 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.209428 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.206569 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206710 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.208390 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.196466 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207954 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198650 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194241 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.199907 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.206568 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.207758 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737317\tvalid_1's rmse: 0.719184\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.205198 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209777 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.207108 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.231863 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.197156 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.222795 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.204096 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.204531 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195680 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195770 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.195603 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.210821 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208952 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.207970 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.194194 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.199229 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207261 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700101\tvalid_1's rmse: 0.712411\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.212195 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.209877 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197600 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.195916 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.196934 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.195267 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.195059 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.198109 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196494 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.201922 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196828 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208579 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.218091 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.205005 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.210954 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.199460 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.680334\tvalid_1's rmse: 0.71161\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.195305 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194959 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198834 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208224 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.210391 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.703937: 100%|#| 20/20 [25:25<00:00, 75.50s/\u001b[32m[I 2021-08-27 14:52:56,678]\u001b[0m Trial 62 finished with value: 0.7108489197879092 and parameters: {'lambda_l1': 1.0761941165951977e-05, 'lambda_l2': 6.390972115804473e-05}. Best is trial 57 with value: 0.703936910956981.\u001b[0m\n",
      "regularization_factors, val_score: 0.703937: 100%|#| 20/20 [25:25<00:00, 76.28s/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[134]\tvalid_0's rmse: 0.685492\tvalid_1's rmse: 0.710849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.703937:   0%|              | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.287470 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.201348 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.278735 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.238961 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.249259 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.300383 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.291030 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.307099 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.249875 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.329115 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.281326 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.266913 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.291346 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.297518 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.199278 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.199520 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.215669 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.198187 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737565\tvalid_1's rmse: 0.715993\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.205412 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205934 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.200302 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.212179 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208574 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.261816 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.220167 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.281150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.200353 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.239550 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.210382 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.262439 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197695 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.198805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.195369 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.206075 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.199373 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.701156\tvalid_1's rmse: 0.712197\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.195928 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.209900 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.200421 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.197220 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198483 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.244083 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.205021 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.229756 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.254280 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.219965 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205497 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207301 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.237054 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.213239 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.282740 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.703937:  20%|#    | 1/5 [01:47<07:11, 107.79s/it]\u001b[32m[I 2021-08-27 14:54:44,468]\u001b[0m Trial 63 finished with value: 0.7102284552514522 and parameters: {'min_child_samples': 25}. Best is trial 63 with value: 0.7102284552514522.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.703937:  20%|#    | 1/5 [01:47<07:11, 107.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[117]\tvalid_0's rmse: 0.693757\tvalid_1's rmse: 0.710228\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.314342 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.214242 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.204460 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.297131 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.212182 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.195239 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.198360 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.259978 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.207430 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.199151 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.195447 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.198130 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.200262 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197773 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194663 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.208636 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.212536 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.208991 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737274\tvalid_1's rmse: 0.714906\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.209577 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.242273 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.263786 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.198766 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196969 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.198157 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196329 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207487 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.207350 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.206361 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.207139 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.196728 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195916 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.194780 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.199094 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.194954 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.211890 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.700969\tvalid_1's rmse: 0.711342\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.194755 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.197217 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197534 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.195160 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198336 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.201044 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.202770 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197747 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198417 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.198337 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.195810 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196515 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.703937:  40%|##4   | 2/5 [03:07<04:34, 91.36s/it]\u001b[32m[I 2021-08-27 14:56:04,329]\u001b[0m Trial 64 finished with value: 0.7105413741325992 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.7102284552514522.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.703937:  40%|##4   | 2/5 [03:07<04:34, 91.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[108]\tvalid_0's rmse: 0.697676\tvalid_1's rmse: 0.710541\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.314974 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.239546 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209616 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.230973 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.240394 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.220950 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.206441 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195968 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.224099 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.204094 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.209779 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.206640 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208383 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208516 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194512 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.193231 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195724 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.196399 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.742799\tvalid_1's rmse: 0.719352\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.193237 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.193980 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196558 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.195105 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198864 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.193767 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.208126 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.208077 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.206390 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197069 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.194725 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.208733 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.206228 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.200946 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.194562 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.196150 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197165 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.71008\tvalid_1's rmse: 0.715882\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.208859 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.209161 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207076 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.703937:  60%|###6  | 3/5 [04:18<02:43, 81.99s/it]\u001b[32m[I 2021-08-27 14:57:15,172]\u001b[0m Trial 65 finished with value: 0.715834742351295 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.7102284552514522.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.703937:  60%|###6  | 3/5 [04:18<02:43, 81.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[81]\tvalid_0's rmse: 0.719084\tvalid_1's rmse: 0.715835\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.303676 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.195147 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194943 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.205882 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.197477 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194974 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195250 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197078 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196096 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206404 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.195058 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.207967 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.207847 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207602 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.207376 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194500 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.203775 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.194697 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.738942\tvalid_1's rmse: 0.72229\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.206648 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.205789 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197009 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.249079 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.205216 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195362 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.194907 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.278558 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197637 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.290691 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.211001 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.258834 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.199492 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.221373 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.281666 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.211420 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.207547 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.703997\tvalid_1's rmse: 0.718901\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.218985 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.226749 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.203464 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.221723 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.249820 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.214037 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.196805 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.239157 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.276230 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.196421 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.197977 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.226220 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.207161 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.227943 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.703937:  80%|####8 | 4/5 [05:44<01:23, 83.54s/it]\u001b[32m[I 2021-08-27 14:58:41,085]\u001b[0m Trial 66 finished with value: 0.7186762819940584 and parameters: {'min_child_samples': 50}. Best is trial 63 with value: 0.7102284552514522.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.703937:  80%|####8 | 4/5 [05:44<01:23, 83.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[112]\tvalid_0's rmse: 0.698928\tvalid_1's rmse: 0.718676\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19699\n",
      "[LightGBM] [Info] Number of data points in the train set: 7876915, number of used features: 143\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (600.96 MB) transferred to GPU in 0.310231 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score 0.310162\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.51 MB) transferred to GPU in 0.196304 secs. 1 sparse feature groups\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196784 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.200089 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196591 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.194896 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.195239 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195044 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.208120 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.208608 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.27 MB) transferred to GPU in 0.209215 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.194455 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.196012 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.206183 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.198318 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.194801 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.31 MB) transferred to GPU in 0.207993 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.30 MB) transferred to GPU in 0.207942 secs. 1 sparse feature groups\n",
      "[50]\tvalid_0's rmse: 0.737277\tvalid_1's rmse: 0.711246\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.43 MB) transferred to GPU in 0.206598 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.210196 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.210910 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.197178 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.197278 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.195105 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.209578 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.195481 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.19 MB) transferred to GPU in 0.197521 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.196136 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.09 MB) transferred to GPU in 0.207193 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.86 MB) transferred to GPU in 0.261946 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.211142 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.40 MB) transferred to GPU in 0.227266 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.29 MB) transferred to GPU in 0.206159 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.79 MB) transferred to GPU in 0.203067 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.203093 secs. 1 sparse feature groups\n",
      "[100]\tvalid_0's rmse: 0.70117\tvalid_1's rmse: 0.706718\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209829 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.208509 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.18 MB) transferred to GPU in 0.198419 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.12 MB) transferred to GPU in 0.196798 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.261136 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.16 MB) transferred to GPU in 0.197254 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.198223 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.197986 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.23 MB) transferred to GPU in 0.239532 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.10 MB) transferred to GPU in 0.197796 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.198383 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.25 MB) transferred to GPU in 0.194344 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.26 MB) transferred to GPU in 0.210916 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.20 MB) transferred to GPU in 0.209492 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.24 MB) transferred to GPU in 0.196075 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.21 MB) transferred to GPU in 0.195843 secs. 1 sparse feature groups\n",
      "[150]\tvalid_0's rmse: 0.680661\tvalid_1's rmse: 0.707077\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (407.83 MB) transferred to GPU in 0.196695 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (408.22 MB) transferred to GPU in 0.197633 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.703937: 100%|######| 5/5 [07:08<00:00, 83.73s/it]\u001b[32m[I 2021-08-27 15:00:05,156]\u001b[0m Trial 67 finished with value: 0.7059349931271311 and parameters: {'min_child_samples': 10}. Best is trial 67 with value: 0.7059349931271311.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.703937: 100%|######| 5/5 [07:08<00:00, 85.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's rmse: 0.689707\tvalid_1's rmse: 0.705935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import optuna.integration.lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "    'objective': 'mse',\n",
    "    'metric': 'rmse',\n",
    "    'verbose': 1,\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id': 0,\n",
    "    'gpu_device_id': 0\n",
    "}\n",
    "cat_feats = ['item_category_id', 'month', 'season']\n",
    "\n",
    "last_block = 33\n",
    "dates = matrix['date_block_num']\n",
    "\n",
    "# drop target and some features that lead to overfitting\n",
    "cols_to_drop = ['item_cnt_month', 'new_item', 'shop_id', 'item_id']\n",
    "\n",
    "# split dataset on train and test sets for model training\n",
    "X_train = matrix.loc[dates <  last_block].drop(cols_to_drop, axis=1)\n",
    "X_val = matrix.loc[dates ==  last_block].drop(cols_to_drop, axis=1)\n",
    "X_test = matrix.loc[dates ==  34].drop(cols_to_drop, axis=1)\n",
    "\n",
    "y_train = matrix.loc[dates <  last_block, 'item_cnt_month']\n",
    "y_val = matrix.loc[dates ==  last_block, 'item_cnt_month']\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "study = optuna.create_study()\n",
    "\n",
    "gbm = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=8000,\n",
    "        study = study,\n",
    "        valid_sets=(lgb_train, lgb_eval), \n",
    "        categorical_feature = cat_feats,\n",
    "        verbose_eval=50, \n",
    "        early_stopping_rounds=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More verbose Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'mse',\n",
       " 'metric': 'rmse',\n",
       " 'verbose': 1,\n",
       " 'device': 'gpu',\n",
       " 'gpu_platform_id': 0,\n",
       " 'gpu_device_id': 0,\n",
       " 'feature_pre_filter': False,\n",
       " 'lambda_l1': 1.0922476640551503e-06,\n",
       " 'lambda_l2': 0.7681177267405033,\n",
       " 'num_leaves': 115,\n",
       " 'feature_fraction': 0.8,\n",
       " 'bagging_fraction': 0.679275144218356,\n",
       " 'bagging_freq': 3,\n",
       " 'min_child_samples': 20,\n",
       " 'num_iterations': 8000,\n",
       " 'early_stopping_round': 30,\n",
       " 'categorical_column': [7, 8, 12]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:02:17,479]\u001b[0m A new study created in memory with name: no-name-fd68ac8c-c773-4a0a-b1a1-8e2ba2b8dac9\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.734472\n",
      "[100]\tvalid_0's rmse: 0.726431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:03:10,546]\u001b[0m Trial 0 finished with value: 0.7264311104480914 and parameters: {'lambda_l1': 0.028801482021584914, 'lambda_l2': 1.5341168281538438e-05, 'num_leaves': 30, 'feature_fraction': 0.7237059817933481, 'bagging_fraction': 0.802845106905673, 'bagging_freq': 2, 'min_child_samples': 15}. Best is trial 0 with value: 0.7264311104480914.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.723195\n",
      "[100]\tvalid_0's rmse: 0.722866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:04:09,364]\u001b[0m Trial 1 finished with value: 0.7228661888008041 and parameters: {'lambda_l1': 8.479741676780124e-08, 'lambda_l2': 0.005299536110087354, 'num_leaves': 585, 'feature_fraction': 0.8315417047877722, 'bagging_fraction': 0.9791137231273153, 'bagging_freq': 5, 'min_child_samples': 63}. Best is trial 1 with value: 0.7228661888008041.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.720494\n",
      "[100]\tvalid_0's rmse: 0.721574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:05:38,591]\u001b[0m Trial 2 finished with value: 0.721574152308014 and parameters: {'lambda_l1': 1.0880134697160294e-06, 'lambda_l2': 6.093378232803597e-07, 'num_leaves': 879, 'feature_fraction': 0.8507112243445558, 'bagging_fraction': 0.9829152535032627, 'bagging_freq': 2, 'min_child_samples': 53}. Best is trial 2 with value: 0.721574152308014.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.727547\n",
      "[100]\tvalid_0's rmse: 0.730093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:06:32,332]\u001b[0m Trial 3 finished with value: 0.7300932194164093 and parameters: {'lambda_l1': 0.15180822637423058, 'lambda_l2': 0.00018838517684023666, 'num_leaves': 818, 'feature_fraction': 0.8654197540638072, 'bagging_fraction': 0.6253238354388261, 'bagging_freq': 7, 'min_child_samples': 50}. Best is trial 2 with value: 0.721574152308014.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.720011\n",
      "[100]\tvalid_0's rmse: 0.718118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:07:16,385]\u001b[0m Trial 4 finished with value: 0.718118457906163 and parameters: {'lambda_l1': 0.5518957116601848, 'lambda_l2': 0.004791926512834685, 'num_leaves': 487, 'feature_fraction': 0.8836186968273025, 'bagging_fraction': 0.6229790735983214, 'bagging_freq': 6, 'min_child_samples': 86}. Best is trial 4 with value: 0.718118457906163.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.720234\n",
      "[100]\tvalid_0's rmse: 0.72014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:08:15,245]\u001b[0m Trial 5 finished with value: 0.7201396456201655 and parameters: {'lambda_l1': 2.6427088618324297e-06, 'lambda_l2': 0.13719506154466946, 'num_leaves': 731, 'feature_fraction': 0.9786265480765693, 'bagging_fraction': 0.8954041961417155, 'bagging_freq': 7, 'min_child_samples': 47}. Best is trial 4 with value: 0.718118457906163.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:08:35,175]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 38.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:08:41,727]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 10.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.717942\n",
      "[100]\tvalid_0's rmse: 0.718883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:09:32,836]\u001b[0m Trial 8 finished with value: 0.718882791803884 and parameters: {'lambda_l1': 3.835406756167753e-05, 'lambda_l2': 0.005093829856641905, 'num_leaves': 482, 'feature_fraction': 0.7025559645910259, 'bagging_fraction': 0.5309409781432198, 'bagging_freq': 2, 'min_child_samples': 9}. Best is trial 4 with value: 0.718118457906163.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:09:39,837]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:09:45,665]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:09:53,837]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:09:59,903]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:04,033]\u001b[0m Trial 13 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:10,808]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:17,152]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:22,854]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:27,372]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:33,866]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:41,555]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:10:50,001]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 11.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.714823\n",
      "[100]\tvalid_0's rmse: 0.713506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:11:50,217]\u001b[0m Trial 21 finished with value: 0.7135063545467554 and parameters: {'lambda_l1': 8.267419279274886e-06, 'lambda_l2': 0.2884948686704193, 'num_leaves': 734, 'feature_fraction': 0.9980134194584929, 'bagging_fraction': 0.9025635456078424, 'bagging_freq': 7, 'min_child_samples': 42}. Best is trial 21 with value: 0.7135063545467554.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:11:56,517]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:12:03,698]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:12:10,357]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:12:17,258]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:12:23,516]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:12:31,210]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 10.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.710094\n",
      "[100]\tvalid_0's rmse: 0.708044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:13:23,681]\u001b[0m Trial 28 finished with value: 0.7080435379492719 and parameters: {'lambda_l1': 1.1015657154632068e-07, 'lambda_l2': 0.002364968762245554, 'num_leaves': 658, 'feature_fraction': 0.6589980837266314, 'bagging_fraction': 0.6582504611070202, 'bagging_freq': 4, 'min_child_samples': 35}. Best is trial 28 with value: 0.7080435379492719.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:13:31,021]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:13:40,240]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:13:46,810]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:13:55,675]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:01,962]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:08,455]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:16,918]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:25,520]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:32,687]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:40,759]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 12.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:48,091]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:14:56,186]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:15:04,159]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:15:12,008]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:15:18,600]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 10.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.715975\n",
      "[100]\tvalid_0's rmse: 0.716599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:16:20,590]\u001b[0m Trial 44 finished with value: 0.7165989189567811 and parameters: {'lambda_l1': 4.036494485036241e-06, 'lambda_l2': 0.03613398884936303, 'num_leaves': 788, 'feature_fraction': 0.8965584798809318, 'bagging_fraction': 0.9488848602881189, 'bagging_freq': 7, 'min_child_samples': 46}. Best is trial 28 with value: 0.7080435379492719.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:16:29,274]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:16:36,122]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:16:45,071]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:16:49,691]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:16:57,163]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:17:00,890]\u001b[0m Trial 50 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:17:09,207]\u001b[0m Trial 51 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:17:17,345]\u001b[0m Trial 52 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:17:26,211]\u001b[0m Trial 53 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:17:33,679]\u001b[0m Trial 54 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:17:42,597]\u001b[0m Trial 55 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:17:49,579]\u001b[0m Trial 56 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:00,536]\u001b[0m Trial 57 pruned. Trial was pruned at iteration 12.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:06,190]\u001b[0m Trial 58 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:15,455]\u001b[0m Trial 59 pruned. Trial was pruned at iteration 12.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:22,166]\u001b[0m Trial 60 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:33,646]\u001b[0m Trial 61 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:40,278]\u001b[0m Trial 62 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:50,182]\u001b[0m Trial 63 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:18:59,524]\u001b[0m Trial 64 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:19:05,634]\u001b[0m Trial 65 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:19:14,114]\u001b[0m Trial 66 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:19:23,975]\u001b[0m Trial 67 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:19:33,208]\u001b[0m Trial 68 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:19:42,099]\u001b[0m Trial 69 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:19:49,237]\u001b[0m Trial 70 pruned. Trial was pruned at iteration 11.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:19:58,059]\u001b[0m Trial 71 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:05,812]\u001b[0m Trial 72 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:15,071]\u001b[0m Trial 73 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:21,410]\u001b[0m Trial 74 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:28,756]\u001b[0m Trial 75 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:37,424]\u001b[0m Trial 76 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:44,996]\u001b[0m Trial 77 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:51,826]\u001b[0m Trial 78 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:20:59,916]\u001b[0m Trial 79 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:07,641]\u001b[0m Trial 80 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:16,058]\u001b[0m Trial 81 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:21,801]\u001b[0m Trial 82 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:26,014]\u001b[0m Trial 83 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:34,197]\u001b[0m Trial 84 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:39,069]\u001b[0m Trial 85 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:47,707]\u001b[0m Trial 86 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:21:55,341]\u001b[0m Trial 87 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:22:03,718]\u001b[0m Trial 88 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:22:10,363]\u001b[0m Trial 89 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:22:17,627]\u001b[0m Trial 90 pruned. Trial was pruned at iteration 10.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.711914\n",
      "[100]\tvalid_0's rmse: 0.7153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:23:07,224]\u001b[0m Trial 91 finished with value: 0.7152997247800995 and parameters: {'lambda_l1': 0.7182257061613085, 'lambda_l2': 0.0001574615990245783, 'num_leaves': 725, 'feature_fraction': 0.8862867245637093, 'bagging_fraction': 0.5601862743140789, 'bagging_freq': 7, 'min_child_samples': 53}. Best is trial 28 with value: 0.7080435379492719.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:23:16,240]\u001b[0m Trial 92 pruned. Trial was pruned at iteration 15.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.712565\n",
      "[100]\tvalid_0's rmse: 0.714127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:24:11,529]\u001b[0m Trial 93 finished with value: 0.7141271182783477 and parameters: {'lambda_l1': 0.31264121770185455, 'lambda_l2': 0.09096825159033363, 'num_leaves': 795, 'feature_fraction': 0.8717598539705508, 'bagging_fraction': 0.5070069218045716, 'bagging_freq': 7, 'min_child_samples': 61}. Best is trial 28 with value: 0.7080435379492719.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:24:18,531]\u001b[0m Trial 94 pruned. Trial was pruned at iteration 10.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:24:27,767]\u001b[0m Trial 95 pruned. Trial was pruned at iteration 15.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:24:33,661]\u001b[0m Trial 96 pruned. Trial was pruned at iteration 10.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.716528\n",
      "[100]\tvalid_0's rmse: 0.720147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:25:34,903]\u001b[0m Trial 97 finished with value: 0.7201473654884855 and parameters: {'lambda_l1': 0.3067001662926845, 'lambda_l2': 0.09169143406924246, 'num_leaves': 927, 'feature_fraction': 0.9095783018404083, 'bagging_fraction': 0.45047992659699576, 'bagging_freq': 7, 'min_child_samples': 57}. Best is trial 28 with value: 0.7080435379492719.\u001b[0m\n",
      "\u001b[32m[I 2021-08-27 13:25:42,293]\u001b[0m Trial 98 pruned. Trial was pruned at iteration 10.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\tvalid_0's rmse: 0.711659\n",
      "[100]\tvalid_0's rmse: 0.715087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-08-27 13:26:45,388]\u001b[0m Trial 99 finished with value: 0.7150869042095217 and parameters: {'lambda_l1': 1.481382977954051, 'lambda_l2': 0.7182924339637471, 'num_leaves': 985, 'feature_fraction': 0.9147662517689658, 'bagging_fraction': 0.43697884995341124, 'bagging_freq': 7, 'min_child_samples': 56}. Best is trial 28 with value: 0.7080435379492719.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cat_feats = ['item_category_id', 'month', 'season']\n",
    "\n",
    "last_block = 33\n",
    "dates = matrix['date_block_num']\n",
    "\n",
    "# drop target and some features that lead to overfitting\n",
    "cols_to_drop = ['item_cnt_month', 'new_item', 'shop_id', 'item_id']\n",
    "\n",
    "# split dataset on train and test sets for model training\n",
    "X_train = matrix.loc[dates <  last_block].drop(cols_to_drop, axis=1)\n",
    "X_val = matrix.loc[dates ==  last_block].drop(cols_to_drop, axis=1)\n",
    "X_test = matrix.loc[dates ==  34].drop(cols_to_drop, axis=1)\n",
    "\n",
    "y_train = matrix.loc[dates <  last_block, 'item_cnt_month']\n",
    "y_val = matrix.loc[dates ==  last_block, 'item_cnt_month']\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, y_train)\n",
    "dvalid = lgb.Dataset(X_val, y_val, reference=dtrain)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"objective\": \"mse\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"num_iterations\": 1000,\n",
    "        \"verbosity\": 0,\n",
    "        \"device\": \"gpu\",\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 1024),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"feature_pre_filter\": False\n",
    "    }\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\")\n",
    "    gbm = lgb.train(\n",
    "            param, \n",
    "            dtrain, \n",
    "            valid_sets=(dvalid), \n",
    "            verbose_eval=50,\n",
    "            categorical_feature=cat_feats,\n",
    "            callbacks=[pruning_callback]\n",
    "    )\n",
    "\n",
    "    y_pred = gbm.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_bagging_fraction</th>\n",
       "      <th>params_bagging_freq</th>\n",
       "      <th>params_feature_fraction</th>\n",
       "      <th>params_lambda_l1</th>\n",
       "      <th>params_lambda_l2</th>\n",
       "      <th>params_min_child_samples</th>\n",
       "      <th>params_num_leaves</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.708044</td>\n",
       "      <td>2021-08-27 13:12:31.210642</td>\n",
       "      <td>2021-08-27 13:13:23.681055</td>\n",
       "      <td>0 days 00:00:52.470413</td>\n",
       "      <td>0.658250</td>\n",
       "      <td>4</td>\n",
       "      <td>0.658998</td>\n",
       "      <td>1.101566e-07</td>\n",
       "      <td>2.364969e-03</td>\n",
       "      <td>35</td>\n",
       "      <td>658</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.713506</td>\n",
       "      <td>2021-08-27 13:10:50.001660</td>\n",
       "      <td>2021-08-27 13:11:50.216615</td>\n",
       "      <td>0 days 00:01:00.214955</td>\n",
       "      <td>0.902564</td>\n",
       "      <td>7</td>\n",
       "      <td>0.998013</td>\n",
       "      <td>8.267419e-06</td>\n",
       "      <td>2.884949e-01</td>\n",
       "      <td>42</td>\n",
       "      <td>734</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>0.714127</td>\n",
       "      <td>2021-08-27 13:23:16.241032</td>\n",
       "      <td>2021-08-27 13:24:11.528775</td>\n",
       "      <td>0 days 00:00:55.287743</td>\n",
       "      <td>0.507007</td>\n",
       "      <td>7</td>\n",
       "      <td>0.871760</td>\n",
       "      <td>3.126412e-01</td>\n",
       "      <td>9.096825e-02</td>\n",
       "      <td>61</td>\n",
       "      <td>795</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>0.715087</td>\n",
       "      <td>2021-08-27 13:25:42.294565</td>\n",
       "      <td>2021-08-27 13:26:45.388114</td>\n",
       "      <td>0 days 00:01:03.093549</td>\n",
       "      <td>0.436979</td>\n",
       "      <td>7</td>\n",
       "      <td>0.914766</td>\n",
       "      <td>1.481383e+00</td>\n",
       "      <td>7.182924e-01</td>\n",
       "      <td>56</td>\n",
       "      <td>985</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>0.715300</td>\n",
       "      <td>2021-08-27 13:22:17.628726</td>\n",
       "      <td>2021-08-27 13:23:07.223239</td>\n",
       "      <td>0 days 00:00:49.594513</td>\n",
       "      <td>0.560186</td>\n",
       "      <td>7</td>\n",
       "      <td>0.886287</td>\n",
       "      <td>7.182257e-01</td>\n",
       "      <td>1.574616e-04</td>\n",
       "      <td>53</td>\n",
       "      <td>725</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>0.716599</td>\n",
       "      <td>2021-08-27 13:15:18.602552</td>\n",
       "      <td>2021-08-27 13:16:20.589662</td>\n",
       "      <td>0 days 00:01:01.987110</td>\n",
       "      <td>0.948885</td>\n",
       "      <td>7</td>\n",
       "      <td>0.896558</td>\n",
       "      <td>4.036494e-06</td>\n",
       "      <td>3.613399e-02</td>\n",
       "      <td>46</td>\n",
       "      <td>788</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.718118</td>\n",
       "      <td>2021-08-27 13:06:32.333506</td>\n",
       "      <td>2021-08-27 13:07:16.385107</td>\n",
       "      <td>0 days 00:00:44.051601</td>\n",
       "      <td>0.622979</td>\n",
       "      <td>6</td>\n",
       "      <td>0.883619</td>\n",
       "      <td>5.518957e-01</td>\n",
       "      <td>4.791927e-03</td>\n",
       "      <td>86</td>\n",
       "      <td>487</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.718883</td>\n",
       "      <td>2021-08-27 13:08:41.727986</td>\n",
       "      <td>2021-08-27 13:09:32.835840</td>\n",
       "      <td>0 days 00:00:51.107854</td>\n",
       "      <td>0.530941</td>\n",
       "      <td>2</td>\n",
       "      <td>0.702556</td>\n",
       "      <td>3.835407e-05</td>\n",
       "      <td>5.093830e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>482</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.720140</td>\n",
       "      <td>2021-08-27 13:07:16.386201</td>\n",
       "      <td>2021-08-27 13:08:15.245138</td>\n",
       "      <td>0 days 00:00:58.858937</td>\n",
       "      <td>0.895404</td>\n",
       "      <td>7</td>\n",
       "      <td>0.978627</td>\n",
       "      <td>2.642709e-06</td>\n",
       "      <td>1.371951e-01</td>\n",
       "      <td>47</td>\n",
       "      <td>731</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.720147</td>\n",
       "      <td>2021-08-27 13:24:33.661803</td>\n",
       "      <td>2021-08-27 13:25:34.902695</td>\n",
       "      <td>0 days 00:01:01.240892</td>\n",
       "      <td>0.450480</td>\n",
       "      <td>7</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>3.067002e-01</td>\n",
       "      <td>9.169143e-02</td>\n",
       "      <td>57</td>\n",
       "      <td>927</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.721574</td>\n",
       "      <td>2021-08-27 13:04:09.365247</td>\n",
       "      <td>2021-08-27 13:05:38.591146</td>\n",
       "      <td>0 days 00:01:29.225899</td>\n",
       "      <td>0.982915</td>\n",
       "      <td>2</td>\n",
       "      <td>0.850711</td>\n",
       "      <td>1.088013e-06</td>\n",
       "      <td>6.093378e-07</td>\n",
       "      <td>53</td>\n",
       "      <td>879</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.722866</td>\n",
       "      <td>2021-08-27 13:03:10.548233</td>\n",
       "      <td>2021-08-27 13:04:09.364149</td>\n",
       "      <td>0 days 00:00:58.815916</td>\n",
       "      <td>0.979114</td>\n",
       "      <td>5</td>\n",
       "      <td>0.831542</td>\n",
       "      <td>8.479742e-08</td>\n",
       "      <td>5.299536e-03</td>\n",
       "      <td>63</td>\n",
       "      <td>585</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.723908</td>\n",
       "      <td>2021-08-27 13:08:15.246237</td>\n",
       "      <td>2021-08-27 13:08:35.175069</td>\n",
       "      <td>0 days 00:00:19.928832</td>\n",
       "      <td>0.416508</td>\n",
       "      <td>5</td>\n",
       "      <td>0.889191</td>\n",
       "      <td>5.417912e-05</td>\n",
       "      <td>7.777508e-06</td>\n",
       "      <td>73</td>\n",
       "      <td>899</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.726431</td>\n",
       "      <td>2021-08-27 13:02:17.480186</td>\n",
       "      <td>2021-08-27 13:03:10.546183</td>\n",
       "      <td>0 days 00:00:53.065997</td>\n",
       "      <td>0.802845</td>\n",
       "      <td>2</td>\n",
       "      <td>0.723706</td>\n",
       "      <td>2.880148e-02</td>\n",
       "      <td>1.534117e-05</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.730093</td>\n",
       "      <td>2021-08-27 13:05:38.592306</td>\n",
       "      <td>2021-08-27 13:06:32.332425</td>\n",
       "      <td>0 days 00:00:53.740119</td>\n",
       "      <td>0.625324</td>\n",
       "      <td>7</td>\n",
       "      <td>0.865420</td>\n",
       "      <td>1.518082e-01</td>\n",
       "      <td>1.883852e-04</td>\n",
       "      <td>50</td>\n",
       "      <td>818</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>0.737619</td>\n",
       "      <td>2021-08-27 13:24:18.532631</td>\n",
       "      <td>2021-08-27 13:24:27.767238</td>\n",
       "      <td>0 days 00:00:09.234607</td>\n",
       "      <td>0.543175</td>\n",
       "      <td>7</td>\n",
       "      <td>0.898543</td>\n",
       "      <td>3.977534e-01</td>\n",
       "      <td>3.005262e-01</td>\n",
       "      <td>72</td>\n",
       "      <td>749</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>0.737948</td>\n",
       "      <td>2021-08-27 13:23:07.226378</td>\n",
       "      <td>2021-08-27 13:23:16.240340</td>\n",
       "      <td>0 days 00:00:09.013962</td>\n",
       "      <td>0.528650</td>\n",
       "      <td>7</td>\n",
       "      <td>0.847325</td>\n",
       "      <td>1.985084e+00</td>\n",
       "      <td>1.122925e-05</td>\n",
       "      <td>60</td>\n",
       "      <td>726</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>0.750580</td>\n",
       "      <td>2021-08-27 13:17:49.579967</td>\n",
       "      <td>2021-08-27 13:18:00.536015</td>\n",
       "      <td>0 days 00:00:10.956048</td>\n",
       "      <td>0.951203</td>\n",
       "      <td>6</td>\n",
       "      <td>0.947216</td>\n",
       "      <td>1.587140e-04</td>\n",
       "      <td>2.652866e-01</td>\n",
       "      <td>75</td>\n",
       "      <td>845</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>0.751132</td>\n",
       "      <td>2021-08-27 13:18:06.190855</td>\n",
       "      <td>2021-08-27 13:18:15.455659</td>\n",
       "      <td>0 days 00:00:09.264804</td>\n",
       "      <td>0.831404</td>\n",
       "      <td>6</td>\n",
       "      <td>0.813772</td>\n",
       "      <td>5.713136e-05</td>\n",
       "      <td>6.397647e-02</td>\n",
       "      <td>46</td>\n",
       "      <td>692</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>0.751528</td>\n",
       "      <td>2021-08-27 13:14:32.688529</td>\n",
       "      <td>2021-08-27 13:14:40.759154</td>\n",
       "      <td>0 days 00:00:08.070625</td>\n",
       "      <td>0.658063</td>\n",
       "      <td>7</td>\n",
       "      <td>0.887198</td>\n",
       "      <td>4.911515e-08</td>\n",
       "      <td>5.129087e-02</td>\n",
       "      <td>44</td>\n",
       "      <td>759</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.758125</td>\n",
       "      <td>2021-08-27 13:19:05.635261</td>\n",
       "      <td>2021-08-27 13:19:14.114215</td>\n",
       "      <td>0 days 00:00:08.478954</td>\n",
       "      <td>0.985928</td>\n",
       "      <td>1</td>\n",
       "      <td>0.991682</td>\n",
       "      <td>5.539106e-02</td>\n",
       "      <td>1.287772e-02</td>\n",
       "      <td>22</td>\n",
       "      <td>1020</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>0.758320</td>\n",
       "      <td>2021-08-27 13:17:33.680351</td>\n",
       "      <td>2021-08-27 13:17:42.597930</td>\n",
       "      <td>0 days 00:00:08.917579</td>\n",
       "      <td>0.590949</td>\n",
       "      <td>2</td>\n",
       "      <td>0.605680</td>\n",
       "      <td>6.674729e-01</td>\n",
       "      <td>2.834474e-02</td>\n",
       "      <td>80</td>\n",
       "      <td>796</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>0.758821</td>\n",
       "      <td>2021-08-27 13:18:40.279146</td>\n",
       "      <td>2021-08-27 13:18:50.182868</td>\n",
       "      <td>0 days 00:00:09.903722</td>\n",
       "      <td>0.622767</td>\n",
       "      <td>2</td>\n",
       "      <td>0.877765</td>\n",
       "      <td>3.638984e-06</td>\n",
       "      <td>1.343313e-08</td>\n",
       "      <td>54</td>\n",
       "      <td>959</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>0.758824</td>\n",
       "      <td>2021-08-27 13:19:42.100350</td>\n",
       "      <td>2021-08-27 13:19:49.237487</td>\n",
       "      <td>0 days 00:00:07.137137</td>\n",
       "      <td>0.494521</td>\n",
       "      <td>2</td>\n",
       "      <td>0.903153</td>\n",
       "      <td>2.943433e-08</td>\n",
       "      <td>9.654339e-07</td>\n",
       "      <td>94</td>\n",
       "      <td>393</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.760361</td>\n",
       "      <td>2021-08-27 13:12:17.260115</td>\n",
       "      <td>2021-08-27 13:12:23.515998</td>\n",
       "      <td>0 days 00:00:06.255883</td>\n",
       "      <td>0.573552</td>\n",
       "      <td>6</td>\n",
       "      <td>0.759789</td>\n",
       "      <td>8.364868e+00</td>\n",
       "      <td>7.056987e-05</td>\n",
       "      <td>85</td>\n",
       "      <td>554</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.761326</td>\n",
       "      <td>2021-08-27 13:10:41.555665</td>\n",
       "      <td>2021-08-27 13:10:50.000982</td>\n",
       "      <td>0 days 00:00:08.445317</td>\n",
       "      <td>0.800796</td>\n",
       "      <td>4</td>\n",
       "      <td>0.950380</td>\n",
       "      <td>3.395240e-02</td>\n",
       "      <td>1.799713e-02</td>\n",
       "      <td>62</td>\n",
       "      <td>667</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>0.767038</td>\n",
       "      <td>2021-08-27 13:24:11.530106</td>\n",
       "      <td>2021-08-27 13:24:18.531878</td>\n",
       "      <td>0 days 00:00:07.001772</td>\n",
       "      <td>0.567192</td>\n",
       "      <td>7</td>\n",
       "      <td>0.875756</td>\n",
       "      <td>7.743221e-01</td>\n",
       "      <td>1.465344e-01</td>\n",
       "      <td>65</td>\n",
       "      <td>787</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>0.767158</td>\n",
       "      <td>2021-08-27 13:21:55.342299</td>\n",
       "      <td>2021-08-27 13:22:03.718542</td>\n",
       "      <td>0 days 00:00:08.376243</td>\n",
       "      <td>0.877605</td>\n",
       "      <td>6</td>\n",
       "      <td>0.969651</td>\n",
       "      <td>4.358613e-04</td>\n",
       "      <td>2.200940e-03</td>\n",
       "      <td>48</td>\n",
       "      <td>777</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>0.767218</td>\n",
       "      <td>2021-08-27 13:19:23.976231</td>\n",
       "      <td>2021-08-27 13:19:33.208119</td>\n",
       "      <td>0 days 00:00:09.231888</td>\n",
       "      <td>0.933126</td>\n",
       "      <td>4</td>\n",
       "      <td>0.557274</td>\n",
       "      <td>4.290932e-07</td>\n",
       "      <td>4.665560e-03</td>\n",
       "      <td>63</td>\n",
       "      <td>722</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.767295</td>\n",
       "      <td>2021-08-27 13:17:17.345686</td>\n",
       "      <td>2021-08-27 13:17:26.211866</td>\n",
       "      <td>0 days 00:00:08.866180</td>\n",
       "      <td>0.754057</td>\n",
       "      <td>7</td>\n",
       "      <td>0.985290</td>\n",
       "      <td>2.472815e-08</td>\n",
       "      <td>5.505584e-03</td>\n",
       "      <td>12</td>\n",
       "      <td>938</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>0.767417</td>\n",
       "      <td>2021-08-27 13:18:22.167427</td>\n",
       "      <td>2021-08-27 13:18:33.646873</td>\n",
       "      <td>0 days 00:00:11.479446</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>2</td>\n",
       "      <td>0.844439</td>\n",
       "      <td>9.774791e-07</td>\n",
       "      <td>1.428637e-07</td>\n",
       "      <td>43</td>\n",
       "      <td>760</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>0.767627</td>\n",
       "      <td>2021-08-27 13:16:29.275454</td>\n",
       "      <td>2021-08-27 13:16:36.122696</td>\n",
       "      <td>0 days 00:00:06.847242</td>\n",
       "      <td>0.605381</td>\n",
       "      <td>7</td>\n",
       "      <td>0.401215</td>\n",
       "      <td>1.206561e-05</td>\n",
       "      <td>7.101714e-03</td>\n",
       "      <td>32</td>\n",
       "      <td>885</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52</td>\n",
       "      <td>0.767676</td>\n",
       "      <td>2021-08-27 13:17:09.208410</td>\n",
       "      <td>2021-08-27 13:17:17.344996</td>\n",
       "      <td>0 days 00:00:08.136586</td>\n",
       "      <td>0.869954</td>\n",
       "      <td>6</td>\n",
       "      <td>0.948089</td>\n",
       "      <td>2.395939e-05</td>\n",
       "      <td>1.188107e-01</td>\n",
       "      <td>63</td>\n",
       "      <td>670</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>0.767762</td>\n",
       "      <td>2021-08-27 13:25:34.903969</td>\n",
       "      <td>2021-08-27 13:25:42.293812</td>\n",
       "      <td>0 days 00:00:07.389843</td>\n",
       "      <td>0.519510</td>\n",
       "      <td>7</td>\n",
       "      <td>0.981616</td>\n",
       "      <td>1.246251e-01</td>\n",
       "      <td>9.349884e-02</td>\n",
       "      <td>61</td>\n",
       "      <td>919</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.767968</td>\n",
       "      <td>2021-08-27 13:13:46.810834</td>\n",
       "      <td>2021-08-27 13:13:55.675620</td>\n",
       "      <td>0 days 00:00:08.864786</td>\n",
       "      <td>0.669471</td>\n",
       "      <td>2</td>\n",
       "      <td>0.726275</td>\n",
       "      <td>6.071006e-08</td>\n",
       "      <td>7.835309e-05</td>\n",
       "      <td>58</td>\n",
       "      <td>629</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.768166</td>\n",
       "      <td>2021-08-27 13:15:04.160423</td>\n",
       "      <td>2021-08-27 13:15:12.008053</td>\n",
       "      <td>0 days 00:00:07.847630</td>\n",
       "      <td>0.914591</td>\n",
       "      <td>6</td>\n",
       "      <td>0.929355</td>\n",
       "      <td>3.157757e-05</td>\n",
       "      <td>2.105196e+00</td>\n",
       "      <td>59</td>\n",
       "      <td>617</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.768832</td>\n",
       "      <td>2021-08-27 13:11:56.517802</td>\n",
       "      <td>2021-08-27 13:12:03.698303</td>\n",
       "      <td>0 days 00:00:07.180501</td>\n",
       "      <td>0.800542</td>\n",
       "      <td>6</td>\n",
       "      <td>0.799030</td>\n",
       "      <td>6.830681e-06</td>\n",
       "      <td>1.007389e-03</td>\n",
       "      <td>25</td>\n",
       "      <td>598</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.768931</td>\n",
       "      <td>2021-08-27 13:09:53.838013</td>\n",
       "      <td>2021-08-27 13:09:59.903861</td>\n",
       "      <td>0 days 00:00:06.065848</td>\n",
       "      <td>0.565211</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999399</td>\n",
       "      <td>3.642750e-04</td>\n",
       "      <td>3.095080e-03</td>\n",
       "      <td>28</td>\n",
       "      <td>440</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.768949</td>\n",
       "      <td>2021-08-27 13:10:27.372819</td>\n",
       "      <td>2021-08-27 13:10:33.866284</td>\n",
       "      <td>0 days 00:00:06.493465</td>\n",
       "      <td>0.424511</td>\n",
       "      <td>6</td>\n",
       "      <td>0.764863</td>\n",
       "      <td>3.073610e-05</td>\n",
       "      <td>1.479175e-08</td>\n",
       "      <td>38</td>\n",
       "      <td>1023</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.769021</td>\n",
       "      <td>2021-08-27 13:11:50.217693</td>\n",
       "      <td>2021-08-27 13:11:56.517230</td>\n",
       "      <td>0 days 00:00:06.299537</td>\n",
       "      <td>0.683418</td>\n",
       "      <td>7</td>\n",
       "      <td>0.934442</td>\n",
       "      <td>1.518440e-05</td>\n",
       "      <td>4.909720e-01</td>\n",
       "      <td>42</td>\n",
       "      <td>468</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.769023</td>\n",
       "      <td>2021-08-27 13:09:45.666613</td>\n",
       "      <td>2021-08-27 13:09:53.837339</td>\n",
       "      <td>0 days 00:00:08.170726</td>\n",
       "      <td>0.469598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704985</td>\n",
       "      <td>3.380262e-03</td>\n",
       "      <td>3.503520e-02</td>\n",
       "      <td>9</td>\n",
       "      <td>397</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>0.769110</td>\n",
       "      <td>2021-08-27 13:20:37.425532</td>\n",
       "      <td>2021-08-27 13:20:44.996219</td>\n",
       "      <td>0 days 00:00:07.570687</td>\n",
       "      <td>0.707847</td>\n",
       "      <td>4</td>\n",
       "      <td>0.715820</td>\n",
       "      <td>3.226931e+00</td>\n",
       "      <td>3.800558e-02</td>\n",
       "      <td>57</td>\n",
       "      <td>698</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>0.769138</td>\n",
       "      <td>2021-08-27 13:22:03.719224</td>\n",
       "      <td>2021-08-27 13:22:10.362976</td>\n",
       "      <td>0 days 00:00:06.643752</td>\n",
       "      <td>0.968485</td>\n",
       "      <td>1</td>\n",
       "      <td>0.832427</td>\n",
       "      <td>3.325487e-05</td>\n",
       "      <td>7.763370e-03</td>\n",
       "      <td>17</td>\n",
       "      <td>832</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>0.769308</td>\n",
       "      <td>2021-08-27 13:19:49.238207</td>\n",
       "      <td>2021-08-27 13:19:58.059232</td>\n",
       "      <td>0 days 00:00:08.821025</td>\n",
       "      <td>0.981030</td>\n",
       "      <td>5</td>\n",
       "      <td>0.825681</td>\n",
       "      <td>1.044438e-07</td>\n",
       "      <td>1.026588e-02</td>\n",
       "      <td>50</td>\n",
       "      <td>602</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>0.769458</td>\n",
       "      <td>2021-08-27 13:17:42.598599</td>\n",
       "      <td>2021-08-27 13:17:49.579274</td>\n",
       "      <td>0 days 00:00:06.980675</td>\n",
       "      <td>0.900319</td>\n",
       "      <td>7</td>\n",
       "      <td>0.507932</td>\n",
       "      <td>3.103591e-06</td>\n",
       "      <td>4.788268e-04</td>\n",
       "      <td>26</td>\n",
       "      <td>423</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>0.769494</td>\n",
       "      <td>2021-08-27 13:19:33.208737</td>\n",
       "      <td>2021-08-27 13:19:42.099679</td>\n",
       "      <td>0 days 00:00:08.890942</td>\n",
       "      <td>0.886084</td>\n",
       "      <td>7</td>\n",
       "      <td>0.933208</td>\n",
       "      <td>8.223894e-08</td>\n",
       "      <td>1.511021e-03</td>\n",
       "      <td>47</td>\n",
       "      <td>901</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>0.770187</td>\n",
       "      <td>2021-08-27 13:20:28.756781</td>\n",
       "      <td>2021-08-27 13:20:37.424855</td>\n",
       "      <td>0 days 00:00:08.668074</td>\n",
       "      <td>0.971963</td>\n",
       "      <td>7</td>\n",
       "      <td>0.919789</td>\n",
       "      <td>5.543593e-05</td>\n",
       "      <td>1.162344e-01</td>\n",
       "      <td>40</td>\n",
       "      <td>589</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>0.770280</td>\n",
       "      <td>2021-08-27 13:20:05.812737</td>\n",
       "      <td>2021-08-27 13:20:15.071748</td>\n",
       "      <td>0 days 00:00:09.259011</td>\n",
       "      <td>0.907747</td>\n",
       "      <td>5</td>\n",
       "      <td>0.858431</td>\n",
       "      <td>2.114240e-06</td>\n",
       "      <td>3.222881e-03</td>\n",
       "      <td>55</td>\n",
       "      <td>814</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>0.770368</td>\n",
       "      <td>2021-08-27 13:20:51.827249</td>\n",
       "      <td>2021-08-27 13:20:59.916585</td>\n",
       "      <td>0 days 00:00:08.089336</td>\n",
       "      <td>0.781763</td>\n",
       "      <td>6</td>\n",
       "      <td>0.622447</td>\n",
       "      <td>1.053101e-08</td>\n",
       "      <td>1.595870e-04</td>\n",
       "      <td>51</td>\n",
       "      <td>867</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.770462</td>\n",
       "      <td>2021-08-27 13:12:23.516559</td>\n",
       "      <td>2021-08-27 13:12:31.210111</td>\n",
       "      <td>0 days 00:00:07.693552</td>\n",
       "      <td>0.758471</td>\n",
       "      <td>7</td>\n",
       "      <td>0.918985</td>\n",
       "      <td>1.166228e-04</td>\n",
       "      <td>2.194392e-02</td>\n",
       "      <td>56</td>\n",
       "      <td>783</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>0.770466</td>\n",
       "      <td>2021-08-27 13:22:10.363661</td>\n",
       "      <td>2021-08-27 13:22:17.627964</td>\n",
       "      <td>0 days 00:00:07.264303</td>\n",
       "      <td>0.693583</td>\n",
       "      <td>3</td>\n",
       "      <td>0.938281</td>\n",
       "      <td>4.026817e-06</td>\n",
       "      <td>1.214226e+00</td>\n",
       "      <td>25</td>\n",
       "      <td>421</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.770648</td>\n",
       "      <td>2021-08-27 13:09:32.836951</td>\n",
       "      <td>2021-08-27 13:09:39.837743</td>\n",
       "      <td>0 days 00:00:07.000792</td>\n",
       "      <td>0.575964</td>\n",
       "      <td>3</td>\n",
       "      <td>0.664197</td>\n",
       "      <td>1.226393e-06</td>\n",
       "      <td>3.330423e-04</td>\n",
       "      <td>78</td>\n",
       "      <td>598</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>0.770735</td>\n",
       "      <td>2021-08-27 13:20:44.996894</td>\n",
       "      <td>2021-08-27 13:20:51.826519</td>\n",
       "      <td>0 days 00:00:06.829625</td>\n",
       "      <td>0.584474</td>\n",
       "      <td>7</td>\n",
       "      <td>0.998172</td>\n",
       "      <td>9.463318e-06</td>\n",
       "      <td>3.807815e-01</td>\n",
       "      <td>89</td>\n",
       "      <td>743</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>0.770869</td>\n",
       "      <td>2021-08-27 13:16:20.590871</td>\n",
       "      <td>2021-08-27 13:16:29.274887</td>\n",
       "      <td>0 days 00:00:08.684016</td>\n",
       "      <td>0.948831</td>\n",
       "      <td>6</td>\n",
       "      <td>0.831883</td>\n",
       "      <td>1.483224e-04</td>\n",
       "      <td>3.381353e-02</td>\n",
       "      <td>40</td>\n",
       "      <td>808</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>0.771027</td>\n",
       "      <td>2021-08-27 13:16:49.692464</td>\n",
       "      <td>2021-08-27 13:16:57.163573</td>\n",
       "      <td>0 days 00:00:07.471109</td>\n",
       "      <td>0.925726</td>\n",
       "      <td>6</td>\n",
       "      <td>0.762673</td>\n",
       "      <td>4.990846e-06</td>\n",
       "      <td>4.920306e-02</td>\n",
       "      <td>46</td>\n",
       "      <td>492</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>0.771193</td>\n",
       "      <td>2021-08-27 13:17:26.212546</td>\n",
       "      <td>2021-08-27 13:17:33.679675</td>\n",
       "      <td>0 days 00:00:07.467129</td>\n",
       "      <td>0.648191</td>\n",
       "      <td>3</td>\n",
       "      <td>0.901529</td>\n",
       "      <td>1.216175e-06</td>\n",
       "      <td>1.181075e-03</td>\n",
       "      <td>52</td>\n",
       "      <td>584</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>0.771424</td>\n",
       "      <td>2021-08-27 13:18:33.647458</td>\n",
       "      <td>2021-08-27 13:18:40.278572</td>\n",
       "      <td>0 days 00:00:06.631114</td>\n",
       "      <td>0.916684</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661085</td>\n",
       "      <td>8.152381e-06</td>\n",
       "      <td>1.260804e-06</td>\n",
       "      <td>85</td>\n",
       "      <td>863</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.771449</td>\n",
       "      <td>2021-08-27 13:14:25.520853</td>\n",
       "      <td>2021-08-27 13:14:32.687848</td>\n",
       "      <td>0 days 00:00:07.166995</td>\n",
       "      <td>0.450414</td>\n",
       "      <td>5</td>\n",
       "      <td>0.810464</td>\n",
       "      <td>1.047479e-03</td>\n",
       "      <td>4.677810e-01</td>\n",
       "      <td>51</td>\n",
       "      <td>905</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>0.771869</td>\n",
       "      <td>2021-08-27 13:19:14.114920</td>\n",
       "      <td>2021-08-27 13:19:23.975538</td>\n",
       "      <td>0 days 00:00:09.860618</td>\n",
       "      <td>0.818609</td>\n",
       "      <td>2</td>\n",
       "      <td>0.955693</td>\n",
       "      <td>7.771041e-06</td>\n",
       "      <td>3.393971e-08</td>\n",
       "      <td>36</td>\n",
       "      <td>638</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>0.771884</td>\n",
       "      <td>2021-08-27 13:20:21.411633</td>\n",
       "      <td>2021-08-27 13:20:28.756036</td>\n",
       "      <td>0 days 00:00:07.344403</td>\n",
       "      <td>0.870359</td>\n",
       "      <td>6</td>\n",
       "      <td>0.960290</td>\n",
       "      <td>3.834061e-08</td>\n",
       "      <td>2.133124e-02</td>\n",
       "      <td>29</td>\n",
       "      <td>509</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.771980</td>\n",
       "      <td>2021-08-27 13:14:16.918875</td>\n",
       "      <td>2021-08-27 13:14:25.520191</td>\n",
       "      <td>0 days 00:00:08.601316</td>\n",
       "      <td>0.553430</td>\n",
       "      <td>2</td>\n",
       "      <td>0.841946</td>\n",
       "      <td>5.614443e-07</td>\n",
       "      <td>1.411549e-04</td>\n",
       "      <td>16</td>\n",
       "      <td>844</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.772136</td>\n",
       "      <td>2021-08-27 13:12:10.358080</td>\n",
       "      <td>2021-08-27 13:12:17.258471</td>\n",
       "      <td>0 days 00:00:06.900391</td>\n",
       "      <td>0.893901</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>3.180115e-04</td>\n",
       "      <td>1.666280e-01</td>\n",
       "      <td>22</td>\n",
       "      <td>305</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>0.772148</td>\n",
       "      <td>2021-08-27 13:20:15.072449</td>\n",
       "      <td>2021-08-27 13:20:21.410892</td>\n",
       "      <td>0 days 00:00:06.338443</td>\n",
       "      <td>0.406803</td>\n",
       "      <td>3</td>\n",
       "      <td>0.883742</td>\n",
       "      <td>6.848013e-07</td>\n",
       "      <td>2.890853e-04</td>\n",
       "      <td>44</td>\n",
       "      <td>651</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.772303</td>\n",
       "      <td>2021-08-27 13:13:23.682203</td>\n",
       "      <td>2021-08-27 13:13:31.021631</td>\n",
       "      <td>0 days 00:00:07.339428</td>\n",
       "      <td>0.641492</td>\n",
       "      <td>4</td>\n",
       "      <td>0.600981</td>\n",
       "      <td>1.861297e-07</td>\n",
       "      <td>1.803607e-03</td>\n",
       "      <td>36</td>\n",
       "      <td>643</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>0.772368</td>\n",
       "      <td>2021-08-27 13:14:56.187333</td>\n",
       "      <td>2021-08-27 13:15:04.159750</td>\n",
       "      <td>0 days 00:00:07.972417</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>7</td>\n",
       "      <td>0.978261</td>\n",
       "      <td>1.617385e-06</td>\n",
       "      <td>1.926283e-01</td>\n",
       "      <td>49</td>\n",
       "      <td>714</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.772518</td>\n",
       "      <td>2021-08-27 13:10:04.033988</td>\n",
       "      <td>2021-08-27 13:10:10.808476</td>\n",
       "      <td>0 days 00:00:06.774488</td>\n",
       "      <td>0.503582</td>\n",
       "      <td>3</td>\n",
       "      <td>0.791015</td>\n",
       "      <td>9.423025e-05</td>\n",
       "      <td>8.806093e-02</td>\n",
       "      <td>28</td>\n",
       "      <td>696</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.772557</td>\n",
       "      <td>2021-08-27 13:14:01.963482</td>\n",
       "      <td>2021-08-27 13:14:08.455097</td>\n",
       "      <td>0 days 00:00:06.491615</td>\n",
       "      <td>0.716365</td>\n",
       "      <td>6</td>\n",
       "      <td>0.870078</td>\n",
       "      <td>1.371724e-05</td>\n",
       "      <td>8.339633e-02</td>\n",
       "      <td>30</td>\n",
       "      <td>529</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>2021-08-27 13:16:36.123407</td>\n",
       "      <td>2021-08-27 13:16:45.071938</td>\n",
       "      <td>0 days 00:00:08.948531</td>\n",
       "      <td>0.961805</td>\n",
       "      <td>5</td>\n",
       "      <td>0.865799</td>\n",
       "      <td>4.899248e-05</td>\n",
       "      <td>2.156209e-01</td>\n",
       "      <td>54</td>\n",
       "      <td>671</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>0.773031</td>\n",
       "      <td>2021-08-27 13:21:34.197891</td>\n",
       "      <td>2021-08-27 13:21:39.069150</td>\n",
       "      <td>0 days 00:00:04.871259</td>\n",
       "      <td>0.793624</td>\n",
       "      <td>7</td>\n",
       "      <td>0.775362</td>\n",
       "      <td>7.649725e+00</td>\n",
       "      <td>4.756223e-07</td>\n",
       "      <td>41</td>\n",
       "      <td>123</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.773566</td>\n",
       "      <td>2021-08-27 13:13:31.022310</td>\n",
       "      <td>2021-08-27 13:13:40.240536</td>\n",
       "      <td>0 days 00:00:09.218226</td>\n",
       "      <td>0.848085</td>\n",
       "      <td>5</td>\n",
       "      <td>0.645629</td>\n",
       "      <td>1.594343e-08</td>\n",
       "      <td>3.229903e-06</td>\n",
       "      <td>43</td>\n",
       "      <td>972</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>0.773626</td>\n",
       "      <td>2021-08-27 13:16:45.072649</td>\n",
       "      <td>2021-08-27 13:16:49.691800</td>\n",
       "      <td>0 days 00:00:04.619151</td>\n",
       "      <td>0.999724</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894494</td>\n",
       "      <td>1.818677e-07</td>\n",
       "      <td>2.722695e-03</td>\n",
       "      <td>68</td>\n",
       "      <td>337</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>51</td>\n",
       "      <td>0.773825</td>\n",
       "      <td>2021-08-27 13:17:00.891130</td>\n",
       "      <td>2021-08-27 13:17:09.207737</td>\n",
       "      <td>0 days 00:00:08.316607</td>\n",
       "      <td>0.894954</td>\n",
       "      <td>7</td>\n",
       "      <td>0.971953</td>\n",
       "      <td>2.107827e-06</td>\n",
       "      <td>1.105121e+00</td>\n",
       "      <td>38</td>\n",
       "      <td>741</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>0.774387</td>\n",
       "      <td>2021-08-27 13:21:39.069854</td>\n",
       "      <td>2021-08-27 13:21:47.707752</td>\n",
       "      <td>0 days 00:00:08.637898</td>\n",
       "      <td>0.633728</td>\n",
       "      <td>2</td>\n",
       "      <td>0.737063</td>\n",
       "      <td>1.040618e-01</td>\n",
       "      <td>6.389703e-08</td>\n",
       "      <td>36</td>\n",
       "      <td>677</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>0.774487</td>\n",
       "      <td>2021-08-27 13:18:50.183436</td>\n",
       "      <td>2021-08-27 13:18:59.524202</td>\n",
       "      <td>0 days 00:00:09.340766</td>\n",
       "      <td>0.963687</td>\n",
       "      <td>4</td>\n",
       "      <td>0.855338</td>\n",
       "      <td>1.350393e-07</td>\n",
       "      <td>2.969570e-05</td>\n",
       "      <td>59</td>\n",
       "      <td>777</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.774496</td>\n",
       "      <td>2021-08-27 13:10:17.153354</td>\n",
       "      <td>2021-08-27 13:10:22.854524</td>\n",
       "      <td>0 days 00:00:05.701170</td>\n",
       "      <td>0.531336</td>\n",
       "      <td>3</td>\n",
       "      <td>0.471741</td>\n",
       "      <td>1.969507e-01</td>\n",
       "      <td>2.508251e-05</td>\n",
       "      <td>99</td>\n",
       "      <td>296</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>0.774497</td>\n",
       "      <td>2021-08-27 13:19:58.059956</td>\n",
       "      <td>2021-08-27 13:20:05.812020</td>\n",
       "      <td>0 days 00:00:07.752064</td>\n",
       "      <td>0.951798</td>\n",
       "      <td>6</td>\n",
       "      <td>0.788548</td>\n",
       "      <td>3.160648e-07</td>\n",
       "      <td>6.025451e-04</td>\n",
       "      <td>67</td>\n",
       "      <td>559</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.774963</td>\n",
       "      <td>2021-08-27 13:10:33.866954</td>\n",
       "      <td>2021-08-27 13:10:41.554987</td>\n",
       "      <td>0 days 00:00:07.688033</td>\n",
       "      <td>0.614289</td>\n",
       "      <td>2</td>\n",
       "      <td>0.515581</td>\n",
       "      <td>1.308453e-03</td>\n",
       "      <td>6.161190e-01</td>\n",
       "      <td>18</td>\n",
       "      <td>503</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>0.775328</td>\n",
       "      <td>2021-08-27 13:15:12.008794</td>\n",
       "      <td>2021-08-27 13:15:18.600881</td>\n",
       "      <td>0 days 00:00:06.592087</td>\n",
       "      <td>0.834872</td>\n",
       "      <td>7</td>\n",
       "      <td>0.683654</td>\n",
       "      <td>7.563363e-07</td>\n",
       "      <td>2.029838e-03</td>\n",
       "      <td>34</td>\n",
       "      <td>452</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>0.775513</td>\n",
       "      <td>2021-08-27 13:24:27.767942</td>\n",
       "      <td>2021-08-27 13:24:33.661005</td>\n",
       "      <td>0 days 00:00:05.893063</td>\n",
       "      <td>0.477115</td>\n",
       "      <td>7</td>\n",
       "      <td>0.863986</td>\n",
       "      <td>4.404550e+00</td>\n",
       "      <td>6.493820e-02</td>\n",
       "      <td>53</td>\n",
       "      <td>532</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.775720</td>\n",
       "      <td>2021-08-27 13:14:40.768783</td>\n",
       "      <td>2021-08-27 13:14:48.091035</td>\n",
       "      <td>0 days 00:00:07.322252</td>\n",
       "      <td>0.769306</td>\n",
       "      <td>3</td>\n",
       "      <td>0.970455</td>\n",
       "      <td>3.008935e-06</td>\n",
       "      <td>8.291965e-03</td>\n",
       "      <td>6</td>\n",
       "      <td>386</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>0.776047</td>\n",
       "      <td>2021-08-27 13:21:47.708433</td>\n",
       "      <td>2021-08-27 13:21:55.341627</td>\n",
       "      <td>0 days 00:00:07.633194</td>\n",
       "      <td>0.852667</td>\n",
       "      <td>7</td>\n",
       "      <td>0.810407</td>\n",
       "      <td>2.496647e-07</td>\n",
       "      <td>7.868613e-04</td>\n",
       "      <td>8</td>\n",
       "      <td>627</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.776152</td>\n",
       "      <td>2021-08-27 13:09:39.838448</td>\n",
       "      <td>2021-08-27 13:09:45.665928</td>\n",
       "      <td>0 days 00:00:05.827480</td>\n",
       "      <td>0.726870</td>\n",
       "      <td>4</td>\n",
       "      <td>0.436408</td>\n",
       "      <td>1.874899e+00</td>\n",
       "      <td>4.224218e+00</td>\n",
       "      <td>94</td>\n",
       "      <td>218</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>0.776359</td>\n",
       "      <td>2021-08-27 13:18:00.536691</td>\n",
       "      <td>2021-08-27 13:18:06.190104</td>\n",
       "      <td>0 days 00:00:05.653413</td>\n",
       "      <td>0.471301</td>\n",
       "      <td>7</td>\n",
       "      <td>0.916872</td>\n",
       "      <td>3.209740e-07</td>\n",
       "      <td>3.717831e+00</td>\n",
       "      <td>99</td>\n",
       "      <td>536</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.776591</td>\n",
       "      <td>2021-08-27 13:13:40.241222</td>\n",
       "      <td>2021-08-27 13:13:46.810147</td>\n",
       "      <td>0 days 00:00:06.568925</td>\n",
       "      <td>0.597182</td>\n",
       "      <td>3</td>\n",
       "      <td>0.681623</td>\n",
       "      <td>5.617531e-06</td>\n",
       "      <td>1.180644e-02</td>\n",
       "      <td>14</td>\n",
       "      <td>440</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.776721</td>\n",
       "      <td>2021-08-27 13:14:08.455764</td>\n",
       "      <td>2021-08-27 13:14:16.918186</td>\n",
       "      <td>0 days 00:00:08.462422</td>\n",
       "      <td>0.973155</td>\n",
       "      <td>7</td>\n",
       "      <td>0.729882</td>\n",
       "      <td>1.746911e-02</td>\n",
       "      <td>8.413356e-04</td>\n",
       "      <td>48</td>\n",
       "      <td>713</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.776745</td>\n",
       "      <td>2021-08-27 13:10:10.809147</td>\n",
       "      <td>2021-08-27 13:10:17.152680</td>\n",
       "      <td>0 days 00:00:06.343533</td>\n",
       "      <td>0.730396</td>\n",
       "      <td>6</td>\n",
       "      <td>0.624569</td>\n",
       "      <td>3.189556e-03</td>\n",
       "      <td>5.892450e-04</td>\n",
       "      <td>81</td>\n",
       "      <td>527</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>0.776865</td>\n",
       "      <td>2021-08-27 13:20:59.917125</td>\n",
       "      <td>2021-08-27 13:21:07.641479</td>\n",
       "      <td>0 days 00:00:07.724354</td>\n",
       "      <td>0.943815</td>\n",
       "      <td>5</td>\n",
       "      <td>0.670206</td>\n",
       "      <td>1.517572e-06</td>\n",
       "      <td>6.625052e-06</td>\n",
       "      <td>77</td>\n",
       "      <td>461</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.777920</td>\n",
       "      <td>2021-08-27 13:08:35.175739</td>\n",
       "      <td>2021-08-27 13:08:41.727432</td>\n",
       "      <td>0 days 00:00:06.551693</td>\n",
       "      <td>0.880084</td>\n",
       "      <td>6</td>\n",
       "      <td>0.573120</td>\n",
       "      <td>1.790147e-06</td>\n",
       "      <td>2.451704e-07</td>\n",
       "      <td>93</td>\n",
       "      <td>354</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>0.778563</td>\n",
       "      <td>2021-08-27 13:21:21.801834</td>\n",
       "      <td>2021-08-27 13:21:26.014001</td>\n",
       "      <td>0 days 00:00:04.212167</td>\n",
       "      <td>0.738267</td>\n",
       "      <td>1</td>\n",
       "      <td>0.754050</td>\n",
       "      <td>5.806378e-02</td>\n",
       "      <td>4.520148e-05</td>\n",
       "      <td>18</td>\n",
       "      <td>251</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.778633</td>\n",
       "      <td>2021-08-27 13:13:55.676316</td>\n",
       "      <td>2021-08-27 13:14:01.962789</td>\n",
       "      <td>0 days 00:00:06.286473</td>\n",
       "      <td>0.523498</td>\n",
       "      <td>1</td>\n",
       "      <td>0.547655</td>\n",
       "      <td>3.944308e-07</td>\n",
       "      <td>3.432291e-03</td>\n",
       "      <td>71</td>\n",
       "      <td>833</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>0.779475</td>\n",
       "      <td>2021-08-27 13:18:15.476135</td>\n",
       "      <td>2021-08-27 13:18:22.166652</td>\n",
       "      <td>0 days 00:00:06.690517</td>\n",
       "      <td>0.549330</td>\n",
       "      <td>5</td>\n",
       "      <td>0.743040</td>\n",
       "      <td>8.583144e-03</td>\n",
       "      <td>6.456238e-01</td>\n",
       "      <td>39</td>\n",
       "      <td>490</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>0.779523</td>\n",
       "      <td>2021-08-27 13:21:26.014676</td>\n",
       "      <td>2021-08-27 13:21:34.197191</td>\n",
       "      <td>0 days 00:00:08.182515</td>\n",
       "      <td>0.928448</td>\n",
       "      <td>3</td>\n",
       "      <td>0.712985</td>\n",
       "      <td>8.177101e-01</td>\n",
       "      <td>4.903913e-03</td>\n",
       "      <td>13</td>\n",
       "      <td>356</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>0.779955</td>\n",
       "      <td>2021-08-27 13:14:48.100627</td>\n",
       "      <td>2021-08-27 13:14:56.186805</td>\n",
       "      <td>0 days 00:00:08.086178</td>\n",
       "      <td>0.936539</td>\n",
       "      <td>4</td>\n",
       "      <td>0.636385</td>\n",
       "      <td>1.834724e-01</td>\n",
       "      <td>3.029864e-04</td>\n",
       "      <td>21</td>\n",
       "      <td>566</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>0.779984</td>\n",
       "      <td>2021-08-27 13:18:59.524872</td>\n",
       "      <td>2021-08-27 13:19:05.634587</td>\n",
       "      <td>0 days 00:00:06.109715</td>\n",
       "      <td>0.682559</td>\n",
       "      <td>3</td>\n",
       "      <td>0.785614</td>\n",
       "      <td>1.904759e-05</td>\n",
       "      <td>2.292227e-07</td>\n",
       "      <td>33</td>\n",
       "      <td>207</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.782517</td>\n",
       "      <td>2021-08-27 13:09:59.904537</td>\n",
       "      <td>2021-08-27 13:10:04.033304</td>\n",
       "      <td>0 days 00:00:04.128767</td>\n",
       "      <td>0.673187</td>\n",
       "      <td>1</td>\n",
       "      <td>0.580040</td>\n",
       "      <td>2.852033e+00</td>\n",
       "      <td>1.382536e+00</td>\n",
       "      <td>34</td>\n",
       "      <td>221</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>0.783401</td>\n",
       "      <td>2021-08-27 13:21:07.642158</td>\n",
       "      <td>2021-08-27 13:21:16.058517</td>\n",
       "      <td>0 days 00:00:08.416359</td>\n",
       "      <td>0.989822</td>\n",
       "      <td>2</td>\n",
       "      <td>0.689355</td>\n",
       "      <td>2.991383e-03</td>\n",
       "      <td>1.075500e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>167</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.784781</td>\n",
       "      <td>2021-08-27 13:10:22.855183</td>\n",
       "      <td>2021-08-27 13:10:27.372153</td>\n",
       "      <td>0 days 00:00:04.516970</td>\n",
       "      <td>0.631736</td>\n",
       "      <td>5</td>\n",
       "      <td>0.909313</td>\n",
       "      <td>1.047755e-08</td>\n",
       "      <td>6.720866e-03</td>\n",
       "      <td>67</td>\n",
       "      <td>91</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.785293</td>\n",
       "      <td>2021-08-27 13:12:03.698975</td>\n",
       "      <td>2021-08-27 13:12:10.357392</td>\n",
       "      <td>0 days 00:00:06.658417</td>\n",
       "      <td>0.498756</td>\n",
       "      <td>7</td>\n",
       "      <td>0.953241</td>\n",
       "      <td>1.794526e-07</td>\n",
       "      <td>9.639992e+00</td>\n",
       "      <td>8</td>\n",
       "      <td>746</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>0.791820</td>\n",
       "      <td>2021-08-27 13:21:16.059182</td>\n",
       "      <td>2021-08-27 13:21:21.801142</td>\n",
       "      <td>0 days 00:00:05.741960</td>\n",
       "      <td>0.661921</td>\n",
       "      <td>2</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>3.654776e-01</td>\n",
       "      <td>1.651168e-06</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.794918</td>\n",
       "      <td>2021-08-27 13:16:57.164244</td>\n",
       "      <td>2021-08-27 13:17:00.890435</td>\n",
       "      <td>0 days 00:00:03.726191</td>\n",
       "      <td>0.699774</td>\n",
       "      <td>7</td>\n",
       "      <td>0.700445</td>\n",
       "      <td>8.142269e-04</td>\n",
       "      <td>1.635094e-02</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "28      28  0.708044 2021-08-27 13:12:31.210642 2021-08-27 13:13:23.681055   \n",
       "21      21  0.713506 2021-08-27 13:10:50.001660 2021-08-27 13:11:50.216615   \n",
       "93      93  0.714127 2021-08-27 13:23:16.241032 2021-08-27 13:24:11.528775   \n",
       "99      99  0.715087 2021-08-27 13:25:42.294565 2021-08-27 13:26:45.388114   \n",
       "91      91  0.715300 2021-08-27 13:22:17.628726 2021-08-27 13:23:07.223239   \n",
       "44      44  0.716599 2021-08-27 13:15:18.602552 2021-08-27 13:16:20.589662   \n",
       "4        4  0.718118 2021-08-27 13:06:32.333506 2021-08-27 13:07:16.385107   \n",
       "8        8  0.718883 2021-08-27 13:08:41.727986 2021-08-27 13:09:32.835840   \n",
       "5        5  0.720140 2021-08-27 13:07:16.386201 2021-08-27 13:08:15.245138   \n",
       "97      97  0.720147 2021-08-27 13:24:33.661803 2021-08-27 13:25:34.902695   \n",
       "2        2  0.721574 2021-08-27 13:04:09.365247 2021-08-27 13:05:38.591146   \n",
       "1        1  0.722866 2021-08-27 13:03:10.548233 2021-08-27 13:04:09.364149   \n",
       "6        6  0.723908 2021-08-27 13:08:15.246237 2021-08-27 13:08:35.175069   \n",
       "0        0  0.726431 2021-08-27 13:02:17.480186 2021-08-27 13:03:10.546183   \n",
       "3        3  0.730093 2021-08-27 13:05:38.592306 2021-08-27 13:06:32.332425   \n",
       "95      95  0.737619 2021-08-27 13:24:18.532631 2021-08-27 13:24:27.767238   \n",
       "92      92  0.737948 2021-08-27 13:23:07.226378 2021-08-27 13:23:16.240340   \n",
       "57      57  0.750580 2021-08-27 13:17:49.579967 2021-08-27 13:18:00.536015   \n",
       "59      59  0.751132 2021-08-27 13:18:06.190855 2021-08-27 13:18:15.455659   \n",
       "38      38  0.751528 2021-08-27 13:14:32.688529 2021-08-27 13:14:40.759154   \n",
       "66      66  0.758125 2021-08-27 13:19:05.635261 2021-08-27 13:19:14.114215   \n",
       "55      55  0.758320 2021-08-27 13:17:33.680351 2021-08-27 13:17:42.597930   \n",
       "63      63  0.758821 2021-08-27 13:18:40.279146 2021-08-27 13:18:50.182868   \n",
       "70      70  0.758824 2021-08-27 13:19:42.100350 2021-08-27 13:19:49.237487   \n",
       "26      26  0.760361 2021-08-27 13:12:17.260115 2021-08-27 13:12:23.515998   \n",
       "20      20  0.761326 2021-08-27 13:10:41.555665 2021-08-27 13:10:50.000982   \n",
       "94      94  0.767038 2021-08-27 13:24:11.530106 2021-08-27 13:24:18.531878   \n",
       "88      88  0.767158 2021-08-27 13:21:55.342299 2021-08-27 13:22:03.718542   \n",
       "68      68  0.767218 2021-08-27 13:19:23.976231 2021-08-27 13:19:33.208119   \n",
       "53      53  0.767295 2021-08-27 13:17:17.345686 2021-08-27 13:17:26.211866   \n",
       "61      61  0.767417 2021-08-27 13:18:22.167427 2021-08-27 13:18:33.646873   \n",
       "46      46  0.767627 2021-08-27 13:16:29.275454 2021-08-27 13:16:36.122696   \n",
       "52      52  0.767676 2021-08-27 13:17:09.208410 2021-08-27 13:17:17.344996   \n",
       "98      98  0.767762 2021-08-27 13:25:34.903969 2021-08-27 13:25:42.293812   \n",
       "32      32  0.767968 2021-08-27 13:13:46.810834 2021-08-27 13:13:55.675620   \n",
       "42      42  0.768166 2021-08-27 13:15:04.160423 2021-08-27 13:15:12.008053   \n",
       "23      23  0.768832 2021-08-27 13:11:56.517802 2021-08-27 13:12:03.698303   \n",
       "12      12  0.768931 2021-08-27 13:09:53.838013 2021-08-27 13:09:59.903861   \n",
       "18      18  0.768949 2021-08-27 13:10:27.372819 2021-08-27 13:10:33.866284   \n",
       "22      22  0.769021 2021-08-27 13:11:50.217693 2021-08-27 13:11:56.517230   \n",
       "11      11  0.769023 2021-08-27 13:09:45.666613 2021-08-27 13:09:53.837339   \n",
       "77      77  0.769110 2021-08-27 13:20:37.425532 2021-08-27 13:20:44.996219   \n",
       "89      89  0.769138 2021-08-27 13:22:03.719224 2021-08-27 13:22:10.362976   \n",
       "71      71  0.769308 2021-08-27 13:19:49.238207 2021-08-27 13:19:58.059232   \n",
       "56      56  0.769458 2021-08-27 13:17:42.598599 2021-08-27 13:17:49.579274   \n",
       "69      69  0.769494 2021-08-27 13:19:33.208737 2021-08-27 13:19:42.099679   \n",
       "76      76  0.770187 2021-08-27 13:20:28.756781 2021-08-27 13:20:37.424855   \n",
       "73      73  0.770280 2021-08-27 13:20:05.812737 2021-08-27 13:20:15.071748   \n",
       "79      79  0.770368 2021-08-27 13:20:51.827249 2021-08-27 13:20:59.916585   \n",
       "27      27  0.770462 2021-08-27 13:12:23.516559 2021-08-27 13:12:31.210111   \n",
       "90      90  0.770466 2021-08-27 13:22:10.363661 2021-08-27 13:22:17.627964   \n",
       "9        9  0.770648 2021-08-27 13:09:32.836951 2021-08-27 13:09:39.837743   \n",
       "78      78  0.770735 2021-08-27 13:20:44.996894 2021-08-27 13:20:51.826519   \n",
       "45      45  0.770869 2021-08-27 13:16:20.590871 2021-08-27 13:16:29.274887   \n",
       "49      49  0.771027 2021-08-27 13:16:49.692464 2021-08-27 13:16:57.163573   \n",
       "54      54  0.771193 2021-08-27 13:17:26.212546 2021-08-27 13:17:33.679675   \n",
       "62      62  0.771424 2021-08-27 13:18:33.647458 2021-08-27 13:18:40.278572   \n",
       "37      37  0.771449 2021-08-27 13:14:25.520853 2021-08-27 13:14:32.687848   \n",
       "67      67  0.771869 2021-08-27 13:19:14.114920 2021-08-27 13:19:23.975538   \n",
       "75      75  0.771884 2021-08-27 13:20:21.411633 2021-08-27 13:20:28.756036   \n",
       "36      36  0.771980 2021-08-27 13:14:16.918875 2021-08-27 13:14:25.520191   \n",
       "25      25  0.772136 2021-08-27 13:12:10.358080 2021-08-27 13:12:17.258471   \n",
       "74      74  0.772148 2021-08-27 13:20:15.072449 2021-08-27 13:20:21.410892   \n",
       "29      29  0.772303 2021-08-27 13:13:23.682203 2021-08-27 13:13:31.021631   \n",
       "41      41  0.772368 2021-08-27 13:14:56.187333 2021-08-27 13:15:04.159750   \n",
       "14      14  0.772518 2021-08-27 13:10:04.033988 2021-08-27 13:10:10.808476   \n",
       "34      34  0.772557 2021-08-27 13:14:01.963482 2021-08-27 13:14:08.455097   \n",
       "47      47  0.772727 2021-08-27 13:16:36.123407 2021-08-27 13:16:45.071938   \n",
       "85      85  0.773031 2021-08-27 13:21:34.197891 2021-08-27 13:21:39.069150   \n",
       "30      30  0.773566 2021-08-27 13:13:31.022310 2021-08-27 13:13:40.240536   \n",
       "48      48  0.773626 2021-08-27 13:16:45.072649 2021-08-27 13:16:49.691800   \n",
       "51      51  0.773825 2021-08-27 13:17:00.891130 2021-08-27 13:17:09.207737   \n",
       "86      86  0.774387 2021-08-27 13:21:39.069854 2021-08-27 13:21:47.707752   \n",
       "64      64  0.774487 2021-08-27 13:18:50.183436 2021-08-27 13:18:59.524202   \n",
       "16      16  0.774496 2021-08-27 13:10:17.153354 2021-08-27 13:10:22.854524   \n",
       "72      72  0.774497 2021-08-27 13:19:58.059956 2021-08-27 13:20:05.812020   \n",
       "19      19  0.774963 2021-08-27 13:10:33.866954 2021-08-27 13:10:41.554987   \n",
       "43      43  0.775328 2021-08-27 13:15:12.008794 2021-08-27 13:15:18.600881   \n",
       "96      96  0.775513 2021-08-27 13:24:27.767942 2021-08-27 13:24:33.661005   \n",
       "39      39  0.775720 2021-08-27 13:14:40.768783 2021-08-27 13:14:48.091035   \n",
       "87      87  0.776047 2021-08-27 13:21:47.708433 2021-08-27 13:21:55.341627   \n",
       "10      10  0.776152 2021-08-27 13:09:39.838448 2021-08-27 13:09:45.665928   \n",
       "58      58  0.776359 2021-08-27 13:18:00.536691 2021-08-27 13:18:06.190104   \n",
       "31      31  0.776591 2021-08-27 13:13:40.241222 2021-08-27 13:13:46.810147   \n",
       "35      35  0.776721 2021-08-27 13:14:08.455764 2021-08-27 13:14:16.918186   \n",
       "15      15  0.776745 2021-08-27 13:10:10.809147 2021-08-27 13:10:17.152680   \n",
       "80      80  0.776865 2021-08-27 13:20:59.917125 2021-08-27 13:21:07.641479   \n",
       "7        7  0.777920 2021-08-27 13:08:35.175739 2021-08-27 13:08:41.727432   \n",
       "83      83  0.778563 2021-08-27 13:21:21.801834 2021-08-27 13:21:26.014001   \n",
       "33      33  0.778633 2021-08-27 13:13:55.676316 2021-08-27 13:14:01.962789   \n",
       "60      60  0.779475 2021-08-27 13:18:15.476135 2021-08-27 13:18:22.166652   \n",
       "84      84  0.779523 2021-08-27 13:21:26.014676 2021-08-27 13:21:34.197191   \n",
       "40      40  0.779955 2021-08-27 13:14:48.100627 2021-08-27 13:14:56.186805   \n",
       "65      65  0.779984 2021-08-27 13:18:59.524872 2021-08-27 13:19:05.634587   \n",
       "13      13  0.782517 2021-08-27 13:09:59.904537 2021-08-27 13:10:04.033304   \n",
       "81      81  0.783401 2021-08-27 13:21:07.642158 2021-08-27 13:21:16.058517   \n",
       "17      17  0.784781 2021-08-27 13:10:22.855183 2021-08-27 13:10:27.372153   \n",
       "24      24  0.785293 2021-08-27 13:12:03.698975 2021-08-27 13:12:10.357392   \n",
       "82      82  0.791820 2021-08-27 13:21:16.059182 2021-08-27 13:21:21.801142   \n",
       "50      50  0.794918 2021-08-27 13:16:57.164244 2021-08-27 13:17:00.890435   \n",
       "\n",
       "                 duration  params_bagging_fraction  params_bagging_freq  \\\n",
       "28 0 days 00:00:52.470413                 0.658250                    4   \n",
       "21 0 days 00:01:00.214955                 0.902564                    7   \n",
       "93 0 days 00:00:55.287743                 0.507007                    7   \n",
       "99 0 days 00:01:03.093549                 0.436979                    7   \n",
       "91 0 days 00:00:49.594513                 0.560186                    7   \n",
       "44 0 days 00:01:01.987110                 0.948885                    7   \n",
       "4  0 days 00:00:44.051601                 0.622979                    6   \n",
       "8  0 days 00:00:51.107854                 0.530941                    2   \n",
       "5  0 days 00:00:58.858937                 0.895404                    7   \n",
       "97 0 days 00:01:01.240892                 0.450480                    7   \n",
       "2  0 days 00:01:29.225899                 0.982915                    2   \n",
       "1  0 days 00:00:58.815916                 0.979114                    5   \n",
       "6  0 days 00:00:19.928832                 0.416508                    5   \n",
       "0  0 days 00:00:53.065997                 0.802845                    2   \n",
       "3  0 days 00:00:53.740119                 0.625324                    7   \n",
       "95 0 days 00:00:09.234607                 0.543175                    7   \n",
       "92 0 days 00:00:09.013962                 0.528650                    7   \n",
       "57 0 days 00:00:10.956048                 0.951203                    6   \n",
       "59 0 days 00:00:09.264804                 0.831404                    6   \n",
       "38 0 days 00:00:08.070625                 0.658063                    7   \n",
       "66 0 days 00:00:08.478954                 0.985928                    1   \n",
       "55 0 days 00:00:08.917579                 0.590949                    2   \n",
       "63 0 days 00:00:09.903722                 0.622767                    2   \n",
       "70 0 days 00:00:07.137137                 0.494521                    2   \n",
       "26 0 days 00:00:06.255883                 0.573552                    6   \n",
       "20 0 days 00:00:08.445317                 0.800796                    4   \n",
       "94 0 days 00:00:07.001772                 0.567192                    7   \n",
       "88 0 days 00:00:08.376243                 0.877605                    6   \n",
       "68 0 days 00:00:09.231888                 0.933126                    4   \n",
       "53 0 days 00:00:08.866180                 0.754057                    7   \n",
       "61 0 days 00:00:11.479446                 0.999345                    2   \n",
       "46 0 days 00:00:06.847242                 0.605381                    7   \n",
       "52 0 days 00:00:08.136586                 0.869954                    6   \n",
       "98 0 days 00:00:07.389843                 0.519510                    7   \n",
       "32 0 days 00:00:08.864786                 0.669471                    2   \n",
       "42 0 days 00:00:07.847630                 0.914591                    6   \n",
       "23 0 days 00:00:07.180501                 0.800542                    6   \n",
       "12 0 days 00:00:06.065848                 0.565211                    4   \n",
       "18 0 days 00:00:06.493465                 0.424511                    6   \n",
       "22 0 days 00:00:06.299537                 0.683418                    7   \n",
       "11 0 days 00:00:08.170726                 0.469598                    1   \n",
       "77 0 days 00:00:07.570687                 0.707847                    4   \n",
       "89 0 days 00:00:06.643752                 0.968485                    1   \n",
       "71 0 days 00:00:08.821025                 0.981030                    5   \n",
       "56 0 days 00:00:06.980675                 0.900319                    7   \n",
       "69 0 days 00:00:08.890942                 0.886084                    7   \n",
       "76 0 days 00:00:08.668074                 0.971963                    7   \n",
       "73 0 days 00:00:09.259011                 0.907747                    5   \n",
       "79 0 days 00:00:08.089336                 0.781763                    6   \n",
       "27 0 days 00:00:07.693552                 0.758471                    7   \n",
       "90 0 days 00:00:07.264303                 0.693583                    3   \n",
       "9  0 days 00:00:07.000792                 0.575964                    3   \n",
       "78 0 days 00:00:06.829625                 0.584474                    7   \n",
       "45 0 days 00:00:08.684016                 0.948831                    6   \n",
       "49 0 days 00:00:07.471109                 0.925726                    6   \n",
       "54 0 days 00:00:07.467129                 0.648191                    3   \n",
       "62 0 days 00:00:06.631114                 0.916684                    1   \n",
       "37 0 days 00:00:07.166995                 0.450414                    5   \n",
       "67 0 days 00:00:09.860618                 0.818609                    2   \n",
       "75 0 days 00:00:07.344403                 0.870359                    6   \n",
       "36 0 days 00:00:08.601316                 0.553430                    2   \n",
       "25 0 days 00:00:06.900391                 0.893901                    5   \n",
       "74 0 days 00:00:06.338443                 0.406803                    3   \n",
       "29 0 days 00:00:07.339428                 0.641492                    4   \n",
       "41 0 days 00:00:07.972417                 0.857400                    7   \n",
       "14 0 days 00:00:06.774488                 0.503582                    3   \n",
       "34 0 days 00:00:06.491615                 0.716365                    6   \n",
       "47 0 days 00:00:08.948531                 0.961805                    5   \n",
       "85 0 days 00:00:04.871259                 0.793624                    7   \n",
       "30 0 days 00:00:09.218226                 0.848085                    5   \n",
       "48 0 days 00:00:04.619151                 0.999724                    1   \n",
       "51 0 days 00:00:08.316607                 0.894954                    7   \n",
       "86 0 days 00:00:08.637898                 0.633728                    2   \n",
       "64 0 days 00:00:09.340766                 0.963687                    4   \n",
       "16 0 days 00:00:05.701170                 0.531336                    3   \n",
       "72 0 days 00:00:07.752064                 0.951798                    6   \n",
       "19 0 days 00:00:07.688033                 0.614289                    2   \n",
       "43 0 days 00:00:06.592087                 0.834872                    7   \n",
       "96 0 days 00:00:05.893063                 0.477115                    7   \n",
       "39 0 days 00:00:07.322252                 0.769306                    3   \n",
       "87 0 days 00:00:07.633194                 0.852667                    7   \n",
       "10 0 days 00:00:05.827480                 0.726870                    4   \n",
       "58 0 days 00:00:05.653413                 0.471301                    7   \n",
       "31 0 days 00:00:06.568925                 0.597182                    3   \n",
       "35 0 days 00:00:08.462422                 0.973155                    7   \n",
       "15 0 days 00:00:06.343533                 0.730396                    6   \n",
       "80 0 days 00:00:07.724354                 0.943815                    5   \n",
       "7  0 days 00:00:06.551693                 0.880084                    6   \n",
       "83 0 days 00:00:04.212167                 0.738267                    1   \n",
       "33 0 days 00:00:06.286473                 0.523498                    1   \n",
       "60 0 days 00:00:06.690517                 0.549330                    5   \n",
       "84 0 days 00:00:08.182515                 0.928448                    3   \n",
       "40 0 days 00:00:08.086178                 0.936539                    4   \n",
       "65 0 days 00:00:06.109715                 0.682559                    3   \n",
       "13 0 days 00:00:04.128767                 0.673187                    1   \n",
       "81 0 days 00:00:08.416359                 0.989822                    2   \n",
       "17 0 days 00:00:04.516970                 0.631736                    5   \n",
       "24 0 days 00:00:06.658417                 0.498756                    7   \n",
       "82 0 days 00:00:05.741960                 0.661921                    2   \n",
       "50 0 days 00:00:03.726191                 0.699774                    7   \n",
       "\n",
       "    params_feature_fraction  params_lambda_l1  params_lambda_l2  \\\n",
       "28                 0.658998      1.101566e-07      2.364969e-03   \n",
       "21                 0.998013      8.267419e-06      2.884949e-01   \n",
       "93                 0.871760      3.126412e-01      9.096825e-02   \n",
       "99                 0.914766      1.481383e+00      7.182924e-01   \n",
       "91                 0.886287      7.182257e-01      1.574616e-04   \n",
       "44                 0.896558      4.036494e-06      3.613399e-02   \n",
       "4                  0.883619      5.518957e-01      4.791927e-03   \n",
       "8                  0.702556      3.835407e-05      5.093830e-03   \n",
       "5                  0.978627      2.642709e-06      1.371951e-01   \n",
       "97                 0.909578      3.067002e-01      9.169143e-02   \n",
       "2                  0.850711      1.088013e-06      6.093378e-07   \n",
       "1                  0.831542      8.479742e-08      5.299536e-03   \n",
       "6                  0.889191      5.417912e-05      7.777508e-06   \n",
       "0                  0.723706      2.880148e-02      1.534117e-05   \n",
       "3                  0.865420      1.518082e-01      1.883852e-04   \n",
       "95                 0.898543      3.977534e-01      3.005262e-01   \n",
       "92                 0.847325      1.985084e+00      1.122925e-05   \n",
       "57                 0.947216      1.587140e-04      2.652866e-01   \n",
       "59                 0.813772      5.713136e-05      6.397647e-02   \n",
       "38                 0.887198      4.911515e-08      5.129087e-02   \n",
       "66                 0.991682      5.539106e-02      1.287772e-02   \n",
       "55                 0.605680      6.674729e-01      2.834474e-02   \n",
       "63                 0.877765      3.638984e-06      1.343313e-08   \n",
       "70                 0.903153      2.943433e-08      9.654339e-07   \n",
       "26                 0.759789      8.364868e+00      7.056987e-05   \n",
       "20                 0.950380      3.395240e-02      1.799713e-02   \n",
       "94                 0.875756      7.743221e-01      1.465344e-01   \n",
       "88                 0.969651      4.358613e-04      2.200940e-03   \n",
       "68                 0.557274      4.290932e-07      4.665560e-03   \n",
       "53                 0.985290      2.472815e-08      5.505584e-03   \n",
       "61                 0.844439      9.774791e-07      1.428637e-07   \n",
       "46                 0.401215      1.206561e-05      7.101714e-03   \n",
       "52                 0.948089      2.395939e-05      1.188107e-01   \n",
       "98                 0.981616      1.246251e-01      9.349884e-02   \n",
       "32                 0.726275      6.071006e-08      7.835309e-05   \n",
       "42                 0.929355      3.157757e-05      2.105196e+00   \n",
       "23                 0.799030      6.830681e-06      1.007389e-03   \n",
       "12                 0.999399      3.642750e-04      3.095080e-03   \n",
       "18                 0.764863      3.073610e-05      1.479175e-08   \n",
       "22                 0.934442      1.518440e-05      4.909720e-01   \n",
       "11                 0.704985      3.380262e-03      3.503520e-02   \n",
       "77                 0.715820      3.226931e+00      3.800558e-02   \n",
       "89                 0.832427      3.325487e-05      7.763370e-03   \n",
       "71                 0.825681      1.044438e-07      1.026588e-02   \n",
       "56                 0.507932      3.103591e-06      4.788268e-04   \n",
       "69                 0.933208      8.223894e-08      1.511021e-03   \n",
       "76                 0.919789      5.543593e-05      1.162344e-01   \n",
       "73                 0.858431      2.114240e-06      3.222881e-03   \n",
       "79                 0.622447      1.053101e-08      1.595870e-04   \n",
       "27                 0.918985      1.166228e-04      2.194392e-02   \n",
       "90                 0.938281      4.026817e-06      1.214226e+00   \n",
       "9                  0.664197      1.226393e-06      3.330423e-04   \n",
       "78                 0.998172      9.463318e-06      3.807815e-01   \n",
       "45                 0.831883      1.483224e-04      3.381353e-02   \n",
       "49                 0.762673      4.990846e-06      4.920306e-02   \n",
       "54                 0.901529      1.216175e-06      1.181075e-03   \n",
       "62                 0.661085      8.152381e-06      1.260804e-06   \n",
       "37                 0.810464      1.047479e-03      4.677810e-01   \n",
       "67                 0.955693      7.771041e-06      3.393971e-08   \n",
       "75                 0.960290      3.834061e-08      2.133124e-02   \n",
       "36                 0.841946      5.614443e-07      1.411549e-04   \n",
       "25                 0.999949      3.180115e-04      1.666280e-01   \n",
       "74                 0.883742      6.848013e-07      2.890853e-04   \n",
       "29                 0.600981      1.861297e-07      1.803607e-03   \n",
       "41                 0.978261      1.617385e-06      1.926283e-01   \n",
       "14                 0.791015      9.423025e-05      8.806093e-02   \n",
       "34                 0.870078      1.371724e-05      8.339633e-02   \n",
       "47                 0.865799      4.899248e-05      2.156209e-01   \n",
       "85                 0.775362      7.649725e+00      4.756223e-07   \n",
       "30                 0.645629      1.594343e-08      3.229903e-06   \n",
       "48                 0.894494      1.818677e-07      2.722695e-03   \n",
       "51                 0.971953      2.107827e-06      1.105121e+00   \n",
       "86                 0.737063      1.040618e-01      6.389703e-08   \n",
       "64                 0.855338      1.350393e-07      2.969570e-05   \n",
       "16                 0.471741      1.969507e-01      2.508251e-05   \n",
       "72                 0.788548      3.160648e-07      6.025451e-04   \n",
       "19                 0.515581      1.308453e-03      6.161190e-01   \n",
       "43                 0.683654      7.563363e-07      2.029838e-03   \n",
       "96                 0.863986      4.404550e+00      6.493820e-02   \n",
       "39                 0.970455      3.008935e-06      8.291965e-03   \n",
       "87                 0.810407      2.496647e-07      7.868613e-04   \n",
       "10                 0.436408      1.874899e+00      4.224218e+00   \n",
       "58                 0.916872      3.209740e-07      3.717831e+00   \n",
       "31                 0.681623      5.617531e-06      1.180644e-02   \n",
       "35                 0.729882      1.746911e-02      8.413356e-04   \n",
       "15                 0.624569      3.189556e-03      5.892450e-04   \n",
       "80                 0.670206      1.517572e-06      6.625052e-06   \n",
       "7                  0.573120      1.790147e-06      2.451704e-07   \n",
       "83                 0.754050      5.806378e-02      4.520148e-05   \n",
       "33                 0.547655      3.944308e-07      3.432291e-03   \n",
       "60                 0.743040      8.583144e-03      6.456238e-01   \n",
       "84                 0.712985      8.177101e-01      4.903913e-03   \n",
       "40                 0.636385      1.834724e-01      3.029864e-04   \n",
       "65                 0.785614      1.904759e-05      2.292227e-07   \n",
       "13                 0.580040      2.852033e+00      1.382536e+00   \n",
       "81                 0.689355      2.991383e-03      1.075500e-05   \n",
       "17                 0.909313      1.047755e-08      6.720866e-03   \n",
       "24                 0.953241      1.794526e-07      9.639992e+00   \n",
       "82                 0.652800      3.654776e-01      1.651168e-06   \n",
       "50                 0.700445      8.142269e-04      1.635094e-02   \n",
       "\n",
       "    params_min_child_samples  params_num_leaves     state  \n",
       "28                        35                658  COMPLETE  \n",
       "21                        42                734  COMPLETE  \n",
       "93                        61                795  COMPLETE  \n",
       "99                        56                985  COMPLETE  \n",
       "91                        53                725  COMPLETE  \n",
       "44                        46                788  COMPLETE  \n",
       "4                         86                487  COMPLETE  \n",
       "8                          9                482  COMPLETE  \n",
       "5                         47                731  COMPLETE  \n",
       "97                        57                927  COMPLETE  \n",
       "2                         53                879  COMPLETE  \n",
       "1                         63                585  COMPLETE  \n",
       "6                         73                899    PRUNED  \n",
       "0                         15                 30  COMPLETE  \n",
       "3                         50                818  COMPLETE  \n",
       "95                        72                749    PRUNED  \n",
       "92                        60                726    PRUNED  \n",
       "57                        75                845    PRUNED  \n",
       "59                        46                692    PRUNED  \n",
       "38                        44                759    PRUNED  \n",
       "66                        22               1020    PRUNED  \n",
       "55                        80                796    PRUNED  \n",
       "63                        54                959    PRUNED  \n",
       "70                        94                393    PRUNED  \n",
       "26                        85                554    PRUNED  \n",
       "20                        62                667    PRUNED  \n",
       "94                        65                787    PRUNED  \n",
       "88                        48                777    PRUNED  \n",
       "68                        63                722    PRUNED  \n",
       "53                        12                938    PRUNED  \n",
       "61                        43                760    PRUNED  \n",
       "46                        32                885    PRUNED  \n",
       "52                        63                670    PRUNED  \n",
       "98                        61                919    PRUNED  \n",
       "32                        58                629    PRUNED  \n",
       "42                        59                617    PRUNED  \n",
       "23                        25                598    PRUNED  \n",
       "12                        28                440    PRUNED  \n",
       "18                        38               1023    PRUNED  \n",
       "22                        42                468    PRUNED  \n",
       "11                         9                397    PRUNED  \n",
       "77                        57                698    PRUNED  \n",
       "89                        17                832    PRUNED  \n",
       "71                        50                602    PRUNED  \n",
       "56                        26                423    PRUNED  \n",
       "69                        47                901    PRUNED  \n",
       "76                        40                589    PRUNED  \n",
       "73                        55                814    PRUNED  \n",
       "79                        51                867    PRUNED  \n",
       "27                        56                783    PRUNED  \n",
       "90                        25                421    PRUNED  \n",
       "9                         78                598    PRUNED  \n",
       "78                        89                743    PRUNED  \n",
       "45                        40                808    PRUNED  \n",
       "49                        46                492    PRUNED  \n",
       "54                        52                584    PRUNED  \n",
       "62                        85                863    PRUNED  \n",
       "37                        51                905    PRUNED  \n",
       "67                        36                638    PRUNED  \n",
       "75                        29                509    PRUNED  \n",
       "36                        16                844    PRUNED  \n",
       "25                        22                305    PRUNED  \n",
       "74                        44                651    PRUNED  \n",
       "29                        36                643    PRUNED  \n",
       "41                        49                714    PRUNED  \n",
       "14                        28                696    PRUNED  \n",
       "34                        30                529    PRUNED  \n",
       "47                        54                671    PRUNED  \n",
       "85                        41                123    PRUNED  \n",
       "30                        43                972    PRUNED  \n",
       "48                        68                337    PRUNED  \n",
       "51                        38                741    PRUNED  \n",
       "86                        36                677    PRUNED  \n",
       "64                        59                777    PRUNED  \n",
       "16                        99                296    PRUNED  \n",
       "72                        67                559    PRUNED  \n",
       "19                        18                503    PRUNED  \n",
       "43                        34                452    PRUNED  \n",
       "96                        53                532    PRUNED  \n",
       "39                         6                386    PRUNED  \n",
       "87                         8                627    PRUNED  \n",
       "10                        94                218    PRUNED  \n",
       "58                        99                536    PRUNED  \n",
       "31                        14                440    PRUNED  \n",
       "35                        48                713    PRUNED  \n",
       "15                        81                527    PRUNED  \n",
       "80                        77                461    PRUNED  \n",
       "7                         93                354    PRUNED  \n",
       "83                        18                251    PRUNED  \n",
       "33                        71                833    PRUNED  \n",
       "60                        39                490    PRUNED  \n",
       "84                        13                356    PRUNED  \n",
       "40                        21                566    PRUNED  \n",
       "65                        33                207    PRUNED  \n",
       "13                        34                221    PRUNED  \n",
       "81                         5                167    PRUNED  \n",
       "17                        67                 91    PRUNED  \n",
       "24                         8                746    PRUNED  \n",
       "82                        10                 50    PRUNED  \n",
       "50                        90                 33    PRUNED  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_df = study.trials_dataframe()\n",
    "study_df.sort_values('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/engine.py:182: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1999: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is ['item_category_id', 'month', 'season']\n",
      "  _log_warning('categorical_feature in Dataset is overridden.\\n'\n",
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1460: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 19736\n",
      "[LightGBM] [Info] Number of data points in the train set: 8098633, number of used features: 143\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce GTX 1070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 78 dense feature groups (617.88 MB) transferred to GPU in 0.360102 secs. 1 sparse feature groups\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/lightgbm/basic.py:1727: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Start training from score 0.308742\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's rmse: 0.758135\tvalid_1's rmse: 0.893564\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttraining's rmse: 1.17677\tvalid_1's rmse: 0.317059\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "     'objective': 'mse',\n",
    "     'metric': 'rmse',\n",
    "     'verbose': 1,\n",
    "     'feature_pre_filter': False,\n",
    "     'lambda_l1': 1.2271425307979717e-06,\n",
    "     'lambda_l2': 0.011281292748567182,\n",
    "     'num_leaves': 67,\n",
    "     'feature_fraction': 0.9840000000000001,\n",
    "     'bagging_fraction': 1.0,\n",
    "     'bagging_freq': 0,\n",
    "     'min_child_samples': 20,\n",
    "     'num_iterations': 1000,\n",
    "     'early_stopping_round': 50,\n",
    "     'categorical_column': [7, 12],\n",
    "    'device': \"gpu\"\n",
    "    }\n",
    "cat_feats = ['item_category_id', 'month', 'season']\n",
    "\n",
    "last_block = 34\n",
    "dates = matrix['date_block_num']\n",
    "\n",
    "# drop target and some features that lead to overfitting\n",
    "cols_to_drop = ['item_cnt_month', 'new_item', 'shop_id', 'item_id']\n",
    "\n",
    "# split dataset on train and test sets for model training\n",
    "X_train = matrix.loc[dates <  last_block].drop(cols_to_drop, axis=1)\n",
    "X_val = matrix.loc[dates ==  last_block].drop(cols_to_drop, axis=1)\n",
    "X_test = matrix.loc[dates ==  34].drop(cols_to_drop, axis=1)\n",
    "\n",
    "y_train = matrix.loc[dates <  last_block, 'item_cnt_month']\n",
    "y_val = matrix.loc[dates ==  last_block, 'item_cnt_month']\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "evals_result = dict()\n",
    "\n",
    "gbm = lgb.train(\n",
    "        params, \n",
    "        lgb_train,\n",
    "        valid_sets=(lgb_train, lgb_eval), \n",
    "#         valid_sets=lgb_train,\n",
    "        categorical_feature = cat_feats,\n",
    "        verbose_eval=50, \n",
    "        evals_result = evals_result,\n",
    "        early_stopping_rounds = 50,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous RMSE on the train set except validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training until validation scores don't improve for 30 rounds\n",
    "# [50]\ttraining's rmse: 0.968815\ttraining's l2: 0.938602\tvalid_1's rmse: 0.873901\tvalid_1's l2: 0.763703\n",
    "# [100]\ttraining's rmse: 0.829555\ttraining's l2: 0.688162\tvalid_1's rmse: 0.780026\tvalid_1's l2: 0.608441\n",
    "# [150]\ttraining's rmse: 0.757286\ttraining's l2: 0.573481\tvalid_1's rmse: 0.740559\tvalid_1's l2: 0.548427\n",
    "# [200]\ttraining's rmse: 0.717651\ttraining's l2: 0.515023\tvalid_1's rmse: 0.722095\tvalid_1's l2: 0.521421\n",
    "# [250]\ttraining's rmse: 0.693096\ttraining's l2: 0.480382\tvalid_1's rmse: 0.715246\tvalid_1's l2: 0.511577\n",
    "# [300]\ttraining's rmse: 0.675777\ttraining's l2: 0.456675\tvalid_1's rmse: 0.711053\tvalid_1's l2: 0.505596\n",
    "# [350]\ttraining's rmse: 0.662477\ttraining's l2: 0.438876\tvalid_1's rmse: 0.709262\tvalid_1's l2: 0.503052\n",
    "# [400]\ttraining's rmse: 0.651249\ttraining's l2: 0.424126\tvalid_1's rmse: 0.708145\tvalid_1's l2: 0.50147\n",
    "# [450]\ttraining's rmse: 0.641991\ttraining's l2: 0.412152\tvalid_1's rmse: 0.707486\tvalid_1's l2: 0.500537\n",
    "# [500]\ttraining's rmse: 0.634088\ttraining's l2: 0.402068\tvalid_1's rmse: 0.706936\tvalid_1's l2: 0.499758\n",
    "# [550]\ttraining's rmse: 0.626927\ttraining's l2: 0.393038\tvalid_1's rmse: 0.706606\tvalid_1's l2: 0.499292\n",
    "# [600]\ttraining's rmse: 0.620879\ttraining's l2: 0.385491\tvalid_1's rmse: 0.706248\tvalid_1's l2: 0.498786\n",
    "# [650]\ttraining's rmse: 0.615213\ttraining's l2: 0.378488\tvalid_1's rmse: 0.706163\tvalid_1's l2: 0.498666\n",
    "# Early stopping, best iteration is:\n",
    "# [630]\ttraining's rmse: 0.617487\ttraining's l2: 0.38129\tvalid_1's rmse: 0.706027\tvalid_1's l2: 0.498474\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous RMSE on the full train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training until validation scores don't improve for 30 rounds\n",
    "# [50]\ttraining's rmse: 0.96647\ttraining's l2: 0.934064\n",
    "# [100]\ttraining's rmse: 0.827901\ttraining's l2: 0.68542\n",
    "# [150]\ttraining's rmse: 0.756018\ttraining's l2: 0.571563\n",
    "# [200]\ttraining's rmse: 0.716688\ttraining's l2: 0.513642\n",
    "# [250]\ttraining's rmse: 0.692276\ttraining's l2: 0.479246\n",
    "# [300]\ttraining's rmse: 0.675353\ttraining's l2: 0.456102\n",
    "# [350]\ttraining's rmse: 0.66207\ttraining's l2: 0.438337\n",
    "# [400]\ttraining's rmse: 0.650908\ttraining's l2: 0.423681\n",
    "# [450]\ttraining's rmse: 0.641561\ttraining's l2: 0.4116\n",
    "# [500]\ttraining's rmse: 0.633598\ttraining's l2: 0.401446\n",
    "# [550]\ttraining's rmse: 0.626378\ttraining's l2: 0.392349\n",
    "# [600]\ttraining's rmse: 0.620459\ttraining's l2: 0.38497\n",
    "# [650]\ttraining's rmse: 0.614824\ttraining's l2: 0.378008\n",
    "# [700]\ttraining's rmse: 0.609965\ttraining's l2: 0.372057\n",
    "# [750]\ttraining's rmse: 0.605311\ttraining's l2: 0.366401\n",
    "# [800]\ttraining's rmse: 0.600958\ttraining's l2: 0.361151\n",
    "# [850]\ttraining's rmse: 0.597002\ttraining's l2: 0.356412\n",
    "# [900]\ttraining's rmse: 0.593272\ttraining's l2: 0.351971\n",
    "# [950]\ttraining's rmse: 0.58962\ttraining's l2: 0.347652\n",
    "# [1000]\ttraining's rmse: 0.586284\ttraining's l2: 0.343729\n",
    "# Did not meet early stopping. Best iteration is:\n",
    "# [1000]\ttraining's rmse: 0.586284\ttraining's l2: 0.343729\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the feature importances ranked by error reduction on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 3.892104,
     "end_time": "2021-04-28T19:37:25.679539",
     "exception": false,
     "start_time": "2021-04-28T19:37:21.787435",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(booster, figsize=(10,50), height=0.7, importance_type=\"gain\", max_num_features=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>item_cnt_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.929894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.154513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.940560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.475317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.685377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.507466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.585706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.257275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.740623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.714995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  item_cnt_month\n",
       "0   0        0.929894\n",
       "1   1        2.154513\n",
       "2   2        0.940560\n",
       "3   3        0.475317\n",
       "4   4        1.685377\n",
       "5   5        0.507466\n",
       "6   6        0.585706\n",
       "7   7        0.257275\n",
       "8   8        0.740623\n",
       "9   9        0.714995"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "test_pred_lgb = gbm.predict(X_test)\n",
    "\n",
    "submission = pd.read_csv(\"sample_submission.csv.zip\")\n",
    "submission['item_cnt_month'] = test_pred_lgb\n",
    "submission.item_cnt_month.clip(0, 20, inplace=True)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "_ = joblib.dump(booster, \"trained_lgbooster.pkl\")\n",
    "print(\"Saved single booster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Futher ideas\n",
    "\n",
    "- increase number of training rounds\n",
    "- fill NaNs with zeros\n",
    "\n",
    "- optimize with Optuna\n",
    "\n",
    "- use LAMA\n",
    "- you can also blend your submission with submissions from other kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple ensembling with VotingRegressor  \n",
    "\n",
    "Here we make a simple ensembling predictor with the scikit-learn [VotingRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html) predictor. This wraps a list of predictors and outputs a linear combination of their predictions. In principle any scikit-learn compatible regression model can be used, but here we just use LightGBM models, as no other model types were efficient enough to run in a Kaggle notebook without memory allocation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and split the data for fitting. For the ensemble predictor the LightGBM models are fit on all the training data with a pre-set number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9.884487,
     "end_time": "2021-04-28T19:16:59.734499",
     "exception": false,
     "start_time": "2021-04-28T19:16:49.850012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n",
    "# Downcast the float columns to reduce RAM usage\n",
    "floatcols = [c for c in matrix.columns if matrix[c].dtype==\"float32\"]\n",
    "matrix[floatcols] = matrix[floatcols].astype(\"float16\")\n",
    "matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\n",
    "keep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\n",
    "test_month = 34\n",
    "dropcols = [\n",
    "    \"shop_id\",\n",
    "    \"item_id\",\n",
    "    \"new_item\",\n",
    "]  # The features are dropped to reduce overfitting\n",
    "categoricals = [\n",
    "    \"item_category_id\",\n",
    "    \"month\",\n",
    "]\n",
    "matrix[categoricals] = matrix[categoricals].astype(\"category\") \n",
    "test = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\n",
    "train = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\n",
    "train = train[train.date_block_num >= keep_from_month]\n",
    "X_train = train.drop(columns=\"item_cnt_month\")\n",
    "y_train = train.item_cnt_month\n",
    "X_test = test.drop(columns=\"item_cnt_month\")\n",
    "y_test = test.item_cnt_month\n",
    "del(matrix, test, train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different parameters are used for each of 5 LightGBM models used in the ensemble. These parameters were the highest scoring parameters found by Optuna when optimizing on the validation date block 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = [\n",
    "    {\n",
    "        \"num_leaves\": 966,\n",
    "        \"cat_smooth\": 45.01680827234465,\n",
    "        \"min_child_samples\": 27,\n",
    "        \"min_child_weight\": 0.021144950289224463,\n",
    "        \"max_bin\": 214,\n",
    "        \"n_estimators\": 500,\n",
    "        \"subsample_for_bin\": 300000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"force_col_wise\": True\n",
    "    },\n",
    "    {\n",
    "        \"num_leaves\": 940,\n",
    "        \"cat_smooth\": 43.418286701105615,\n",
    "        \"min_child_samples\": 29,\n",
    "        \"min_child_weight\": 0.003944267312494195,\n",
    "        \"max_bin\": 133,\n",
    "        \"n_estimators\": 572,\n",
    "        \"subsample_for_bin\": 300000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"force_col_wise\": True\n",
    "    },\n",
    "    {\n",
    "        \"num_leaves\": 971,\n",
    "        \"cat_smooth\": 40.103611531065525,\n",
    "        \"min_child_samples\": 30,\n",
    "        \"min_child_weight\": 0.03951287458923346,\n",
    "        \"max_bin\": 212,\n",
    "        \"n_estimators\": 828,\n",
    "        \"subsample_for_bin\": 300000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"force_col_wise\": True\n",
    "    },\n",
    "    {\n",
    "        \"num_leaves\": 965,\n",
    "        \"cat_smooth\": 40.05144976454027,\n",
    "        \"min_child_samples\": 27,\n",
    "        \"min_child_weight\": 0.029220951478909872,\n",
    "        \"max_bin\": 211,\n",
    "        \"n_estimators\": 870,\n",
    "        \"subsample_for_bin\": 300000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"force_col_wise\": True\n",
    "    },\n",
    "    {\n",
    "        \"num_leaves\": 961,\n",
    "        \"cat_smooth\": 40.013529776221134,\n",
    "        \"min_child_samples\": 29,\n",
    "        \"min_child_weight\": 0.026526521644599493,\n",
    "        \"max_bin\": 210,\n",
    "        \"n_estimators\": 897,\n",
    "        \"subsample_for_bin\": 300000,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"force_col_wise\": True\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and fit the VotingRegressor ensemble on the training data. This takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "regressors = []\n",
    "for i, params in enumerate(best_params):\n",
    "    booster = LGBMRegressor(**params)\n",
    "    regressors.append((f\"lgbr_{i}\", booster))\n",
    "vr = VotingRegressor(regressors, verbose=True)\n",
    "print(\"Fitting voting regressor\")\n",
    "vr = vr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize the trained regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "_ = joblib.dump(vr, \"trained_votingregressor.pkl\")\n",
    "print(\"Voting regressor trained and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.111863,
     "end_time": "2021-04-28T19:37:25.904072",
     "exception": false,
     "start_time": "2021-04-28T19:37:25.792209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create the test submission  \n",
    "Split the test items from the data matrix and use the trained voting regressor to predict the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "booster = joblib.load(\"trained_votingregressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 50.515225,
     "end_time": "2021-04-28T19:38:16.529825",
     "exception": false,
     "start_time": "2021-04-28T19:37:26.0146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle(\"checkpoint_final.pkl\")\n",
    "keep_from_month = 2\n",
    "test_month = 34\n",
    "dropcols = [\n",
    "    \"shop_id\",\n",
    "    \"item_id\",\n",
    "    \"new_item\",\n",
    "]  # The features are dropped to reduce overfitting\n",
    "categoricals = [\n",
    "    \"item_category_id\",\n",
    "    \"month\",\n",
    "]\n",
    "matrix[categoricals] = matrix[categoricals].astype(\"category\")\n",
    "test = matrix.loc[matrix.date_block_num == test_month, :]\n",
    "X_test = test.drop(columns=\"item_cnt_month\")\n",
    "y_test = test.item_cnt_month\n",
    "del matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 9.884487,
     "end_time": "2021-04-28T19:16:59.734499",
     "exception": false,
     "start_time": "2021-04-28T19:16:49.850012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test[\"item_cnt_month\"] = booster.predict(X_test.drop(columns=dropcols)).clip(0, 20)\n",
    "# Merge the predictions with the provided template\n",
    "test_orig = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")\n",
    "test = test_orig.merge(\n",
    "    X_test[[\"shop_id\", \"item_id\", \"item_cnt_month\"]],\n",
    "    on=[\"shop_id\", \"item_id\"],\n",
    "    how=\"inner\",\n",
    "    copy=True,\n",
    ")\n",
    "# Verify that the indices of the submission match the original\n",
    "assert test_orig.equals(test[[\"ID\", \"shop_id\", \"item_id\"]])\n",
    "test[[\"ID\", \"item_cnt_month\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.122639,
     "end_time": "2021-04-28T19:38:27.697031",
     "exception": false,
     "start_time": "2021-04-28T19:38:27.574392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from os import remove\n",
    "# remove(\"checkpoint_final.pkl\")\n",
    "# remove(\"matrixcheckpoint.pkl\")\n",
    "# print(\"Finished everything!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
